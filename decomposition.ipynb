{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed50fd6-302d-482e-8936-27511f12aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decompose Phi-3 into constituent parts. Runs each part peice by piece. Test logit lens, position loss, and feature clustering via the MLP layer.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9585a2aa-4381-4732-ae84-8a4786fe3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML, Markdown\n",
    "import plotly.express as px \n",
    "import os\n",
    "import wandb \n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib\n",
    "import pathlib\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from helpers.phi3.phi3 import Phi3Config, Phi3ForCausalLM, _prepare_4d_causal_attention_mask\n",
    "from helpers.phi3.parse import parse_phi\n",
    "from helpers.memory import check_memory\n",
    "\n",
    "load_dotenv('secrets.env')\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09c855-42cd-49f4-bc72-e4a44bbddb10",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb52446-f4a9-42e5-b896-6d25bde904f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_implementation = None # None/flash_attention_2\n",
    "\n",
    "# Load Model\n",
    "# Padding side not important EXCEPT for flash attention, needs to be left\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-4k-instruct', add_eos_token = False, add_bos_token = False, padding_side = 'left') \n",
    "\n",
    "# Load the usual model from HF transformers - attn_implementation = None to disable flash attention\n",
    "my_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'microsoft/Phi-3-mini-4k-instruct',\n",
    "    device_map = device,\n",
    "    trust_remote_code = True, \n",
    "    torch_dtype = torch.bfloat16, \n",
    "    attn_implementation = attn_implementation\n",
    "    ).to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f486b8-1676-45af-9eea-643fdc9f6982",
   "metadata": {},
   "source": [
    "## Initialize FENCE Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a78ab15-a163-4c20-a8a2-5bcef3ceeee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass indices starting at 1\n",
    "fence_dict = {\n",
    "    'animals': (3061, 3072),\n",
    "    'cats': (3061, 3064),\n",
    "    'dogs': (3065, 3068),\n",
    "    'food': (3020, 3028),\n",
    "    'programming': (2961, 2972)\n",
    "}\n",
    "\n",
    "Kfstart = 1\n",
    "Kfend = 32\n",
    "Kf_target_values = {\n",
    "    'hkrs': {Kfstart + j - 1: (j - 1) * .25 + .25/2 for j in range(Kfstart, Kfend + 1)},\n",
    "    'hks': {Kfstart + j - 1: (j - 1) * .25 + .25 for j in range(Kfstart, Kfend + 1)},\n",
    "}\n",
    "\n",
    "print(fence_dict)\n",
    "Kf_target_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202c2d4-0e95-4603-862f-f6c7df219d45",
   "metadata": {},
   "source": [
    "## Test Inference & Visualizations with Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76e7ac-3146-4aba-9379-5f7a4652e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.eval'))\n",
    "from helpers.fence.eval import generate_fence\n",
    "\n",
    "# Test\n",
    "dog_prompt = parse_phi([{'role': 'user', 'content': 'What\\'s your favorite animal?'}], True)\n",
    "dog_gens = generate_fence(my_model, tokenizer, prompt = dog_prompt, max_tokens = 12)\n",
    "\n",
    "animal_prompt = '<s>Animals are,'\n",
    "animal_gens = generate_fence(my_model, tokenizer, prompt = animal_prompt, max_tokens = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b57ce7-1059-45df-8126-d7d818e46961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'importlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimportlib\u001b[49m\u001b[38;5;241m.\u001b[39mreload(importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhelpers.fence.visualize\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfence\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m visualize_fence\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'importlib' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.visualize'))\n",
    "from helpers.fence.visualize import visualize_fence\n",
    "\n",
    "for l in [1]:\n",
    "    visualize_fence(\n",
    "        dog_gens['text'],\n",
    "        dog_gens['hks'],\n",
    "        [l],\n",
    "        fence_dict,\n",
    "        start_dim = 2900, end_dim = 3072,\n",
    "        min_range = 0, max_range = Kf_target_values['hks'][l]\n",
    "    ).update_layout(title = 'H<sub>' + str(l) + '</sub>', height = 300).show('colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d6645f-3357-41b2-8b57-c2b05aa9cf83",
   "metadata": {},
   "source": [
    "## Test Component-by-Component Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005fbbc-8f14-4656-9582-2268b0c3ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.misc import is_notebook\n",
    "from helpers.phi3.phi3 import _prepare_4d_causal_attention_mask, apply_rotary_pos_emb\n",
    "import math\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_fence_with_force(model, tokenizer, prompt, echo_output = True, max_tokens = 128, device = 'cuda'):\n",
    "    \"\"\"\n",
    "    Runs a forward pass and stores FENCE-relevant intermediate hidden states. Allows for forced-FENCE (see foward pass code).\n",
    "    Also calculates the modularity loss. Position loss is NOT calculated as there are no true-FENCE states.\n",
    "\n",
    "    Returns a dictionary with keys:\n",
    "        - `text`: The decoded output text, as a list\n",
    "        - `hk1s`: The first residual stream output\n",
    "        - `hk2s`: The final residual stream output\n",
    "        - `hksas`: The hidden state outputs of the SA component\n",
    "        - `hkmlps`: The hidden state outputs of the MLP component\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated_tokens = 0\n",
    "    \n",
    "    input_ids_0 = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']\n",
    "    input_ids = input_ids_0\n",
    "\n",
    "    while True:\n",
    "        embeds_output = model.model.embed_tokens(input_ids)\n",
    "        hidden_state = embeds_output\n",
    "        \n",
    "        B, N, D = embeds_output.shape\n",
    "        H = 32\n",
    "        Dh = int(D/H)\n",
    "        \n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        \n",
    "        # Flash attention = use default attention mask 2d\n",
    "        if model.model._attn_implementation == 'flash_attention_2':\n",
    "            attention_mask = None\n",
    "        # Non flash-attention: Make a triangular attention mask to hide right context\n",
    "        else:\n",
    "            attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = model.model.config.sliding_window) \n",
    "\n",
    "        saved_sa_outputs = []\n",
    "        saved_hkrs = []\n",
    "        saved_mlp_outputs = []\n",
    "        saved_hks = []\n",
    "        ### Transformer Blocks ###\n",
    "        for i, layer in enumerate(model.model.layers):            \n",
    "\n",
    "            residual = hidden_state\n",
    "            sa_input = layer.input_layernorm(hidden_state)\n",
    "            \n",
    "            ### SA ###\n",
    "            sa_module = layer.self_attn\n",
    "            # sa_output = sa_module(sa_input, attention_mask, position_ids)[0]\n",
    "            qkv = sa_module.qkv_proj(sa_input)\n",
    "            queries = qkv[:, :, :D].view(B, N, H, Dh).transpose(1, 2)\n",
    "            keys = qkv[:, :, D:2*D].view(B, N, H, Dh).transpose(1, 2)\n",
    "            values = qkv[:, :, 2*D:].view(B, N, H, Dh).transpose(1, 2)\n",
    "\n",
    "            if model.model._attn_implementation == 'flash_attention_2':     \n",
    "                # Flash attention requires the input to have the shape B x N x Dh x D           \n",
    "                # Because the input can be padded, the absolute sequence length depends on the max position id.\n",
    "                rotary_seq_len = max(N, position_ids[:, -1].max().item()) + 1\n",
    "                cos, sin = sa_module.rotary_emb(values, position_ids, seq_len = rotary_seq_len)\n",
    "                queries, keys = apply_rotary_pos_emb(queries, keys, cos, sin, position_ids)\n",
    "                ################## # Reshape to the expected shape for Flash Attention\n",
    "                queries = queries.transpose(1, 2)\n",
    "                keys = keys.transpose(1, 2)\n",
    "                values = values.transpose(1, 2)\n",
    "                ###################\n",
    "                sa_output = sa_module._flash_attention_forward(queries, keys, values, attention_mask, N)\n",
    "                sa_output = sa_output.reshape(B, N, D).contiguous()\n",
    "            else:    \n",
    "                cos, sin = sa_module.rotary_emb(values, position_ids, seq_len = N)\n",
    "                queries, keys = apply_rotary_pos_emb(queries, keys, cos, sin, position_ids)\n",
    "                attn_weights = torch.matmul(queries, keys.transpose(2, 3))/math.sqrt(Dh)  # Should be shape B x H x N x N\n",
    "                attn_weights = attn_weights + attention_mask # Attemtion mask is upper triangular of negative infinity\n",
    "                attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(values.dtype)\n",
    "                sa_output = torch.matmul(attn_weights, values) # B x H x N x D/H\n",
    "                sa_output = sa_output.transpose(1, 2).contiguous() # Reorder into B x N x H x D/H\n",
    "                sa_output = sa_output.reshape(B, N, D) # Concatenate vertically back into B x N x D\n",
    "    \n",
    "            # Finally post-concatenation linear layer\n",
    "            sa_output = sa_module.o_proj(sa_output)\n",
    "\n",
    "            saved_sa_outputs.append(sa_output[0, :, :].detach())\n",
    "            \n",
    "            ### add residual -> store residual -> layernorm -> mlp -> add residual\n",
    "            hidden_state = residual + sa_output\n",
    "\n",
    "            # FENCE\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                # To extract the right layer from hkr_feature_targets:\n",
    "                # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "                # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "                # - => k = l + 1 - Kfstart\n",
    "                # hidden_state[:, 1:, 3064:3072] = Kf_target_values['hkrs'][l+1] # B x N x Df                \n",
    "                pass\n",
    "            \n",
    "            residual = hidden_state\n",
    "            saved_hkrs.append(hidden_state[0, :, :].detach())\n",
    "\n",
    "            hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "            ## MLP            \n",
    "            up_state = layer.mlp.gate_up_proj(hidden_state) # B x N x (2I, I = intermediate MLP dimension)\n",
    "            gate, up_state = up_state.chunk(2, dim = -1) # B x N x I\n",
    "            up_state = up_state * layer.mlp.activation_fn(gate)  # Elementwise\n",
    "            hidden_state = layer.mlp.down_proj(up_state) # Back to B x N x D\n",
    "            ## End MLP\n",
    "            \n",
    "            saved_mlp_outputs.append(hidden_state[0, :, :].detach())\n",
    "\n",
    "            hidden_state = residual + hidden_state\n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                # hidden_state[:, 1:, 3064:3072] = Kf_target_values['hks'][l+1] # B x N x Df\n",
    "                pass\n",
    "\n",
    "            saved_hks.append(hidden_state[0, :, :].detach())\n",
    "                \n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "        logits = model.lm_head(hidden_state)\n",
    "\n",
    "        # Get argmax tokens + concatenate onto previous tokens\n",
    "        output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "        input_ids = torch.cat((input_ids, output_token.view(1, 1)), dim = 1)\n",
    "\n",
    "        # Break while loop if EOS or generation > max tokens\n",
    "        generated_tokens = generated_tokens + 1\n",
    "        if output_token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end|>\")] or generated_tokens >= max_tokens:\n",
    "            break\n",
    "\n",
    "    # Use it on the last pass only\n",
    "    all_hksas = [h.cpu().to(torch.float16).numpy() for h in saved_sa_outputs]\n",
    "    all_hkrs = [h.cpu().to(torch.float16).numpy() for h in saved_hkrs]\n",
    "    all_hkmlps = [h.cpu().to(torch.float16).numpy() for h in saved_mlp_outputs]\n",
    "    all_hks = [h.cpu().to(torch.float16).numpy() for h in saved_hks]\n",
    "\n",
    "    final_output = input_ids.squeeze()\n",
    "    decoded_text = tokenizer.batch_decode(final_output)\n",
    "    decoded_output = tokenizer.decode(final_output[input_ids_0.size()[1]:])\n",
    "\n",
    "    if echo_output:\n",
    "        if is_notebook():\n",
    "            display(HTML(\n",
    "                '<div style=\"padding: 1rem 2rem; background-color:honeydew\">' + \n",
    "                    '<h4>Modified model output</h4>' + \n",
    "                    '<span style=\"color:green\">' + tokenizer.batch_decode(input_ids_0)[0][3:] + '</span> ' + \n",
    "                    '<span style=\"color:red\">' + decoded_output + '</span>' +\n",
    "                '</div>'\n",
    "            ))\n",
    "        else:\n",
    "            print(colored(tokenizer.batch_decode(input_ids_0)[0][3:], 'green'), colored(tokenizer.decode(final_output[input_ids_0.size()[1]:]), 'red'))\n",
    "\n",
    "generate_fence(my_model, tokenizer, prompt = dog_prompt, max_tokens = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a2d43-1656-4e63-8532-3b53949fa59a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fcc429-45b0-40a0-b3ff-844492534718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - SINGLE PASS\n",
    "\"\"\"\n",
    "This does not collect any position loss, but it does collect modularity loss\n",
    "\"\"\"\n",
    "from helpers.phi3.phi3 import _prepare_4d_causal_attention_mask, apply_rotary_pos_emb\n",
    "import math\n",
    "\n",
    "model = my_model\n",
    "prompt = dog_prompt\n",
    "\n",
    "model.eval()\n",
    "generated_tokens = 0\n",
    "\n",
    "input_ids_0 = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']\n",
    "input_ids = input_ids_0\n",
    "\n",
    "embeds_output = model.model.embed_tokens(input_ids)\n",
    "hidden_state = embeds_output\n",
    "\n",
    "B, N, D = embeds_output.shape\n",
    "H = 32\n",
    "Dh = int(D/H)\n",
    "\n",
    "position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "\n",
    "# Flash attention = use default attention mask 2d\n",
    "if model.model._attn_implementation == 'flash_attention_2':\n",
    "    attention_mask = None\n",
    "# Non flash-attention: Make a triangular attention mask to hide right context\n",
    "else:\n",
    "    attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = model.model.config.sliding_window) \n",
    "\n",
    "saved_sa_outputs = []\n",
    "saved_hkrs = []\n",
    "saved_mlp_outputs = []\n",
    "saved_hks = []\n",
    "### Transformer Blocks ###\n",
    "for i, layer in enumerate(model.model.layers):            \n",
    "\n",
    "    residual = hidden_state\n",
    "    sa_input = layer.input_layernorm(hidden_state)\n",
    "    \n",
    "    ### SA ###\n",
    "    sa_module = layer.self_attn\n",
    "    # sa_output = sa_module(sa_input, attention_mask, position_ids)[0]\n",
    "    qkv = sa_module.qkv_proj(sa_input)\n",
    "    queries = qkv[:, :, :D].view(B, N, H, Dh).transpose(1, 2)\n",
    "    keys = qkv[:, :, D:2*D].view(B, N, H, Dh).transpose(1, 2)\n",
    "    values = qkv[:, :, 2*D:].view(B, N, H, Dh).transpose(1, 2)\n",
    "\n",
    "    if model.model._attn_implementation == 'flash_attention_2':     \n",
    "        # Flash attention requires the input to have the shape B x N x Dh x D           \n",
    "        # Because the input can be padded, the absolute sequence length depends on the max position id.\n",
    "        rotary_seq_len = max(N, position_ids[:, -1].max().item()) + 1\n",
    "        cos, sin = sa_module.rotary_emb(values, position_ids, seq_len = rotary_seq_len)\n",
    "        queries, keys = apply_rotary_pos_emb(queries, keys, cos, sin, position_ids)\n",
    "        ################## # Reshape to the expected shape for Flash Attention\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        ###################\n",
    "        sa_output = sa_module._flash_attention_forward(queries, keys, values, attention_mask, N)\n",
    "        sa_output = sa_output.reshape(B, N, D).contiguous()\n",
    "    else:    \n",
    "        cos, sin = sa_module.rotary_emb(values, position_ids, seq_len = N)\n",
    "        queries, keys = apply_rotary_pos_emb(queries, keys, cos, sin, position_ids)\n",
    "        attn_weights = torch.matmul(queries, keys.transpose(2, 3))/math.sqrt(Dh)  # Should be shape B x H x N x N\n",
    "        attn_weights = attn_weights + attention_mask # Attemtion mask is upper triangular of negative infinity\n",
    "        attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(values.dtype)\n",
    "        sa_output = torch.matmul(attn_weights, values) # B x H x N x D/H\n",
    "        sa_output = sa_output.transpose(1, 2).contiguous() # Reorder into B x N x H x D/H\n",
    "        sa_output = sa_output.reshape(B, N, D) # Concatenate vertically back into B x N x D\n",
    "\n",
    "    # Finally post-concatenation linear layer\n",
    "    sa_output = sa_module.o_proj(sa_output)\n",
    "\n",
    "    saved_sa_outputs.append(sa_output[0, :, :].detach())\n",
    "    \n",
    "    ### add residual -> store residual -> layernorm -> mlp -> add residual\n",
    "    hidden_state = residual + sa_output\n",
    "\n",
    "    # FENCE\n",
    "    if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "        # To extract the right layer from hkr_feature_targets:\n",
    "        # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "        # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "        # - => k = l + 1 - Kfstart\n",
    "        # hidden_state[:, 1:, 3064:3072] = Kf_target_values['hkrs'][l+1] # B x N x Df                \n",
    "        pass\n",
    "    \n",
    "    residual = hidden_state\n",
    "    saved_hkrs.append(hidden_state[0, :, :].detach())\n",
    "\n",
    "    hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "    hidden_state_pre_mlp = hidden_state\n",
    "    \n",
    "    ## MLP       \n",
    "    # hidden_state is of size B x N x D\n",
    "\n",
    "    # Original\n",
    "    # gate_plus_vmat = layer.mlp.gate_up_proj(hidden_state) # B x N x (2I, I = intermediate MLP dimension)\n",
    "    # gate, vmat = gate_plus_fvals.chunk(2, dim = -1) # B x N x I\n",
    "\n",
    "    # Alternative - split gate and vmat\n",
    "    weight_matrix = layer.mlp.gate_up_proj.weight\n",
    "    # Split the weight matrix into two parts along the output dimension (dim=0)\n",
    "    gate_weight, vmat_weight = weight_matrix.chunk(2, dim=0)\n",
    "    # Manually perform the linear transformation: gate = hidden_state @ gate_weight^T; vmat = hidden_state @ vmat_weight^T\n",
    "    gate = torch.matmul(hidden_state, gate_weight.T)\n",
    "    hv = torch.matmul(hidden_state, vmat_weight.T)\n",
    "\n",
    "    # At this point the up_state = values (see Geva et al), and the gate is the keys\n",
    "    up_state = hv * layer.mlp.activation_fn(gate)  # Elementwise\n",
    "    hidden_state = layer.mlp.down_proj(up_state) # Back to B x N x D\n",
    "    ## End MLP\n",
    "    \n",
    "    saved_mlp_outputs.append(hidden_state[0, :, :].detach())\n",
    "\n",
    "    hidden_state = residual + hidden_state\n",
    "\n",
    "    if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "        # hidden_state[:, 1:, 3064:3072] = Kf_target_values['hks'][l+1] # B x N x Df\n",
    "        pass\n",
    "\n",
    "    saved_hks.append(hidden_state[0, :, :].detach())\n",
    "        \n",
    "hidden_state = model.model.norm(hidden_state)\n",
    "logits = model.lm_head(hidden_state)\n",
    "\n",
    "# Get argmax tokens + concatenate onto previous tokens\n",
    "output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "input_ids = torch.cat((input_ids, output_token.view(1, 1)), dim = 1)\n",
    "\n",
    "\n",
    "tokenizer.batch_decode([output_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967739d-cac2-4148-9b6e-dddb823f82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.memory import check_memory\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b0268-9236-4e53-8ad2-80e245838fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d26a7-1300-4644-8dc3-73d8c17593a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmat_weight.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d5480-20c5-4cac-955a-d6b28fe366c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def normalized_L1_loss(H, V, dim_range=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Computes the normalized L1 distance-weighted interaction loss for the given input tensors H and V.\n",
    "\n",
    "    Parameters:\n",
    "    - H (torch.Tensor): The input tensor of shape (B, N, D), where B is the batch size, N is the token length, and D is the dimension.\n",
    "    - V (torch.Tensor): The learnable weight matrix of shape (D, I).\n",
    "    \n",
    "    Returns:\n",
    "    - loss (torch.Tensor): The computed normalized L1 loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    B, N, D = H.shape  # B: batch size, N: token length, D: dimension\n",
    "    I = V.shape[1]     # I: the output dimension (same as p_j length)\n",
    "    \n",
    "    # If dim_range is None, use the full range of dimensions\n",
    "    if dim_range is None:\n",
    "        dim_range = (0, D)\n",
    "        \n",
    "    start_dim, end_dim = dim_range\n",
    "    assert 0 <= start_dim < end_dim <= D, \"Invalid dimension range.\"\n",
    "\n",
    "    # Calculate P = H @ V (for each batch and token)\n",
    "    # H has shape (B, N, D) and V has shape (D, I), so the result P will have shape (B, N, I)\n",
    "    P = torch.matmul(H, V)  # This computes P efficiently in batch mode\n",
    "\n",
    "    # Initialize loss\n",
    "    loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    # Compute the normalized L1 loss\n",
    "    # We'll need to iterate over the dimension D for the interactions between elements of H.\n",
    "    for i in range(start_dim, end_dim):\n",
    "        print(i)\n",
    "        for j in range(i + 1, end_dim):  # Only consider upper triangular part to avoid double-counting\n",
    "            # Interaction term |h_i * h_j|\n",
    "            interaction = torch.abs(H[:, :, i] * H[:, :, j])\n",
    "\n",
    "            # Distance-weighted interaction: Multiply by |i - j| (the distance between indices)\n",
    "            weighted_interaction = interaction * abs(i - j)\n",
    "\n",
    "            # Sum over batches (B) and tokens (N) for the total weighted loss\n",
    "            loss += weighted_interaction.sum()\n",
    "\n",
    "    # Normalization factor: Sum of all interaction terms (i.e., without the distance weighting)\n",
    "    sum_interactions = 0\n",
    "    for i in range(start_dim, end_dim):\n",
    "        for j in range(i + 1, end_dim):  # Again, only consider the upper triangular part\n",
    "            interaction = torch.abs(H[:, :, i] * H[:, :, j])\n",
    "            sum_interactions += interaction.sum()\n",
    "\n",
    "    # Normalize the loss\n",
    "    if sum_interactions > 0:\n",
    "        loss = loss / sum_interactions\n",
    "\n",
    "    return loss\n",
    "\n",
    "normalized_L1_loss(hidden_state_pre_mlp, vmat_weight.T, dim_range = (2900, 3072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09471480-e3c2-4ca5-b510-a9677bdaf099",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state_pre_mlp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae514c0d-176a-49af-a42d-66e4caafd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_L1_loss_memory_efficient(H, V):\n",
    "    \"\"\"\n",
    "    Calculate the normalized L1 loss for a batch of input matrices H (B x N x D)\n",
    "    and a learnable weight matrix V (D x I) using a more memory-efficient approach.\n",
    "    \n",
    "    Arguments:\n",
    "    H -- Input matrix of shape (B x N x D)\n",
    "    V -- Learnable weight matrix of shape (D x I)\n",
    "    \n",
    "    Returns:\n",
    "    Loss value -- Scalar representing the normalized L1 loss\n",
    "    \"\"\"\n",
    "    B, N, D = H.shape  # B = batch size, N = token length, D = dimension\n",
    "    I = V.shape[1]     # I = number of outputs (same as number of columns in V)\n",
    "\n",
    "    # Precompute the distance matrix for dimensions D x D\n",
    "    distance_matrix = torch.abs(torch.arange(D).unsqueeze(0) - torch.arange(D).unsqueeze(1)).to(H.device)  # Shape (D, D)\n",
    "\n",
    "    # (1) Compute the matrix product H[b, n, d] * V[d, i] for each b, n, and i\n",
    "    # Shape of intermediate result: (B, N, D, I)\n",
    "    HV = torch.einsum('bnd,di->bndi', H, V)  # Shape (B, N, D, I)\n",
    "    \n",
    "    # (2) Compute pairwise interactions between different dimensions d1 and d2:\n",
    "    # HV[b, n, d1, i] * HV[b, n, d2, i] (for all d1, d2 pairs)\n",
    "    # Shape of result: (B, N, D, D, I)\n",
    "    interaction_matrix = HV.unsqueeze(3) * HV.unsqueeze(2)  # Shape (B, N, D, D, I)\n",
    "    \n",
    "    # (3) Apply the distance matrix to weigh interactions\n",
    "    distance_weighted_interactions = interaction_matrix * distance_matrix.unsqueeze(-1)  # Shape (B, N, D, D, I)\n",
    "    \n",
    "    # (4) Sum over the D x D interaction matrix to compute the raw penalty\n",
    "    raw_penalty = torch.sum(distance_weighted_interactions, dim=(2, 3))  # Shape (B, N, I)\n",
    "    \n",
    "    # (5) Sum over interactions (without distance weighting) to compute sum of interactions\n",
    "    sum_interactions = torch.sum(interaction_matrix, dim=(2, 3))  # Shape (B, N, I)\n",
    "    \n",
    "    # (6) Avoid division by zero by clamping sum_interactions\n",
    "    sum_interactions = torch.clamp(sum_interactions, min=1e-8)\n",
    "    \n",
    "    # (7) Normalize the raw penalty by the sum of interactions\n",
    "    normalized_penalty = raw_penalty / sum_interactions  # Shape (B, N, I)\n",
    "    \n",
    "    # (8) Compute the total loss: sum over batches, tokens, and output dimensions\n",
    "    loss = torch.mean(normalized_penalty)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "    \n",
    "normalized_L1_loss_memory_efficient(hidden_state_pre_mlp, vmat_weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2a63a-3c72-43b3-a580-835a40509d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_memory_usage(H, V):\n",
    "    \"\"\"\n",
    "    Calculate the memory usage for tensors in the normalized L1 loss computation.\n",
    "    \"\"\"\n",
    "    B, N, D = H.shape  # B = batch size, N = token length, D = dimension\n",
    "    I = V.shape[1]     # I = number of outputs\n",
    "\n",
    "    # Memory for H and V\n",
    "    H_mem = H.numel() * H.element_size()  # Number of elements * bytes per element\n",
    "    V_mem = V.numel() * V.element_size()\n",
    "\n",
    "    # Memory for HV: (B, N, D, I)\n",
    "    HV_mem = B * N * D * I * H.element_size()\n",
    "\n",
    "    # Memory for interaction_matrix: (B, N, D, D, I)\n",
    "    interaction_matrix_mem = B * N * D * D * I * H.element_size()\n",
    "\n",
    "    # Memory for distance_weighted_interactions: (B, N, D, D, I)\n",
    "    distance_weighted_mem = B * N * D * D * I * H.element_size()\n",
    "\n",
    "    # Memory for raw_penalty and sum_interactions: both are (B, N, I)\n",
    "    raw_penalty_mem = B * N * I * H.element_size()\n",
    "    sum_interactions_mem = B * N * I * H.element_size()\n",
    "\n",
    "    # Total memory used\n",
    "    total_memory = (H_mem + V_mem + HV_mem + interaction_matrix_mem +\n",
    "                    distance_weighted_mem + raw_penalty_mem + sum_interactions_mem)\n",
    "\n",
    "    # Convert to megabytes (MB)\n",
    "    total_memory_MB = total_memory / (1024 ** 2)  # Convert from bytes to MB\n",
    "\n",
    "    return total_memory_MB\n",
    "\n",
    "calculate_memory_usage(hidden_state_pre_mlp, vmat_weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36f6b6-b039-461f-b0e6-e157e38c961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_L1_loss_chunked(H, V, chunk_size=64):\n",
    "    \"\"\"\n",
    "    Calculate the normalized L1 loss for a batch of input matrices H (B x N x D)\n",
    "    and a learnable weight matrix V (D x I), using chunking to reduce memory usage.\n",
    "    \n",
    "    Arguments:\n",
    "    H -- Input matrix of shape (B x N x D)\n",
    "    V -- Learnable weight matrix of shape (D x I)\n",
    "    chunk_size -- The number of dimensions to process at once to save memory.\n",
    "    \n",
    "    Returns:\n",
    "    Loss value -- Scalar representing the normalized L1 loss\n",
    "    \"\"\"\n",
    "    B, N, D = H.shape  # B = batch size, N = token length, D = dimension\n",
    "    I = V.shape[1]     # I = number of outputs (same as number of columns in V)\n",
    "\n",
    "    # Precompute the distance matrix for dimensions D x D\n",
    "    distance_matrix = torch.abs(torch.arange(D).unsqueeze(0) - torch.arange(D).unsqueeze(1)).to(H.device)  # Shape (D, D)\n",
    "\n",
    "    total_loss = 0.0  # Accumulate loss across batches and tokens\n",
    "\n",
    "    # Chunking the dimension D\n",
    "    for d_start in range(0, D, chunk_size):\n",
    "        print(d_start)\n",
    "        d_end = min(d_start + chunk_size, D)\n",
    "\n",
    "        # Extract the chunk of H and V\n",
    "        H_chunk = H[:, :, d_start:d_end]  # Shape (B, N, chunk_size)\n",
    "        V_chunk = V[d_start:d_end, :]     # Shape (chunk_size, I)\n",
    "        distance_chunk = distance_matrix[d_start:d_end, d_start:d_end]  # Shape (chunk_size, chunk_size)\n",
    "\n",
    "        # (1) Compute the matrix product H[b, n, d] * V[d, i] for each b, n, and i\n",
    "        HV_chunk = torch.einsum('bnd,di->bndi', H_chunk, V_chunk)  # Shape (B, N, chunk_size, I)\n",
    "\n",
    "        # (2) Compute pairwise interactions between different dimensions within the chunk\n",
    "        interaction_matrix = HV_chunk.unsqueeze(3) * HV_chunk.unsqueeze(2)  # Shape (B, N, chunk_size, chunk_size, I)\n",
    "\n",
    "        # (3) Apply the distance matrix to weigh interactions\n",
    "        distance_weighted_interactions = interaction_matrix * distance_chunk.unsqueeze(-1)  # Shape (B, N, chunk_size, chunk_size, I)\n",
    "\n",
    "        # (4) Sum over the chunked interaction matrix\n",
    "        raw_penalty = torch.sum(distance_weighted_interactions, dim=(2, 3))  # Shape (B, N, I)\n",
    "        sum_interactions = torch.sum(interaction_matrix, dim=(2, 3))  # Shape (B, N, I)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        sum_interactions = torch.clamp(sum_interactions, min=1e-8)\n",
    "        \n",
    "        # Compute the normalized penalty\n",
    "        normalized_penalty = raw_penalty / sum_interactions\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += normalized_penalty.sum()  # Sum over all output dimensions I\n",
    "\n",
    "    # Return the averaged loss across batches, tokens, and outputs\n",
    "    return total_loss / (B * N * I)\n",
    "\n",
    "normalized_L1_loss_chunked(hidden_state_pre_mlp, vmat_weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88feb7d3-1bb5-48e4-9764-3e68382de594",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_memory_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
