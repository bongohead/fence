{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed50fd6-302d-482e-8936-27511f12aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decompose Phi-3 into constituent parts. Runs each part peice by piece. Test logit lens, position loss, and feature clustering via the MLP layer.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9585a2aa-4381-4732-ae84-8a4786fe3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML, Markdown\n",
    "import plotly.express as px \n",
    "import os\n",
    "import wandb \n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib\n",
    "import pathlib\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from helpers.phi3.phi3 import Phi3Config, Phi3ForCausalLM, _prepare_4d_causal_attention_mask\n",
    "from helpers.phi3.parse import parse_phi\n",
    "from helpers.memory import check_memory\n",
    "\n",
    "load_dotenv('secrets.env')\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09c855-42cd-49f4-bc72-e4a44bbddb10",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb52446-f4a9-42e5-b896-6d25bde904f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_implementation = None # None/flash_attention_2\n",
    "\n",
    "# Load Model\n",
    "# Padding side not important EXCEPT for flash attention, needs to be left\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-4k-instruct', add_eos_token = False, add_bos_token = False, padding_side = 'left') \n",
    "\n",
    "# Load the usual model from HF transformers - attn_implementation = None to disable flash attention\n",
    "my_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'microsoft/Phi-3-mini-4k-instruct',\n",
    "    device_map = device,\n",
    "    trust_remote_code = True, \n",
    "    torch_dtype = torch.bfloat16, \n",
    "    attn_implementation = attn_implementation\n",
    "    ).to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f486b8-1676-45af-9eea-643fdc9f6982",
   "metadata": {},
   "source": [
    "## Initialize FENCE Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a78ab15-a163-4c20-a8a2-5bcef3ceeee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass indices starting at 1\n",
    "fence_dict = {\n",
    "    'dogs': (3064, 3072),\n",
    "    'test': (3050, 3052)\n",
    "}\n",
    "\n",
    "Kfstart = 1\n",
    "Kfend = 32\n",
    "Kf_target_values = {\n",
    "    'hkrs': {Kfstart + j - 1: (j - 1) * .25 + .25/2 for j in range(Kfstart, Kfend + 1)},\n",
    "    'hks': {Kfstart + j - 1: (j - 1) * .25 + .25 for j in range(Kfstart, Kfend + 1)},\n",
    "}\n",
    "\n",
    "print(fence_dict)\n",
    "Kf_target_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202c2d4-0e95-4603-862f-f6c7df219d45",
   "metadata": {},
   "source": [
    "## Test Inference & Visualizations with Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76e7ac-3146-4aba-9379-5f7a4652e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.eval'))\n",
    "from helpers.fence.eval import generate_fence\n",
    "\n",
    "# Test\n",
    "dog_prompt = parse_phi([{'role': 'user', 'content': 'What\\'s your favorite animal?'}], True)\n",
    "dog_gens = generate_fence(my_model, tokenizer, prompt = dog_prompt, max_tokens = 12)\n",
    "\n",
    "animal_prompt = '<s>Animals are,'\n",
    "animal_gens = generate_fence(my_model, tokenizer, prompt = animal_prompt, max_tokens = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b57ce7-1059-45df-8126-d7d818e46961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.visualize'))\n",
    "from helpers.fence.visualize import visualize_fence, visualize_fence_by_layer, visualize_fence_differentials\n",
    "\n",
    "for l in [1, 15, 30]:\n",
    "    visualize_fence(\n",
    "        dog_gens['text'],\n",
    "        dog_gens['hks'],\n",
    "        [l],\n",
    "        fence_dict,\n",
    "        start_dim = 2950, end_dim = 3072,\n",
    "        min_range = 0, max_range = Kf_target_values['hks'][l]\n",
    "    ).update_layout(title = 'H<sup>R</sup><sub>' + str(l) + '</sub>', height = 300).show('colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d6645f-3357-41b2-8b57-c2b05aa9cf83",
   "metadata": {},
   "source": [
    "## Test Component-by-Component Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005fbbc-8f14-4656-9582-2268b0c3ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.misc import is_notebook\n",
    "from helpers.phi3.phi3 import _prepare_4d_causal_attention_mask, apply_rotary_pos_emb\n",
    "import math\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_fence_with_force(model, tokenizer, prompt, echo_output = True, max_tokens = 128, device = 'cuda'):\n",
    "    \"\"\"\n",
    "    Runs a forward pass and stores FENCE-relevant intermediate hidden states. Allows for forced-FENCE (see foward pass code).\n",
    "    Also calculates the modularity loss. Position loss is NOT calculated as there are no true-FENCE states.\n",
    "\n",
    "    Returns a dictionary with keys:\n",
    "        - `text`: The decoded output text, as a list\n",
    "        - `hk1s`: The first residual stream output\n",
    "        - `hk2s`: The final residual stream output\n",
    "        - `hksas`: The hidden state outputs of the SA component\n",
    "        - `hkmlps`: The hidden state outputs of the MLP component\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated_tokens = 0\n",
    "    \n",
    "    input_ids_0 = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']\n",
    "    input_ids = input_ids_0\n",
    "\n",
    "    while True:\n",
    "        embeds_output = model.model.embed_tokens(input_ids)\n",
    "        hidden_state = embeds_output\n",
    "        \n",
    "        B, N, D = embeds_output.shape\n",
    "        H = 32\n",
    "        Dh = int(D/H)\n",
    "        \n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        \n",
    "        # Flash attention = use default attention mask 2d\n",
    "        if model.model._attn_implementation == 'flash_attention_2':\n",
    "            attention_mask = None\n",
    "        # Non flash-attention: Make a triangular attention mask to hide right context\n",
    "        else:\n",
    "            attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = model.model.config.sliding_window) \n",
    "\n",
    "        saved_sa_outputs = []\n",
    "        saved_hkrs = []\n",
    "        saved_mlp_outputs = []\n",
    "        saved_hks = []\n",
    "        ### Transformer Blocks ###\n",
    "        for i, layer in enumerate(model.model.layers):            \n",
    "\n",
    "            residual = hidden_state\n",
    "            sa_input = layer.input_layernorm(hidden_state)\n",
    "            \n",
    "            ### SA ###\n",
    "            sa_module = layer.self_attn\n",
    "            # sa_output = sa_module(sa_input, attention_mask, position_ids)[0]\n",
    "            qkv = sa_module.qkv_proj(sa_input)\n",
    "            queries = qkv[:, :, :D].view(B, N, H, Dh).transpose(1, 2)\n",
    "            keys = qkv[:, :, D:2*D].view(B, N, H, Dh).transpose(1, 2)\n",
    "            values = qkv[:, :, 2*D:].view(B, N, H, Dh).transpose(1, 2)\n",
    "\n",
    "            if model.model._attn_implementation == 'flash_attention_2':     \n",
    "                # Flash attention requires the input to have the shape B x N x Dh x D           \n",
    "                # Because the input can be padded, the absolute sequence length depends on the max position id.\n",
    "                rotary_seq_len = max(N, position_ids[:, -1].max().item()) + 1\n",
    "                cos, sin = sa_module.rotary_emb(values, position_ids, seq_len = rotary_seq_len)\n",
    "                queries, keys = apply_rotary_pos_emb(queries, keys, cos, sin, position_ids)\n",
    "                ################## # Reshape to the expected shape for Flash Attention\n",
    "                queries = queries.transpose(1, 2)\n",
    "                keys = keys.transpose(1, 2)\n",
    "                values = values.transpose(1, 2)\n",
    "                ###################\n",
    "                sa_output = sa_module._flash_attention_forward(queries, keys, values, attention_mask, N)\n",
    "                sa_output = sa_output.reshape(B, N, D).contiguous()\n",
    "            else:    \n",
    "                cos, sin = sa_module.rotary_emb(values, position_ids, seq_len = N)\n",
    "                queries, keys = apply_rotary_pos_emb(queries, keys, cos, sin, position_ids)\n",
    "                attn_weights = torch.matmul(queries, keys.transpose(2, 3))/math.sqrt(Dh)  # Should be shape B x H x N x N\n",
    "                attn_weights = attn_weights + attention_mask # Attemtion mask is upper triangular of negative infinity\n",
    "                attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(values.dtype)\n",
    "                sa_output = torch.matmul(attn_weights, values) # B x H x N x D/H\n",
    "                sa_output = sa_output.transpose(1, 2).contiguous() # Reorder into B x N x H x D/H\n",
    "                sa_output = sa_output.reshape(B, N, D) # Concatenate vertically back into B x N x D\n",
    "    \n",
    "            # Finally post-concatenation linear layer\n",
    "            sa_output = sa_module.o_proj(sa_output)\n",
    "\n",
    "            saved_sa_outputs.append(sa_output[0, :, :].detach())\n",
    "            \n",
    "            ### add residual -> store residual -> layernorm -> mlp -> add residual\n",
    "            hidden_state = residual + sa_output\n",
    "\n",
    "            # FENCE\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                # To extract the right layer from hkr_feature_targets:\n",
    "                # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "                # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "                # - => k = l + 1 - Kfstart\n",
    "                # hidden_state[:, 1:, 3064:3072] = Kf_target_values['hkrs'][l+1] # B x N x Df                \n",
    "                pass\n",
    "            \n",
    "            residual = hidden_state\n",
    "            saved_hkrs.append(hidden_state[0, :, :].detach())\n",
    "\n",
    "            hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "            ## MLP            \n",
    "            up_state = layer.mlp.gate_up_proj(hidden_state) # B x N x (2I, I = intermediate MLP dimension)\n",
    "            gate, up_state = up_state.chunk(2, dim = -1) # B x N x I\n",
    "            up_state = up_state * layer.mlp.activation_fn(gate)  # Elementwise\n",
    "            hidden_state = layer.mlp.down_proj(up_state) # Back to B x N x D\n",
    "            ## End MLP\n",
    "            \n",
    "            saved_mlp_outputs.append(hidden_state[0, :, :].detach())\n",
    "\n",
    "            hidden_state = residual + hidden_state\n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                # hidden_state[:, 1:, 3064:3072] = Kf_target_values['hks'][l+1] # B x N x Df\n",
    "                pass\n",
    "\n",
    "            saved_hks.append(hidden_state[0, :, :].detach())\n",
    "                \n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "        logits = model.lm_head(hidden_state)\n",
    "\n",
    "        # Get argmax tokens + concatenate onto previous tokens\n",
    "        output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "        input_ids = torch.cat((input_ids, output_token.view(1, 1)), dim = 1)\n",
    "\n",
    "        # Break while loop if EOS or generation > max tokens\n",
    "        generated_tokens = generated_tokens + 1\n",
    "        if output_token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end|>\")] or generated_tokens >= max_tokens:\n",
    "            break\n",
    "\n",
    "    # Use it on the last pass only\n",
    "    all_hksas = [h.cpu().to(torch.float16).numpy() for h in saved_sa_outputs]\n",
    "    all_hkrs = [h.cpu().to(torch.float16).numpy() for h in saved_hkrs]\n",
    "    all_hkmlps = [h.cpu().to(torch.float16).numpy() for h in saved_mlp_outputs]\n",
    "    all_hks = [h.cpu().to(torch.float16).numpy() for h in saved_hks]\n",
    "\n",
    "    final_output = input_ids.squeeze()\n",
    "    decoded_text = tokenizer.batch_decode(final_output)\n",
    "    decoded_output = tokenizer.decode(final_output[input_ids_0.size()[1]:])\n",
    "\n",
    "    if echo_output:\n",
    "        if is_notebook():\n",
    "            display(HTML(\n",
    "                '<div style=\"padding: 1rem 2rem; background-color:honeydew\">' + \n",
    "                    '<h4>Modified model output</h4>' + \n",
    "                    '<span style=\"color:green\">' + tokenizer.batch_decode(input_ids_0)[0][3:] + '</span> ' + \n",
    "                    '<span style=\"color:red\">' + decoded_output + '</span>' +\n",
    "                '</div>'\n",
    "            ))\n",
    "        else:\n",
    "            print(colored(tokenizer.batch_decode(input_ids_0)[0][3:], 'green'), colored(tokenizer.decode(final_output[input_ids_0.size()[1]:]), 'red'))\n",
    "\n",
    "generate_fence(my_model, tokenizer, prompt = dog_prompt, max_tokens = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a2d43-1656-4e63-8532-3b53949fa59a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "17fcc429-45b0-40a0-b3ff-844492534718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['As']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST - SINGLE PASS\n",
    "model = my_model\n",
    "prompt = dog_prompt\n",
    "\n",
    "model.eval()\n",
    "generated_tokens = 0\n",
    "\n",
    "input_ids_0 = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']\n",
    "input_ids = input_ids_0\n",
    "\n",
    "embeds_output = model.model.embed_tokens(input_ids)\n",
    "hidden_state = embeds_output\n",
    "\n",
    "B, N, D = embeds_output.shape\n",
    "H = 32\n",
    "Dh = int(D/H)\n",
    "\n",
    "position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "\n",
    "# Flash attention = use default attention mask 2d\n",
    "if model.model._attn_implementation == 'flash_attention_2':\n",
    "    attention_mask = None\n",
    "# Non flash-attention: Make a triangular attention mask to hide right context\n",
    "else:\n",
    "    attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = model.model.config.sliding_window) \n",
    "\n",
    "saved_sa_outputs = []\n",
    "saved_hkrs = []\n",
    "saved_mlp_outputs = []\n",
    "saved_hks = []\n",
    "### Transformer Blocks ###\n",
    "for i, layer in enumerate(model.model.layers):            \n",
    "\n",
    "    residual = hidden_state\n",
    "    sa_input = layer.input_layernorm(hidden_state)\n",
    "    \n",
    "    ### SA ###\n",
    "    sa_module = layer.self_attn\n",
    "    # sa_output = sa_module(sa_input, attention_mask, position_ids)[0]\n",
    "    qkv = sa_module.qkv_proj(sa_input)\n",
    "    queries = qkv[:, :, :D].view(B, N, H, Dh).transpose(1, 2)\n",
    "    keys = qkv[:, :, D:2*D].view(B, N, H, Dh).transpose(1, 2)\n",
    "    values = qkv[:, :, 2*D:].view(B, N, H, Dh).transpose(1, 2)\n",
    "\n",
    "    if model.model._attn_implementation == 'flash_attention_2':     \n",
    "        # Flash attention requires the input to have the shape B x N x Dh x D           \n",
    "        # Because the input can be padded, the absolute sequence length depends on the max position id.\n",
    "        rotary_seq_len = max(N, position_ids[:, -1].max().item()) + 1\n",
    "        cos, sin = sa_module.rotary_emb(values, position_ids, seq_len = rotary_seq_len)\n",
    "        queries, keys = apply_rotary_pos_emb(queries, keys, cos, sin, position_ids)\n",
    "        ################## # Reshape to the expected shape for Flash Attention\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        ###################\n",
    "        sa_output = sa_module._flash_attention_forward(queries, keys, values, attention_mask, N)\n",
    "        sa_output = sa_output.reshape(B, N, D).contiguous()\n",
    "    else:    \n",
    "        cos, sin = sa_module.rotary_emb(values, position_ids, seq_len = N)\n",
    "        queries, keys = apply_rotary_pos_emb(queries, keys, cos, sin, position_ids)\n",
    "        attn_weights = torch.matmul(queries, keys.transpose(2, 3))/math.sqrt(Dh)  # Should be shape B x H x N x N\n",
    "        attn_weights = attn_weights + attention_mask # Attemtion mask is upper triangular of negative infinity\n",
    "        attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(values.dtype)\n",
    "        sa_output = torch.matmul(attn_weights, values) # B x H x N x D/H\n",
    "        sa_output = sa_output.transpose(1, 2).contiguous() # Reorder into B x N x H x D/H\n",
    "        sa_output = sa_output.reshape(B, N, D) # Concatenate vertically back into B x N x D\n",
    "\n",
    "    # Finally post-concatenation linear layer\n",
    "    sa_output = sa_module.o_proj(sa_output)\n",
    "\n",
    "    saved_sa_outputs.append(sa_output[0, :, :].detach())\n",
    "    \n",
    "    ### add residual -> store residual -> layernorm -> mlp -> add residual\n",
    "    hidden_state = residual + sa_output\n",
    "\n",
    "    # FENCE\n",
    "    if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "        # To extract the right layer from hkr_feature_targets:\n",
    "        # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "        # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "        # - => k = l + 1 - Kfstart\n",
    "        # hidden_state[:, 1:, 3064:3072] = Kf_target_values['hkrs'][l+1] # B x N x Df                \n",
    "        pass\n",
    "    \n",
    "    residual = hidden_state\n",
    "    saved_hkrs.append(hidden_state[0, :, :].detach())\n",
    "\n",
    "    hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "    hidden_state_pre_mlp = hidden_state\n",
    "    ## MLP       \n",
    "    # hidden_state is of size B x N x D\n",
    "    gate_plus_fvals = layer.mlp.gate_up_proj(hidden_state) # B x N x (2I, I = intermediate MLP dimension)\n",
    "    gate, fvals = gate_plus_fvals.chunk(2, dim = -1) # B x N x I\n",
    "    # At this point the up_state = values (see Geva et al), and the gate is the keys\n",
    "    up_state = fvals * layer.mlp.activation_fn(gate)  # Elementwise\n",
    "    hidden_state = layer.mlp.down_proj(up_state) # Back to B x N x D\n",
    "    ## End MLP\n",
    "    \n",
    "    saved_mlp_outputs.append(hidden_state[0, :, :].detach())\n",
    "\n",
    "    hidden_state = residual + hidden_state\n",
    "\n",
    "    if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "        # hidden_state[:, 1:, 3064:3072] = Kf_target_values['hks'][l+1] # B x N x Df\n",
    "        pass\n",
    "\n",
    "    saved_hks.append(hidden_state[0, :, :].detach())\n",
    "        \n",
    "hidden_state = model.model.norm(hidden_state)\n",
    "logits = model.lm_head(hidden_state)\n",
    "\n",
    "# Get argmax tokens + concatenate onto previous tokens\n",
    "output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "input_ids = torch.cat((input_ids, output_token.view(1, 1)), dim = 1)\n",
    "\n",
    "\n",
    "tokenizer.batch_decode([output_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "615d26a7-1300-4644-8dc3-73d8c17593a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1992, -0.1187, -0.5352,  ..., -0.2754, -1.1641,  0.6445],\n",
       "         [-0.1816,  1.8359, -0.9102,  ...,  0.3574, -1.8594,  1.2656],\n",
       "         [-0.1953,  2.0156,  0.2422,  ...,  1.1172, -0.3496,  0.7734],\n",
       "         ...,\n",
       "         [ 0.1133,  1.1875, -0.6523,  ...,  0.1035, -1.4922,  1.1797],\n",
       "         [-1.1797,  1.8906, -2.5156,  ...,  0.8359, -1.4609, -1.4844],\n",
       "         [-1.1250,  2.5312, -2.1094,  ...,  0.2217, -1.1172, -0.2129]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SplitBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "45423f55-dd88-45d8-a8e8-cba6a5b16786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3145, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_modularity_loss(hidden_state, fvals):\n",
    "    \n",
    "    B, N, D = hidden_state.shape  # Batch size, Sequence length, Feature dimension\n",
    "    _, _, I = fvals.shape  # Intermediate dimension\n",
    "\n",
    "    # Element-wise products in the interaction space (sequence-wise)\n",
    "    # We need to compute: abs(h_i * h_k * v_ij * v_kj) but only for sequence positions, not their feature products directly\n",
    "\n",
    "    # First, compute outer products of sequence elements' activations in V space (N x N interactions for each batch):\n",
    "    # B x N x 1 x I\n",
    "    fvals_exp = fvals.unsqueeze(2)  \n",
    "    # B x 1 x N x I\n",
    "    fvals_exp_k = fvals.unsqueeze(1)\n",
    "\n",
    "    # Calculate the element-wise absolute products\n",
    "    interaction_terms = torch.abs(fvals_exp * fvals_exp_k)  # B x N x N x I\n",
    "\n",
    "    # Now, sum across the interaction dimension I for each pair (i, k):\n",
    "    interaction_sums = interaction_terms.sum(dim=-1)  # B x N x N\n",
    "\n",
    "    # Calculate |i - k| using broadcasting\n",
    "    i_indices = torch.arange(N).view(1, -1, 1)  # 1 x N x 1\n",
    "    k_indices = torch.arange(N).view(1, 1, -1)  # 1 x 1 x N\n",
    "    distances = torch.abs(i_indices - k_indices)  # N x N\n",
    "    distances = distances.to(hidden_state.device)  # Ensure distances are on the same device\n",
    "\n",
    "    # Broadcast distances to match interaction terms\n",
    "    distances_exp = distances.unsqueeze(0)  # 1 x N x N\n",
    "\n",
    "    # Calculate the raw L1 norm loss: multiply the interactions by their distances\n",
    "    L1_raw = (interaction_sums * distances_exp).sum(dim=(1, 2))  # Sum over N x N\n",
    "\n",
    "    # Calculate the sum of interactions for normalization\n",
    "    sum_interactions = interaction_sums.sum(dim=(1, 2))  # Sum over N x N\n",
    "\n",
    "    # Normalize the loss, handling division by zero\n",
    "    L1_norm_loss = torch.where(sum_interactions != 0, L1_raw / sum_interactions, torch.tensor(0.0).to(hidden_state.device))\n",
    "\n",
    "    # Average loss across the batch\n",
    "    return L1_norm_loss.mean()\n",
    "\n",
    "get_modularity_loss(hidden_state_pre_mlp, fvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "734d5480-20c5-4cac-955a-d6b28fe366c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 3072])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state_pre_mlp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae514c0d-176a-49af-a42d-66e4caafd2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
