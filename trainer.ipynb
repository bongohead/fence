{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb0bcec-cadd-4ec1-8294-5a57b77785c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "V5 experiments:\n",
    "- Add in MLP value decay based off distance matrix\n",
    "- Training with cleaner DS\n",
    "- Integrate layer-wise logit lens\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a93d9e37-2fbc-49f6-9c89-78be8199302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML, Markdown\n",
    "import plotly.express as px\n",
    "import os\n",
    "import wandb \n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib\n",
    "import pathlib\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from py_helpers.fence.phi3 import Phi3Config, Phi3ForCausalLM, _prepare_4d_causal_attention_mask\n",
    "from py_helpers.fence.dataset import parse_phi, FenceDataSet\n",
    "from py_helpers.memory import check_memory\n",
    "\n",
    "load_dotenv('secrets.env')\n",
    "device = 'cuda'\n",
    "\n",
    "RUN_ID = f\"{datetime.now(pytz.timezone('US/Eastern')).strftime('%Y%m%dT%H%M')}\"\n",
    "SAVE_DIR = f\"./models/{RUN_ID}\"\n",
    "USE_WANDB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ac28d-28f8-4fd8-a42e-5e2db1859d8a",
   "metadata": {},
   "source": [
    "## Initialize Run Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741df29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    'angry': 8,\n",
    "    'math': 8,\n",
    "    'dog': 8\n",
    "}\n",
    "Kfstart = 1\n",
    "Kfend = 32\n",
    "Kf_target_values = {\n",
    "    # 'hkrs': {Kfstart + j - 1: (j - 1) * .125 + .125/2 for j in range(Kfstart, Kfend + 1)},\n",
    "    # 'hks': {Kfstart + j - 1: (j - 1) * .125 + .125 for j in range(Kfstart, Kfend + 1)},\n",
    "    'hkrs': {Kfstart + j - 1: (j - 1) * .25 + .25/2 for j in range(Kfstart, Kfend + 1)},\n",
    "    'hks': {Kfstart + j - 1: (j - 1) * .25 + .25 for j in range(Kfstart, Kfend + 1)},\n",
    "}\n",
    "\n",
    "Kf_target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0e751c-7557-4647-a01b-f301e1aae180",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(SAVE_DIR).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "if USE_WANDB:\n",
    "    \n",
    "    os.environ['WANDB_INIT_TIMEOUT'] = '120'\n",
    "    wandb.login(key = os.getenv('WANDB_API_KEY'))\n",
    "    run = wandb.init(\n",
    "        project = 'fence_v4', \n",
    "        name = RUN_ID,\n",
    "        notes = '',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3714e",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7755f5-0c64-4d5f-9df4-5de05463ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_implementation = 'flash_attention_2' # None/flash_attention_2\n",
    "\n",
    "# Load Model\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-4k-instruct', add_eos_token = False, add_bos_token = False, padding_side = 'left') # Padding side not important EXCEPT for flash attention, needs to be left\n",
    "\n",
    "# Load the usual model from HF transformers - attn_implementation = None to disable flash attention\n",
    "my_model = AutoModelForCausalLM.from_pretrained('microsoft/Phi-3-mini-4k-instruct', device_map = device, trust_remote_code = True, torch_dtype = torch.bfloat16, attn_implementation = attn_implementation).to(device).eval()\n",
    "\n",
    "# Now load a model seperately from the underlying model object code\n",
    "# my_model = Phi3ForCausalLM(base_model.config).to(device).eval().to(dtype = torch.bfloat16) # Phi3Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a071720b-7619-4494-9259-84d7dda29af4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Check everything is bfloat16\n",
    "# for p in base_model.parameters():\n",
    "#     print(p.dtype)\n",
    "\n",
    "# # Check attention implementation\n",
    "# my_model.model.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad3756-c2c2-4f36-8e4f-1185e994ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Next, we want to clone params from base_model into model\n",
    "# # Let's store all params from the base_model\n",
    "# all_params = {}\n",
    "# for name, param in base_model.named_parameters():\n",
    "#     all_params[name] = param.cpu().clone()\n",
    "\n",
    "# # Then copy them over to the new model\n",
    "# for name, param in my_model.named_parameters():\n",
    "#     param.data.copy_(all_params[name].data)\n",
    "\n",
    "# # Verify these are the same\n",
    "# for name, p in my_model.named_parameters():\n",
    "#     if name == 'model.embed_tokens.weight': \n",
    "#         print(p)\n",
    "# for name, p in base_model.named_parameters():\n",
    "#     if name == 'model.embed_tokens.weight': \n",
    "#         print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c3278-1aa0-40d6-abc6-635a1eda15a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_memory()\n",
    "# # Need to delete ALl references to original model to clear memory properly https://discuss.pytorch.org/t/cuda-memory-not-released-by-torch-cuda-empty-cache/129913/6\n",
    "# if 'base_model' in globals():\n",
    "#     del base_model\n",
    "# if 'name' in globals():\n",
    "#     del name\n",
    "# if 'param' in globals():\n",
    "#     del param\n",
    "# if 'p' in globals():\n",
    "#     del p\n",
    "# if 'all_params' in globals():\n",
    "#     del all_params\n",
    "    \n",
    "# torch.cuda.empty_cache()\n",
    "# check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee225f-dc3f-4261-aa65-6dba823314dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(my_model.state_dict(), f'./models/phi3_base.pt')\n",
    "# base_model.load_state_dict(torch.load('./models/FENCEV2-20240619T0813/e2.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9272b-7bdf-44a5-be55-7cedc7838fc4",
   "metadata": {},
   "source": [
    "## Test Inference & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59244cd-a37d-4c41-847a-5ed37e398499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_fence\n",
    "importlib.reload(importlib.import_module('py_helpers.fence.eval'))\n",
    "from py_helpers.fence.eval import generate_fence\n",
    "\n",
    "# Test\n",
    "test_prompts = [\n",
    "    '<s>Animals are multicellular, eukaryotic organisms in the biological kingdom Animalia. With few',\n",
    "    parse_phi([{'role': 'user', 'content': 'What did you do today?'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'What should I bring to take my dog hiking?'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'What should I bring to take my friend hiking?'}], True)\n",
    "]\n",
    "\n",
    "test_gens = generate_fence(my_model, tokenizer, prompt = test_prompts[0], max_tokens = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4d6e9-af60-4bde-b2c9-ef837c719f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose where the classification is happening\n",
    "importlib.reload(importlib.import_module('py_helpers.fence.visualize'))\n",
    "from py_helpers.fence.visualize import visualize_fence, visualize_fence_by_layer, visualize_fence_differentials\n",
    "\n",
    "for l in [1, 10, 20, 30]:\n",
    "    #visualize_fence(test_gens['text'], test_gens['hksas'], [l], feature_dict, 2950, -1, 1).update_layout(title = 'H<sup>SA</sup><sub>' + str(l) + '</sub>', height = 300).show()\n",
    "    visualize_fence(test_gens['text'], test_gens['hkrs'], [l], feature_dict, 3000, -1, 1).update_layout(title = 'H<sup>1</sup><sub>' + str(l) + '</sub>', height = 500).show('colab')\n",
    "    #visualize_fence(test_gens['text'], test_gens['hkmlps'], [l], feature_dict, 2950, -1, 1).update_layout(title = 'H<sup>MLP</sup><sub>' + str(l) + '</sub>', height = 300).show()\n",
    "    visualize_fence(test_gens['text'], test_gens['hks'], [l], feature_dict, 3000, -1, 1).update_layout(title = 'H<sub>' + str(l) + '</sub>', height = 400).show('colab')\n",
    "\n",
    "# visualize_fence(test_gens['text'], test_gens['hk2s'], [15], feature_dict, 2950).show()\n",
    "# visualize_fence(test_gens['text'], test_gens['hksas'], [15], feature_dict, 2950).show()\n",
    "# visualize_fence(test_gens['text'], test_gens['hkmlps'], [15], feature_dict, 2950, 0, 1).show()\n",
    "# visualize_fence_by_layer(test_gens['text'], test_gens['hk2s'], [1, 5, 10, 15], feature_dict, 2950, 0, 1).show()\n",
    "# visualize_fence_differentials(test_gens['text'], test_gens['hks'], [1, 2], feature_dict, 2950, 0, 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bb5dc5-4496-40f0-ab22-51f853705d04",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61595b92-82f3-4ba8-8fcf-ce8ddaf60aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('train.csv')\n",
    "train_nosup_raw = train_raw[train_raw['is_surprise'] == 0] # No-surprise is used for position-loss trainintg\n",
    "test_raw = pd.read_csv('test.csv')\n",
    "print(len(train_raw), len(train_nosup_raw), len(test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113957d6-b147-44f5-b564-b96c4fd28666",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_classifications = train_raw[feature_dict.keys()].to_dict('records')\n",
    "train_nosup_feature_classifications = train_nosup_raw[feature_dict.keys()].to_dict('records')\n",
    "test_feature_classifications = test_raw[feature_dict.keys()].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ed90f-c7c1-4c60-a88c-377b4fed5bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('py_helpers.fence.dataset'))\n",
    "from py_helpers.fence.dataset import parse_phi, FenceDataSet\n",
    "token_length = 1024\n",
    "\n",
    "tmp_tokens_len_test = [tokenizer(x, return_tensors = 'pt').to(device) for x in train_raw['phi3_text'].tolist()]\n",
    "px.histogram(pd.DataFrame({\"j\": [t['input_ids'].shape[1] for t in tmp_tokens_len_test]}), x = \"j\").show('colab')\n",
    "\n",
    "train_tokens = tokenizer(train_raw['phi3_text'].tolist(), truncation = True, max_length = token_length, padding = 'max_length', return_tensors = 'pt').to(device)\n",
    "train_nosup_tokens = tokenizer(train_nosup_raw['phi3_text'].tolist(), truncation = True, max_length = token_length, padding = 'max_length', return_tensors = 'pt').to(device)\n",
    "test_tokens = tokenizer(test_raw['phi3_text'].tolist(), truncation = True, max_length = token_length, padding = 'max_length', return_tensors = 'pt').to(device)\n",
    "\n",
    "position_mask_start_token_id = tokenizer.encode('<s>')[0]\n",
    "train_ds = FenceDataSet(train_tokens, feature_dict, train_feature_classifications, position_mask_start_token_id)\n",
    "train_nosup_ds = FenceDataSet(train_nosup_tokens, feature_dict, train_nosup_feature_classifications, position_mask_start_token_id)\n",
    "test_ds = FenceDataSet(test_tokens, feature_dict, test_feature_classifications, position_mask_start_token_id)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size = 10, shuffle = True)\n",
    "train_nosup_dl = DataLoader(train_nosup_ds, batch_size = 10, shuffle = True)\n",
    "test_dl = DataLoader(test_ds, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5272e4-6448-403f-9e2c-58d52e1f7ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Don't train embeddings/lm head/RMSnorm, only parts within transformer blocks\n",
    "for name, param in my_model.named_parameters():\n",
    "    if 1 == 2: #'model.norm' in name or 'lm_head' in name: #\"embed_tokens\", \"model.norm\", \"lm_head\", \"layernorm\" in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for name, param in my_model.named_parameters():\n",
    "    if 'layers' not in name or '.0.' in name:\n",
    "        print(name, param.requires_grad)\n",
    "\n",
    "del name\n",
    "del param\n",
    "\n",
    "check_memory()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fa42cd-f692-4136-a214-91945d0cd7ca",
   "metadata": {},
   "source": [
    "## Training Testing - Single Input + No Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0bc791-3f85-458c-bd34-faf0fd5a2644",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set single batch\n",
    "for i, batch in enumerate(train_dl):\n",
    "    if i > 1: break\n",
    "\n",
    "input_ids = batch['input_ids']\n",
    "mask = batch['attention_mask']\n",
    "feature_targets = batch['feature_targets']\n",
    "position_mask = batch['position_mask']\n",
    "\n",
    "print(input_ids, mask, feature_targets, position_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83871f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training R1: Force FENCE (i.e., no Position Loss)\n",
    "model = my_model\n",
    "force_fence = False\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    ##### Forward Pass ######\n",
    "    embeds_output = model.model.embed_tokens(input_ids) # B x N x D\n",
    "    hidden_state = embeds_output\n",
    "\n",
    "    # Execute transformers layers\n",
    "    # B = batch size, N = token length, D = token dim, Dh = token per-head dim, H = number of heads, K = # transformer blocks\n",
    "    # Df = total FENCE dimensino width\n",
    "    # Kfstart, Kfend = starting and ending indices for transformer blocks to include in FENCE (indices starts with 1, not 0)\n",
    "    # Kf = number of transformer blocks to include in FENCE\n",
    "    B, N, D = embeds_output.shape\n",
    "    H = 32\n",
    "    Dh = int(D/H)\n",
    "    K = 32 \n",
    "\n",
    "    Df = feature_targets.shape[1] # Total FENCE width\n",
    "    Kf = Kfend - Kfstart + 1\n",
    "\n",
    "    # Prepare SA inputs\n",
    "    position_ids = torch.arange(0, N, dtype = torch.long, device = device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "    if model.model._attn_implementation == 'flash_attention_2':\n",
    "        attention_mask = mask if (mask is not None and 0 in mask) else None  # Flash attention = use default attention mask 2d\n",
    "    else: \n",
    "        attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = model.model.config.sliding_window)  # Non FA: Make a triangular attention mask to hide right context\n",
    "\n",
    "    # Create Hkr and Hk target values (hkr = .25, .75, 1.25, hk = .5, 1, 1.5) when feature_target = 1\n",
    "    hkr_target_values = torch.tensor(list(Kf_target_values['hkrs'].values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "    hk_target_values = torch.tensor(list(Kf_target_values['hks'].values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "\n",
    "    # Multiply it by the actual feature targets by layer\n",
    "    feature_targets_bkd = feature_targets.unsqueeze(1) # B x K x Df\n",
    "    hkr_feature_targets = hkr_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "    hkr_feature_targets = hkr_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "    hk_feature_targets = hk_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "    hk_feature_targets = hk_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "\n",
    "    # Saved_hk2s will be of shape B x K x N x Df\n",
    "    saved_hkrs = None\n",
    "    saved_hks = None\n",
    "    for l, layer in enumerate(model.model.layers):\n",
    "        \n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        sa_input = layer.input_layernorm(hidden_state)\n",
    "        sa_output = layer.self_attn(sa_input, attention_mask, position_ids)[0]\n",
    "        \n",
    "        # Sum back to resid stream\n",
    "        hidden_state = residual + layer.resid_attn_dropout(sa_output)    \n",
    "\n",
    "        if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "            if force_fence: # Forcibly set H_K^R\n",
    "                # To extract the right layer from hkr_feature_targets:\n",
    "                # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "                # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "                # - => k = l + 1 - Kfstart\n",
    "                hidden_state[:, :, -Df: ] = hkr_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df\n",
    "\n",
    "            this_hidden_state = hidden_state[:, :, -Df:].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "            if saved_hkrs is None:\n",
    "                saved_hkrs = this_hidden_state\n",
    "            else:\n",
    "                saved_hkrs = torch.cat((saved_hkrs, this_hidden_state), dim = 1)\n",
    "\n",
    "        # MLP\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "        mlp_output = layer.mlp(hidden_state)\n",
    "\n",
    "        # Sum back to resid stream\n",
    "        hidden_state = residual + layer.resid_mlp_dropout(mlp_output)\n",
    "\n",
    "        if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "            if force_fence: # Forcibly set H_K\n",
    "                hidden_state[:, :, -Df: ] = hk_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df\n",
    "\n",
    "            this_hidden_state = hidden_state[:, :, -Df:].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "            if saved_hks is None:\n",
    "                saved_hks = this_hidden_state\n",
    "            else:\n",
    "                saved_hks = torch.cat((saved_hks, this_hidden_state), dim = 1)\n",
    "                \n",
    "\n",
    "    # RMS norm the final transformer layer output\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "\n",
    "    # Run LM head\n",
    "    logits = model.lm_head(hidden_state).float() # B x N x D\n",
    "    #### End Forward Pass ######\n",
    "\n",
    "    ##### Calculate loss #####\n",
    "    # Mask loss anywhere where the input ids are pad tokens\n",
    "    label_ids = torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids)\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous() # Remove the last token from the sequence\n",
    "    shift_labels = label_ids[..., 1:].contiguous() # Remove the first token from the sequence\n",
    "    # Flatten tokens\n",
    "    loss_fct = CrossEntropyLoss(ignore_index = -100)\n",
    "    shift_logits = shift_logits.view(-1, model.config.vocab_size)\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    # Enable model parallelism\n",
    "    # shift_labels = shift_labels.to(device)\n",
    "    base_loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "    # Calculate the position loss\n",
    "    # Apply the position target mask\n",
    "    position_mask_bknd = position_mask.unsqueeze(1).unsqueeze(-1).expand(B, Kf, N, Df) # B x K X N x Df (recycles across K and Df)\n",
    "\n",
    "    position_loss_hkrs = torch.where(position_mask_bknd == 1, torch.abs((saved_hkrs - hkr_feature_targets)/(hkr_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "    position_loss_hks = torch.where(position_mask_bknd == 1, torch.abs((saved_hks - hk_feature_targets)/(hk_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "    \n",
    "    position_loss_hkrs_by_k = position_loss_hkrs.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "    position_loss_hks_by_k = position_loss_hks.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "    \n",
    "    position_loss_hkrs_by_dim = position_loss_hkrs.sum(dim = (0, 1, 2))/position_mask_bknd.sum(dim = (0, 1, 2))\n",
    "    position_loss_hks_by_dim = position_loss_hks.sum(dim = (0, 1, 2))/position_mask_bknd.sum(dim = (0, 1, 2))\n",
    "\n",
    "    position_loss_hkrs = position_loss_hkrs.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "    position_loss_hks = position_loss_hks.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "    \n",
    "    # # Add additional hinge penalty to disproportionately penalize values with abs value outside desired range\n",
    "    # hinge_loss = (torch.clamp(saved_l2s, max = 0) - 0) ** 2 + (torch.clamp(saved_l2s, min = 1) - 1) ** 2 \n",
    "    # hinge_loss = torch.where(l2_mask == 1, hinge_loss, torch.tensor(0.0)).sum()/l2_mask.sum() \n",
    "\n",
    "    loss = base_loss + position_loss_hks + position_loss_hkrs\n",
    "    ##### End loss calcaulation ######\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "del model\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb802616-03bf-40e2-831c-8f7415baabb7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Old Version W/O HKRs\n",
    "# model = my_model\n",
    "\n",
    "# ##### Forward Pass ######\n",
    "# embeds_output = model.model.embed_tokens(input_ids) # B x N x D\n",
    "# hidden_state = embeds_output\n",
    "\n",
    "# # Execute transformers layers\n",
    "# # B = batch size, N = token length, D = token dim, Dh = token per-head dim, H = number of heads, Df = total fence width, kend = 30, kstart = 1\n",
    "# B, N, D = embeds_output.shape\n",
    "# H = 32\n",
    "# Dh = int(D/H)\n",
    "# Df = feature_targets.shape[1] # Total FENCE width\n",
    "# kstart = 1\n",
    "# kend = 30\n",
    "# K = kend - kstart + 1\n",
    "\n",
    "# position_ids = torch.arange(0, N, dtype = torch.long, device = device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "\n",
    "# if model.model._attn_implementation == 'flash_attention_2':\n",
    "#     attention_mask = mask if (mask is not None and 0 in mask) else None  # Flash attention = use default attention mask 2d\n",
    "# else: \n",
    "#     attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = model.model.config.sliding_window)  # Non FA: Make a triangular attention mask to hide right context\n",
    "\n",
    "\n",
    "# saved_hk2s = None\n",
    "# for l, layer in enumerate(model.model.layers):            \n",
    "#     residual = hidden_state\n",
    "#     sa_input = layer.input_layernorm(hidden_state)\n",
    "\n",
    "#     ### SA ###\n",
    "#     sa_module = layer.self_attn\n",
    "#     qkv = sa_module.qkv_proj(sa_input)\n",
    "#     queries = qkv[:, :, :D].view(B, N, H, Dh).transpose(1, 2)\n",
    "#     keys = qkv[:, :, D:2*D].view(B, N, H, Dh).transpose(1, 2)\n",
    "#     values = qkv[:, :, 2*D:].view(B, N, H, Dh).transpose(1, 2)\n",
    "\n",
    "#     if model.model._attn_implementation == 'flash_attention_2':\n",
    "#         # Because the input can be padded, the absolute sequence length depends on the max position id.\n",
    "#         rotary_seq_len = max(N, position_ids[:, -1].max().item()) + 1\n",
    "#         cos, sin = sa_module.rotary_emb(values, position_ids, seq_len = rotary_seq_len)\n",
    "#         queries, keys = apply_rotary_pos_emb(queries, keys, cos, sin, position_ids)\n",
    "#         ################## # Reshape to the expected shape for Flash Attention\n",
    "#         queries = queries.transpose(1, 2)\n",
    "#         keys = keys.transpose(1, 2)\n",
    "#         values = values.transpose(1, 2)\n",
    "#         ###################\n",
    "#         sa_output = sa_module._flash_attention_forward(queries, keys, values, attention_mask, N)\n",
    "#         sa_output = sa_output.reshape(B, N, D).contiguous()\n",
    "#     else:\n",
    "#         cos, sin = sa_module.rotary_emb(values, position_ids, seq_len = N)\n",
    "#         queries, keys = apply_rotary_pos_emb(queries, keys, cos, sin, position_ids)\n",
    "#         attn_weights = torch.matmul(queries, keys.transpose(2, 3))/math.sqrt(Dh)  # Should be shape B x H x N x N\n",
    "#         attn_weights = attn_weights + attention_mask # Attemtion mask is upper triangular of negative infinity\n",
    "#         attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(values.dtype)\n",
    "#         sa_output = torch.matmul(attn_weights, values) # B x H x N x D/H\n",
    "#         sa_output = sa_output.transpose(1, 2).contiguous() # Reorder into B x N x H x D/H\n",
    "#         sa_output = sa_output.reshape(B, N, D) # Concatenate vertically back into B x N x D\n",
    "    \n",
    "#     # Finally post-concatenation linear layer\n",
    "#     sa_output = sa_module.o_proj(sa_output)\n",
    "\n",
    "#     saved_sa_outputs.append(sa_output[0, :, :].detach())\n",
    "    \n",
    "#     ### add residual -> store residual -> layernorm -> mlp -> add residual\n",
    "#     hidden_state = residual + sa_output\n",
    "#     residual = hidden_state\n",
    "#     saved_hk1s.append(hidden_state[0, :, :].detach())\n",
    "\n",
    "#     hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "#     ## MLP            \n",
    "#     up_state = layer.mlp.gate_up_proj(hidden_state) # B x N x (2I, I = intermediate MLP dimension)\n",
    "#     gate, up_state = up_state.chunk(2, dim = -1) # B x N x I\n",
    "#     up_state = up_state * layer.mlp.activation_fn(gate)  # Elementwise\n",
    "#     hidden_state = layer.mlp.down_proj(up_state) # Back to B x N x D\n",
    "#     ## End MLP\n",
    "    \n",
    "#     saved_mlp_outputs.append(hidden_state[0, :, :].detach())\n",
    "\n",
    "#     hidden_state = residual + hidden_state\n",
    "\n",
    "#     saved_hk2s.append(hidden_state[0, :, :].detach())\n",
    "\n",
    "#     if l >= kstart - 1 and l <= kend - 1: # Only calculate loss on the first K transformer blocks\n",
    "#         current_l2s = hidden_state[:, :, (D - Df):D].unsqueeze(dim = 1)  # Save B x 1 x N x Df without detaching\n",
    "        \n",
    "#         if saved_hk2s is None:\n",
    "#             saved_hk2s = current_l2s\n",
    "#         else:\n",
    "#             saved_hk2s = torch.cat((saved_hk2s, current_l2s), dim = 1)\n",
    "\n",
    "\n",
    "#         # saved_l2s will be of shape B x K x N x Dl\n",
    "    \n",
    "# l2_mask = mask.unsqueeze(1).unsqueeze(-1).expand(B, K, N, Df) # Creates shape B x K x N x Df with 0s as appropriate\n",
    "\n",
    "# # RMS norm the final transformer layer output\n",
    "# hidden_state = model.model.norm(hidden_state)\n",
    "\n",
    "# # Run LM head\n",
    "# logits = model.lm_head(hidden_state).float() # B x N x D\n",
    "# #### End Forward Pass ######\n",
    "\n",
    "# ##### Calculate loss #####\n",
    "# # Mask loss anywhere where the input ids are pad tokens\n",
    "# label_ids = torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids)\n",
    "# # Shift so that tokens < n predict n\n",
    "# shift_logits = logits[..., :-1, :].contiguous() # Remove the last token from the sequence\n",
    "# shift_labels = label_ids[..., 1:].contiguous() # Remove the first token from the sequence\n",
    "# # Flatten tokens\n",
    "# loss_fct = CrossEntropyLoss(ignore_index = -100)\n",
    "# shift_logits = shift_logits.view(-1, model.config.vocab_size)\n",
    "# shift_labels = shift_labels.view(-1)\n",
    "# # Enable model parallelism\n",
    "# # shift_labels = shift_labels.to(device)\n",
    "# base_loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "# position_loss = torch.where(\n",
    "#     l2_mask == 1, \n",
    "#     (saved_l2s - feature_targets.unsqueeze(1).unsqueeze(2)) ** 2,\n",
    "#     torch.tensor(0.0)\n",
    "# )\n",
    "# position_loss = position_loss.sum(dim = (0, 1, 2, 3))/l2_mask.sum(dim = (0, 1, 2, 3))\n",
    "\n",
    "# # # Add additional hinge penalty to disproportionately penalize values with abs value outside desired range\n",
    "# hinge_loss = (torch.clamp(saved_l2s, max = 0) - 0) ** 2 + (torch.clamp(saved_l2s, min = 1) - 1) ** 2 \n",
    "# hinge_loss = torch.where(l2_mask == 1, hinge_loss, torch.tensor(0.0)).sum()/l2_mask.sum() \n",
    "\n",
    "# loss = base_loss + position_loss * 3 + hinge_loss * 1  # v2: 3/1\n",
    "# ##### End loss calcaulation ######\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# del model\n",
    "# check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b32903-d5cb-4af1-89b2-3b8fee666c09",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebffd3-8e76-4269-a75d-a07e297cb2bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Pre-train Visualizations\n",
    "test_prompts = [\n",
    "    parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my dog?'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my friend?'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'Hey idiot, what\\'s wrong with my dog?'}], True),\n",
    "]\n",
    "test_gens = [generate_fence(my_model, tokenizer, prompt = t, max_tokens = 16) for t in test_prompts]\n",
    "\n",
    "test_plots = [visualize_fence(gen['text'], gen['hks'], [30], feature_dict, 2950, 0, 5).update_layout(title = 'H<sub>10</sub>', height = 300) for gen in test_gens]\n",
    "\n",
    "for p in test_plots:\n",
    "    p.show('colab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a366a-1828-4271-9fb0-d154ed0822e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def eval_fence(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    test_ds: FenceDataSet, \n",
    "    Kf_target_values: dict,\n",
    "    Kfstart: int = 1, \n",
    "    Kfend: int = 32, \n",
    "    force_fence: bool = True, \n",
    "    batch_size: int = 10, \n",
    "    num_batches: int = 20,\n",
    "    device = 'cuda'\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Evaluation function to get test losses    \n",
    "\n",
    "    Params:\n",
    "        @model: The model to use\n",
    "        @tokenizer: The tokenizer object\n",
    "        @test_ds: The test dataset object, with a FENCE feature dict object\n",
    "        @Kf_target_values: The targeted values by layer, e.g. {'hk': [0, 1, 2], 'hkrs': [1, 2, 3]}\n",
    "        @Kfstart: The starting index layer index to track position losses (or force a FENCE) - starts at 1, not 0\n",
    "        @Kfend: The ending index layer index to track position losses \n",
    "        @force_fence: Whether to force FENCE position indices\n",
    "        @batch_size: The batch size to use for eval\n",
    "        @num_batches: The number of batches to use fo reval\n",
    "        @device: The torch device\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch_results = []\n",
    "    batches_to_eval = num_batches\n",
    "    input_count = 0\n",
    "    \n",
    "    for ix, batch in enumerate(DataLoader(test_ds, batch_size = batch_size, shuffle = True)):\n",
    "        \n",
    "        if ix >= batches_to_eval:\n",
    "            break\n",
    "            \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        feature_targets = batch['feature_targets'].to(device)\n",
    "        position_mask = batch['position_mask'].to(device)\n",
    "                \n",
    "        ##### Forward Pass ######\n",
    "        embeds_output = model.model.embed_tokens(input_ids) # B x N x D\n",
    "        hidden_state = embeds_output\n",
    "\n",
    "        B, N, D = embeds_output.shape\n",
    "        H = 32\n",
    "        Df = feature_targets.shape[1] # Total FENCE width\n",
    "        Kf = Kfend - Kfstart + 1\n",
    "\n",
    "        # Prepare SA inputs\n",
    "        position_ids = torch.arange(0, N, dtype = torch.long, device = device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        if model.model._attn_implementation == 'flash_attention_2':\n",
    "            attention_mask = mask if (mask is not None and 0 in mask) else None  # Flash attention = use default attention mask 2d\n",
    "        else: \n",
    "            attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = model.model.config.sliding_window)  # Non FA: Make a triangular attention mask to hide right context\n",
    "\n",
    "        hkr_target_values = torch.tensor(list(Kf_target_values['hkrs'].values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "        hk_target_values = torch.tensor(list(Kf_target_values['hks'].values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "\n",
    "        # Multiply it by the actual feature targets by layer\n",
    "        feature_targets_bkd = feature_targets.unsqueeze(1) # B x K x Df\n",
    "        hkr_feature_targets = hkr_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hkr_feature_targets = hkr_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "        hk_feature_targets = hk_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hk_feature_targets = hk_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "\n",
    "        # Saved_hk2s will be of shape B x K x N x Df\n",
    "        saved_hkrs = None\n",
    "        saved_hks = None\n",
    "        for l, layer in enumerate(model.model.layers):\n",
    "            \n",
    "            # SA\n",
    "            residual = hidden_state\n",
    "            sa_input = layer.input_layernorm(hidden_state)\n",
    "            sa_output = layer.self_attn(sa_input, attention_mask, position_ids)[0]\n",
    "            \n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_attn_dropout(sa_output)    \n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                if force_fence: # Forcibly set H_K^R\n",
    "                    # To extract the right layer from hkr_feature_targets:\n",
    "                    # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "                    # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "                    # - => k = l + 1 - Kfstart\n",
    "                    hidden_state[:, :, -Df: ] = hkr_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df\n",
    "\n",
    "                this_hidden_state = hidden_state[:, :, -Df:].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hkrs is None:\n",
    "                    saved_hkrs = this_hidden_state\n",
    "                else:\n",
    "                    saved_hkrs = torch.cat((saved_hkrs, this_hidden_state), dim = 1)\n",
    "\n",
    "            # MLP\n",
    "            residual = hidden_state\n",
    "            hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "            mlp_output = layer.mlp(hidden_state)\n",
    "\n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_mlp_dropout(mlp_output)\n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                if force_fence: # Forcibly set H_K\n",
    "                    hidden_state[:, :, -Df: ] = hk_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df\n",
    "\n",
    "                this_hidden_state = hidden_state[:, :, -Df:].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hks is None:\n",
    "                    saved_hks = this_hidden_state\n",
    "                else:\n",
    "                    saved_hks = torch.cat((saved_hks, this_hidden_state), dim = 1)\n",
    "                    \n",
    "\n",
    "        # RMS norm the final transformer layer output\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "\n",
    "        # Run LM head\n",
    "        logits = model.lm_head(hidden_state).float() # B x N x D\n",
    "        #### End Forward Pass ######\n",
    "\n",
    "        ##### Calculate loss #####\n",
    "        # Mask loss anywhere where the input ids are pad tokens\n",
    "        label_ids = torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids)\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous() # Remove the last token from the sequence\n",
    "        shift_labels = label_ids[..., 1:].contiguous() # Remove the first token from the sequence\n",
    "        # Flatten tokens\n",
    "        loss_fct = CrossEntropyLoss(ignore_index = -100)\n",
    "        shift_logits = shift_logits.view(-1, model.config.vocab_size)\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        # Enable model parallelism\n",
    "        # shift_labels = shift_labels.to(device)\n",
    "        base_loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        # Calculate the position loss\n",
    "        # Apply the position target mask\n",
    "        position_mask_bknd = position_mask.unsqueeze(1).unsqueeze(-1).expand(B, Kf, N, Df) # B x K X N x Df (recycles across K and Df)\n",
    "\n",
    "        # Creates a B x K x N x Df tensor with targets differing by K and Df, and values masked (-100) or not (0, .25, .5, etc.) varying by N\n",
    "        # MAPE Loss\n",
    "        position_loss_hkrs = torch.where(position_mask_bknd == 1, torch.abs((saved_hkrs - hkr_feature_targets)/(hkr_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "        position_loss_hks = torch.where(position_mask_bknd == 1, torch.abs((saved_hks - hk_feature_targets)/(hk_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "        \n",
    "        position_loss_hkrs_by_k = position_loss_hkrs.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "        position_loss_hks_by_k = position_loss_hks.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "\n",
    "        position_loss_hkrs_by_dim = position_loss_hkrs.sum(dim = (0, 1, 2))/position_mask_bknd.sum(dim = (0, 1, 2))\n",
    "        position_loss_hks_by_dim = position_loss_hks.sum(dim = (0, 1, 2))/position_mask_bknd.sum(dim = (0, 1, 2))\n",
    "\n",
    "        position_loss_hkrs = position_loss_hkrs.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "        position_loss_hks = position_loss_hks.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "        \n",
    "        loss = base_loss + position_loss_hks + position_loss_hkrs\n",
    "        ##### End loss calcaulation ######\n",
    "        \n",
    "        dim_ix = 0\n",
    "        position_loss_hks_by_feature = {}\n",
    "        for fname, fdim in test_ds.feature_dict.items():\n",
    "            position_loss_hks_by_feature[fname] = position_loss_hks_by_dim[dim_ix : dim_ix + fdim].mean().detach().cpu()\n",
    "            dim_ix = dim_ix + fdim\n",
    "\n",
    "        batch_results.append({\n",
    "            'base_loss': base_loss.detach().cpu().item(),\n",
    "            'position_loss_hkrs': position_loss_hkrs.detach().cpu().item(),\n",
    "            'position_loss_hks': position_loss_hks.detach().cpu().item(),\n",
    "            'position_loss_hkrs_by_k': dict(zip([i + Kfstart for i in list(range(Kfstart - 1, Kfend))], position_loss_hkrs_by_k.detach().cpu().tolist())),\n",
    "            'position_loss_hks_by_k': dict(zip([i + Kfstart for i in list(range(Kfstart - 1, Kfend))], position_loss_hks_by_k.detach().cpu().tolist())),\n",
    "            'position_loss_hks_by_feature': position_loss_hks_by_feature\n",
    "        })\n",
    "        input_count = input_count + B\n",
    "\n",
    "    return {\n",
    "        'input_count': input_count,\n",
    "        'base_loss': np.mean([b['base_loss'] for b in batch_results]),\n",
    "        'position_loss_hkrs': np.mean([b['position_loss_hkrs'] for b in batch_results]),\n",
    "        'position_loss_hks': np.mean([b['position_loss_hks'] for b in batch_results]),\n",
    "        'position_loss_hkrs_by_k': {k: np.mean([b['position_loss_hkrs_by_k'][k] for b in batch_results]) for k in batch_results[0]['position_loss_hkrs_by_k'].keys()},\n",
    "        'position_loss_hks_by_k': {k: np.mean([b['position_loss_hks_by_k'][k] for b in batch_results]) for k in batch_results[0]['position_loss_hkrs_by_k'].keys()},\n",
    "        'position_loss_hks_by_feature': {\n",
    "            fname: torch.stack([b['position_loss_hks_by_feature'][fname] for b in batch_results], dim = 0).mean().item()\n",
    "            for fname in test_ds.feature_dict.keys()\n",
    "        }\n",
    "    }\n",
    "\n",
    "eval_fence(my_model, tokenizer, test_ds, Kf_target_values, Kfstart = Kfstart, Kfend = Kfend, force_fence = False, batch_size = 10, num_batches = 5, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f9fce-519e-443d-b138-1372da98a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nosup_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688b604-801c-4d75-a9d8-e7a4b2ca986b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# Investigate lower LR\n",
    "# Investigate partial position-loss targeting\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, my_model.parameters()), lr = 3e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 10000, gamma = 0.8)\n",
    "train_with_force_fence = True\n",
    "\n",
    "my_model.train()\n",
    "\n",
    "step = 0\n",
    "max_grad_norm = 5.0  # Set the value for gradient clipping\n",
    "\n",
    "# STEPS\n",
    "# 1-100: Nothing, logging purposes only\n",
    "# 100-5k: Always force forcing FENCE [no position loss] - This is just to get the model to adjust to force FENCE (control only!)\n",
    "# 5k-10k: Force FENCE 90% of the time, remaining 10% trains position loss w/weight 2\n",
    "# 10k-15k: Force FENCE 80% of the time, remaining 20% trains position loss w/weight 5\n",
    "# 15k-20k: Force FENCE 70% of the time, remaining 30% trains position loss w/weight 10\n",
    "# 20k-30k: Force FENCE 50% of the time, remaining 50% trains position loss w/weight 20\n",
    "# 30k+: Force FENCE 50% of the time, remaining 50% trains position loss w/weight 25\n",
    "for epoch_ix in range(0, 100):\n",
    "    \n",
    "    for batch_ix, batch in enumerate(train_dl):\n",
    "\n",
    "        # If force FENCE (default), then there is no position loss\n",
    "        force_fence = (\n",
    "            train_with_force_fence and (\n",
    "                (step < 5000) or\n",
    "                (step >= 5000 and step < 10000 and step % 10 >= 1) or\n",
    "                (step >= 10000 and step < 15000 and step % 10 >= 2) or\n",
    "                (step >= 15000 and step < 20000 and step % 10 >= 3) or\n",
    "                (step >= 20000 and step % 10 >= 5)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if step < 4:\n",
    "            check_memory()\n",
    "\n",
    "        # If not force FENCE (i.e., there exists some position loss, then train with no-surprise data)\n",
    "        if force_fence == False:\n",
    "            batch = next(iter(train_nosup_dl))\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        feature_targets = batch['feature_targets'].to(device)\n",
    "        position_mask = batch['position_mask'].to(device)\n",
    "\n",
    "        ##### Forward Pass ######\n",
    "        embeds_output = my_model.model.embed_tokens(input_ids) # B x N x D\n",
    "        hidden_state = embeds_output\n",
    "\n",
    "        # Execute transformers layers\n",
    "        # B = batch size, N = token length, D = token dim, Dh = token per-head dim, H = number of heads, K = # transformer blocks\n",
    "        # Df = total FENCE dimensino width\n",
    "        # Kfstart, Kfend = starting and ending indices for transformer blocks to include in FENCE (indices starts with 1, not 0)\n",
    "        # Kf = number of transformer blocks to include in FENCE\n",
    "        B, N, D = embeds_output.shape\n",
    "        H = 32\n",
    "        Dh = int(D/H)\n",
    "        K = 32 \n",
    "\n",
    "        Df = feature_targets.shape[1] # Total FENCE width\n",
    "        Kf = Kfend - Kfstart + 1\n",
    "\n",
    "        # Prepare SA inputs\n",
    "        position_ids = torch.arange(0, N, dtype = torch.long, device = device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        if my_model.model._attn_implementation == 'flash_attention_2':\n",
    "            attention_mask = mask if (mask is not None and 0 in mask) else None  # Flash attention = use default attention mask 2d\n",
    "        else: \n",
    "            attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = my_model.model.config.sliding_window)  # Non FA: Make a triangular attention mask to hide right context\n",
    "\n",
    "        hkr_target_values = torch.tensor(list(Kf_target_values['hkrs'].values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "        hk_target_values = torch.tensor(list(Kf_target_values['hks'].values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "\n",
    "        # Multiply it by the actual feature targets by layer\n",
    "        feature_targets_bkd = feature_targets.unsqueeze(1) # B x K x Df\n",
    "        hkr_feature_targets = hkr_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hkr_feature_targets = hkr_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "        hk_feature_targets = hk_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hk_feature_targets = hk_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "\n",
    "        # Saved_hk2s will be of shape B x K x N x Df\n",
    "        saved_hkrs = None\n",
    "        saved_hks = None\n",
    "        for l, layer in enumerate(my_model.model.layers):\n",
    "            \n",
    "            # SA\n",
    "            residual = hidden_state\n",
    "            hidden_state = layer.input_layernorm(hidden_state)\n",
    "            hidden_state = layer.self_attn(hidden_state, attention_mask, position_ids)[0]\n",
    "            \n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_attn_dropout(hidden_state)    \n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                if force_fence and step >= 100: # Forcibly set H_K^R\n",
    "                    # To extract the right layer from hkr_feature_targets:\n",
    "                    # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "                    # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "                    # - => k = l + 1 - Kfstart\n",
    "                    hidden_state[:, :, -Df: ] = hkr_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                this_hidden_state = hidden_state[:, :, -Df:].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hkrs is None:\n",
    "                    saved_hkrs = this_hidden_state\n",
    "                else:\n",
    "                    saved_hkrs = torch.cat((saved_hkrs, this_hidden_state), dim = 1)\n",
    "\n",
    "            # MLP\n",
    "            residual = hidden_state\n",
    "            hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "            hidden_state = layer.mlp(hidden_state)\n",
    "\n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_mlp_dropout(hidden_state)\n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                if force_fence and step >= 100: # Forcibly set H_K\n",
    "                    hidden_state[:, :, -Df: ] = hk_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df                \n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                this_hidden_state = hidden_state[:, :, -Df:].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hks is None:\n",
    "                    saved_hks = this_hidden_state\n",
    "                else:\n",
    "                    saved_hks = torch.cat((saved_hks, this_hidden_state), dim = 1)\n",
    "\n",
    "        # RMS norm the final transformer layer output\n",
    "        hidden_state = my_model.model.norm(hidden_state)\n",
    "\n",
    "        # Run LM head\n",
    "        logits = my_model.lm_head(hidden_state).float() # B x N x D\n",
    "        #### End Forward Pass ######\n",
    "\n",
    "        ##### Calculate loss #####\n",
    "        # Mask loss anywhere where the input ids are pad tokens\n",
    "        label_ids = torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids)\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous() # Remove the last token from the sequence\n",
    "        shift_labels = label_ids[..., 1:].contiguous() # Remove the first token from the sequence\n",
    "        # Flatten tokens\n",
    "        loss_fct = CrossEntropyLoss(ignore_index = -100)\n",
    "        shift_logits = shift_logits.view(-1, my_model.config.vocab_size)\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        # Enable model parallelism\n",
    "        # shift_labels = shift_labels.to(device)\n",
    "        base_loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        # Calculate the position loss\n",
    "        # Apply the position target mask\n",
    "        position_mask_bknd = position_mask.unsqueeze(1).unsqueeze(-1).expand(B, Kf, N, Df) # B x K X N x Df (recycles across K and Df)\n",
    "\n",
    "        position_loss_hkrs = torch.where(position_mask_bknd == 1, torch.abs((saved_hkrs - hkr_feature_targets)/(hkr_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "        position_loss_hks = torch.where(position_mask_bknd == 1, torch.abs((saved_hks - hk_feature_targets)/(hk_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "        \n",
    "        position_loss_hkrs_by_k = position_loss_hkrs.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "        position_loss_hks_by_k = position_loss_hks.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "\n",
    "        # Maybe consider using MAPE\n",
    "        position_loss_hkrs = position_loss_hkrs.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "        position_loss_hks = position_loss_hks.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "        \n",
    "        # # Add additional hinge penalty to disproportionately penalize values with abs value outside desired range\n",
    "        # hinge_loss = (torch.clamp(saved_l2s, max = 0) - 0) ** 2 + (torch.clamp(saved_l2s, min = 1) - 1) ** 2 \n",
    "        # hinge_loss = torch.where(l2_mask == 1, hinge_loss, torch.tensor(0.0)).sum()/l2_mask.sum() \n",
    "        if step < 5000:\n",
    "            loss = base_loss\n",
    "        elif step >= 5000 and step < 10000:\n",
    "            loss = base_loss + 2 * position_loss_hks + 2 * position_loss_hkrs\n",
    "        elif step >= 10000 and step < 15000:\n",
    "            loss = base_loss + 5 * position_loss_hks + 5 * position_loss_hkrs\n",
    "        elif step >= 15000 and step < 20000:\n",
    "            loss = base_loss + 10 * position_loss_hks + 10 * position_loss_hkrs\n",
    "        elif step >= 20000 and step < 30000:\n",
    "            loss = base_loss + 20 * position_loss_hks + 20 * position_loss_hkrs\n",
    "        else:\n",
    "            loss = base_loss + 25 * position_loss_hks + 25 * position_loss_hkrs\n",
    "        ##### End loss calcaulation ######\n",
    "\n",
    "        ##### Logging #####\n",
    "        if step % 50 == 0:\n",
    "            print(np.round(base_loss.detach().cpu().item(), 2), np.round(position_loss_hks.detach().cpu().item(), 2))\n",
    "            \n",
    "        logging_dict = {\n",
    "            'epoch': epoch_ix,\n",
    "            'step': step,\n",
    "            'lr': optimizer.param_groups[0]['lr'],\n",
    "            'train': {\n",
    "                'base_loss': base_loss.detach().cpu().item(),\n",
    "                'position_loss_hkrs': position_loss_hkrs.detach().cpu().item(),\n",
    "                'position_loss_hks': position_loss_hks.detach().cpu().item(),\n",
    "                'position_loss_hkrs_by_k': dict(zip([i + Kfstart for i in list(range(Kfstart - 1, Kfend))], position_loss_hkrs_by_k.detach().cpu().tolist())),\n",
    "                'position_loss_hks_by_k': dict(zip([i + Kfstart for i in list(range(Kfstart - 1, Kfend))], position_loss_hks_by_k.detach().cpu().tolist())),\n",
    "                'total_loss': loss.detach().cpu().item()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            logging_dict = {\n",
    "                **logging_dict, \n",
    "                **{'test_forced': eval_fence(my_model, tokenizer, test_ds, Kf_target_values = Kf_target_values, Kfstart = Kfstart, Kfend = Kfend, force_fence = True, batch_size = 10, num_batches = 20, device = device)},\n",
    "                **{'test_unforced': eval_fence(my_model, tokenizer, test_ds, Kf_target_values = Kf_target_values, Kfstart = Kfstart, Kfend = Kfend, force_fence = False, batch_size = 10, num_batches = 20, device = device)}\n",
    "            }\n",
    "            my_model.train()\n",
    "            \n",
    "        # Log losses\n",
    "        if USE_WANDB:\n",
    "            wandb.log(logging_dict)\n",
    "        ##### End Logging #####\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(my_model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Step the scheduler to decay the learning rate\n",
    "        step = step + 1\n",
    "\n",
    "        del input_ids, mask, feature_targets, position_mask, embeds_output, hidden_state, this_hidden_state, logits, residual\n",
    "        del label_ids, shift_logits, shift_labels, base_loss, position_mask_bknd, position_loss_hkrs, position_loss_hks, position_loss_hkrs_by_k, position_loss_hks_by_k\n",
    "        del hkr_target_values, hk_target_values, feature_targets_bkd, hkr_feature_targets, hk_feature_targets\n",
    "        del loss, position_ids, attention_mask\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if epoch_ix % 2 == 0:\n",
    "        torch.save(my_model.state_dict(), f\"{SAVE_DIR}/e{str(epoch_ix + 1)}.pt\")\n",
    "    \n",
    "    # nondog = visualize_fence(list(range(10, 20)), my_model, tokenizer, parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my friend?'}], True), train_ds.feature_dict, max_tokens = 16)    \n",
    "    # dog = visualize_fence(list(range(10, 20)), my_model, tokenizer, parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my dog?'}], True), train_ds.feature_dict, max_tokens = 16)\n",
    "    # angrydog = visualize_fence(list(range(10, 20)), my_model, tokenizer, parse_phi([{'role': 'user', 'content': 'My dogs and cats make me so MAD!'}], True), train_ds.feature_dict, max_tokens = 16)\n",
    "\n",
    "    # nondog[2].write_html(f\"{SAVE_DIR}/{str(epoch_ix + 1)}_nondog.html\")\n",
    "    # dog[2].write_html(f\"{SAVE_DIR}/{str(epoch_ix + 1)}_yesdog.html\")\n",
    "\n",
    "    # angrydog[2].write_html(f\"{SAVE_DIR}/{str(epoch_ix + 1)}_angrydog.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb5c99-c2f8-44de-9326-70b20eb00729",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(Kf_target_values['hkrs'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c1d90-9ed6-4d75-9020-59ba20533d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = ['input_ids', 'mask', 'feature_targets', 'position_mask', 'embeds_output', 'hidden_state', 'this_hidden_state', 'logits', 'residual'\n",
    "             'label_ids', 'shift_logits', 'shift_labels', 'base_loss', 'position_mask_bknd', 'position_loss_hkrs', 'position_loss_hks',  'position_loss_hkrs_by_k', 'position_loss_hks_by_k',\n",
    "             'hkr_target_values', 'hk_target_values', 'feature_targets_bkd', 'hkr_feature_targets', 'hk_feature_targets',\n",
    "             'loss', 'position_ids', 'attention_mask',\n",
    "             'optimizer', 'scheduler']\n",
    "\n",
    "for var in to_delete:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b6207-8fc9-408b-bb4c-b6dee3f45355",
   "metadata": {},
   "outputs": [],
   "source": [
    "nondog = visualize_fence(list(range(20, 21)), my_model, tokenizer, parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling?'}], True), train_ds.feature_dict, max_tokens = 32)\n",
    "nondog[2].show('colab')\n",
    "\n",
    "dog = visualize_fence(list(range(20, 21)), my_model, tokenizer, parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my dog?'}], True), train_ds.feature_dict, max_tokens = 32)\n",
    "dog[2].show('colab')\n",
    "\n",
    "table = wandb.Table(columns = ['Nondog', 'Dog'])\n",
    "nondog[2].write_html('./fig1.html')\n",
    "dog[2].write_html('./fig2.html')\n",
    "table.add_data(wandb.Html('./fig1.html'), wandb.Html('./fig2.html'))\n",
    "run.log({\"name\": 'Trained - Nondog/dog, Layer 20'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
