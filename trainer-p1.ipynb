{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0bcec-cadd-4ec1-8294-5a57b77785c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "V5 experiments:\n",
    "- Add in MLP value decay based off distance matrix\n",
    "- Training with cleaner DS\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a93d9e37-2fbc-49f6-9c89-78be8199302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML, Markdown\n",
    "import plotly.express as px\n",
    "import os\n",
    "import wandb \n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib\n",
    "import pathlib\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from helpers.phi3.phi3 import Phi3Config, Phi3ForCausalLM, _prepare_4d_causal_attention_mask\n",
    "from helpers.phi3.parse import parse_phi\n",
    "from helpers.memory import check_memory\n",
    "\n",
    "load_dotenv('secrets.env')\n",
    "device = 'cuda'\n",
    "\n",
    "RUN_ID = f\"{datetime.now(pytz.timezone('US/Eastern')).strftime('%Y%m%dT%H%M')}\"\n",
    "SAVE_DIR = f\"./models/{RUN_ID}\"\n",
    "USE_WANDB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f80fa6-a7af-4fdd-9e80-7663f76099de",
   "metadata": {},
   "source": [
    "## Setup Save Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0e751c-7557-4647-a01b-f301e1aae180",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(SAVE_DIR).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "if USE_WANDB:\n",
    "    \n",
    "    os.environ['WANDB_INIT_TIMEOUT'] = '120'\n",
    "    wandb.login(key = os.getenv('WANDB_API_KEY'))\n",
    "    run = wandb.init(\n",
    "        project = 'fence_v5', \n",
    "        name = RUN_ID,\n",
    "        notes = '',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3714e",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7755f5-0c64-4d5f-9df4-5de05463ba25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236fc14cb9b940658e02569b6fd664c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_implementation = 'flash_attention_2' # None/flash_attention_2\n",
    "\n",
    "# Load Model\n",
    "# Padding side not important EXCEPT for flash attention, needs to be left\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-4k-instruct', add_eos_token = False, add_bos_token = False, padding_side = 'left') \n",
    "\n",
    "# Load the usual model from HF transformers - attn_implementation = None to disable flash attention\n",
    "my_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'microsoft/Phi-3-mini-4k-instruct',\n",
    "    device_map = device,\n",
    "    trust_remote_code = True, \n",
    "    torch_dtype = torch.bfloat16, \n",
    "    attn_implementation = attn_implementation\n",
    "    ).to(device).eval()\n",
    "\n",
    "# Now load a model seperately from the underlying model object code\n",
    "# my_model = Phi3ForCausalLM(base_model.config).to(device).eval().to(dtype = torch.bfloat16) # Phi3Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a071720b-7619-4494-9259-84d7dda29af4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Check everything is bfloat16\n",
    "# for p in base_model.parameters():\n",
    "#     print(p.dtype)\n",
    "\n",
    "# # Check attention implementation\n",
    "# my_model.model.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad3756-c2c2-4f36-8e4f-1185e994ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Next, we want to clone params from base_model into model\n",
    "# # Let's store all params from the base_model\n",
    "# all_params = {}\n",
    "# for name, param in base_model.named_parameters():\n",
    "#     all_params[name] = param.cpu().clone()\n",
    "\n",
    "# # Then copy them over to the new model\n",
    "# for name, param in my_model.named_parameters():\n",
    "#     param.data.copy_(all_params[name].data)\n",
    "\n",
    "# # Verify these are the same\n",
    "# for name, p in my_model.named_parameters():\n",
    "#     if name == 'model.embed_tokens.weight': \n",
    "#         print(p)\n",
    "# for name, p in base_model.named_parameters():\n",
    "#     if name == 'model.embed_tokens.weight': \n",
    "#         print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c3278-1aa0-40d6-abc6-635a1eda15a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_memory()\n",
    "# # Need to delete ALl references to original model to clear memory properly https://discuss.pytorch.org/t/cuda-memory-not-released-by-torch-cuda-empty-cache/129913/6\n",
    "# if 'base_model' in globals():\n",
    "#     del base_model\n",
    "# if 'name' in globals():\n",
    "#     del name\n",
    "# if 'param' in globals():\n",
    "#     del param\n",
    "# if 'p' in globals():\n",
    "#     del p\n",
    "# if 'all_params' in globals():\n",
    "#     del all_params\n",
    "    \n",
    "# torch.cuda.empty_cache()\n",
    "# check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee225f-dc3f-4261-aa65-6dba823314dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(my_model.state_dict(), f'./models/phi3_base.pt')\n",
    "# base_model.load_state_dict(torch.load('./models/FENCEV2-20240619T0813/e2.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ac28d-28f8-4fd8-a42e-5e2db1859d8a",
   "metadata": {},
   "source": [
    "## Initialize FENCE Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "741df29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FENCE Params Object:\n",
       "  FENCE Dictionary: {'programming': (2980, 2983), 'food': (3000, 3003), 'animals': (3020, 3023), 'dogs': (3040, 3043), 'cats': (3060, 3063)}\n",
       "  Number of position loss layers: 32 (Layer 1 to layer 32)\n",
       "  Df: 20  \n",
       "  hkr_target_values: {1: 0.125, 2: 0.375, 3: 0.625, 4: 0.875, 5: 1.125, 6: 1.375, 7: 1.625, 8: 1.875, 9: 2.125, 10: 2.375, 11: 2.625, 12: 2.875, 13: 3.125, 14: 3.375, 15: 3.625, 16: 3.875, 17: 4.125, 18: 4.375, 19: 4.625, 20: 4.875, 21: 5.125, 22: 5.375, 23: 5.625, 24: 5.875, 25: 6.125, 26: 6.375, 27: 6.625, 28: 6.875, 29: 7.125, 30: 7.375, 31: 7.625, 32: 7.875}\n",
       "  hk_target_values: {1: 0.25, 2: 0.5, 3: 0.75, 4: 1.0, 5: 1.25, 6: 1.5, 7: 1.75, 8: 2.0, 9: 2.25, 10: 2.5, 11: 2.75, 12: 3.0, 13: 3.25, 14: 3.5, 15: 3.75, 16: 4.0, 17: 4.25, 18: 4.5, 19: 4.75, 20: 5.0, 21: 5.25, 22: 5.5, 23: 5.75, 24: 6.0, 25: 6.25, 26: 6.5, 27: 6.75, 28: 7.0, 29: 7.25, 30: 7.5, 31: 7.75, 32: 8.0}\n",
       "  Df_zero_indices: [2979, 2980, 2981, 2982, 2999, 3000, 3001, 3002, 3019, 3020, 3021, 3022, 3039, 3040, 3041, 3042, 3059, 3060, 3061, 3062]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FenceParams():\n",
    "    def __init__(self, fence_dict: dict[str, int], D: int, Kfstart: int, Kfend: int, hkr_target_values: dict[int, float], hk_target_values: dict[int, float]):\n",
    "        \"\"\"\n",
    "        Creates a new FENCE Params object\n",
    "        \n",
    "        Description:\n",
    "            Creates a FENCE params object which stores the FENCE dict and the position loss target values\n",
    "        \n",
    "        Params: \n",
    "            @fence_dict: A dict of features and their corresponding fence dimensions, e.g. {'dogs': (3065, 3068), 'cats': (3061, 3064)}. \n",
    "             - These dimensions are 1-indexed and inclusive of both the start and ending numbers passed into the tuples. (3061, 3064) means dimensions 3061, 3062, 3063, and 3064.\n",
    "            @D: The dimension of the hidden state.\n",
    "            @Kfstart: The index (1-indexed) of the first transformer block with which to calculate position loss.\n",
    "            @Kfend: The index (1-indexed) of the last transformer block with which to calculate position loss.\n",
    "            @hkr_target_values: A dict where the keys are the layer index (1-indexed), and the values representing the target FENCE values for each layer's residual stream output.\n",
    "            @hk_target_values:  A dict where the keys are the layer index (1-indexed), and the values representing the target FENCE values for each layer's transformer block output.\n",
    "        \"\"\"\n",
    "        if not all(r1[1] < r2[0] for r1, r2 in zip(sorted(fence_dict.values()), sorted(fence_dict.values())[1:])):\n",
    "            raise ValueError('FENCE dict contains overlapping values')\n",
    "\n",
    "        if not all(curr[0] > prev[0] for prev, curr in zip(fence_dict.values(), list(fence_dict.values())[1:])):\n",
    "            raise ValueError('FENCE dict not passed in order!')\n",
    "\n",
    "        self.fence_dict = fence_dict\n",
    "        self.Kfstart = Kfstart\n",
    "        self.Kfend = Kfend\n",
    "        self.Kf = Kfend - Kfstart + 1\n",
    "        self.hkr_target_values = hkr_target_values\n",
    "        self.hk_target_values = hk_target_values\n",
    "        self.D = D\n",
    "        self.Df = sum([v[1] - v[0] + 1 for k, v in fence_dict.items()])\n",
    "        self.Df_zero_indices = [i - 1 for start, end in fence_dict.values() for i in range(start, end + 1)]\n",
    "        # Dfmask = torch.zeros(D)\n",
    "        # for k, v in fence_dict.items():\n",
    "        #     Dfmask[fence_dict[k][0] - 1:fence_dict[k][1]] = 1\n",
    "        # self.Dfmask = Dfmask\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"FENCE Params Object:\\n\"\n",
    "                f\"  FENCE Dictionary: {self.fence_dict}\\n\"\n",
    "                f\"  Number of position loss layers: {self.Kf} (Layer {self.Kfstart} to layer {self.Kfend})\\n\"\n",
    "                f\"  Df: {self.Df}  \\n\"\n",
    "                f\"  hkr_target_values: {self.hkr_target_values}\\n\"\n",
    "                f\"  hk_target_values: {self.hk_target_values}\\n\"\n",
    "                f\"  Df_zero_indices: {self.Df_zero_indices}\")\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "# Pass indices starting at 1\n",
    "Kfstart = 1\n",
    "Kfend = 32\n",
    "fence_params = FenceParams(\n",
    "    fence_dict = {\n",
    "        'programming': (2980, 2983),\n",
    "        'food': (3000, 3003),\n",
    "        'animals': (3020, 3023),\n",
    "        'dogs': (3040, 3043),\n",
    "        'cats': (3060, 3063)\n",
    "    },\n",
    "    D = 3072,\n",
    "    Kfstart = 1,\n",
    "    Kfend = 32,\n",
    "    hkr_target_values = {Kfstart + j - 1: (j - 1) * .25 + .25/2 for j in range(Kfstart, Kfend + 1)},\n",
    "    hk_target_values = {Kfstart + j - 1: (j - 1) * .25 + .25 for j in range(Kfstart, Kfend + 1)}\n",
    ")\n",
    "\n",
    "fence_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9272b-7bdf-44a5-be55-7cedc7838fc4",
   "metadata": {},
   "source": [
    "## Test Inference & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a59244cd-a37d-4c41-847a-5ed37e398499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"padding: 1rem 2rem; background-color:honeydew\"><h4>Modified model output</h4><span style=\"color:green\"> Animals are multicellular, eukaryotic organisms in the biological kingdom Animalia. With few</span> <span style=\"color:red\">exceptions, animals consume organic material, use energy, and are heterotroph</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"padding: 1rem 2rem; background-color:honeydew\"><h4>Modified model output</h4><span style=\"color:green\"><|user|> What did you do today?<|end|><|assistant|></span> <span style=\"color:red\">As an AI, I don't have personal experiences or a daily routine</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"padding: 1rem 2rem; background-color:honeydew\"><h4>Modified model output</h4><span style=\"color:green\"><|user|> What should I bring to take my dog hiking?<|end|><|assistant|></span> <span style=\"color:red\">When taking your dog on a hike, it's important to ensure their</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"padding: 1rem 2rem; background-color:honeydew\"><h4>Modified model output</h4><span style=\"color:green\"><|user|> What should I bring to take my friend hiking?<|end|><|assistant|></span> <span style=\"color:red\">When preparing for a hiking trip with a friend, it's</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.eval'))\n",
    "from helpers.fence.eval import generate_fence\n",
    "\n",
    "# Test\n",
    "test_prompts = [\n",
    "    '<s>Animals are multicellular, eukaryotic organisms in the biological kingdom Animalia. With few',\n",
    "    parse_phi([{'role': 'user', 'content': 'What did you do today?'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'What should I bring to take my dog hiking?'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'What should I bring to take my friend hiking?'}], True)\n",
    "]\n",
    "\n",
    "test_gens = [generate_fence(my_model, tokenizer, prompt = test_prompt, max_tokens = 16) for test_prompt in test_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4d6e9-af60-4bde-b2c9-ef837c719f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.visualize'))\n",
    "from helpers.fence.visualize import visualize_fence\n",
    "\n",
    "for l in [1, 10, 20, 30]:\n",
    "    visualize_fence(\n",
    "        test_gens[0]['text'],\n",
    "        test_gens[0]['hks'],\n",
    "        [l],\n",
    "        fence_params.fence_dict,\n",
    "        start_dim = 2900, end_dim = 3072,\n",
    "        min_range = 0, max_range = fence_params.hk_target_values[l]\n",
    "    ).update_layout(title = 'H<sub>' + str(l) + '</sub>', height = 300).show('colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bb5dc5-4496-40f0-ab22-51f853705d04",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61595b92-82f3-4ba8-8fcf-ce8ddaf60aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('train.csv')\n",
    "train_nosup_raw = train_raw[train_raw['is_surprise'] == 0] # No-surprise is used for position-loss trainintg\n",
    "test_raw = pd.read_csv('test.csv')\n",
    "print(len(train_raw), len(train_nosup_raw), len(test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113957d6-b147-44f5-b564-b96c4fd28666",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_classifications = train_raw[fence_params.fence_dict.keys()].to_dict('records')\n",
    "train_nosup_feature_classifications = train_nosup_raw[fence_params.fence_dict.keys()].to_dict('records')\n",
    "test_feature_classifications = test_raw[fence_params.fence_dict.keys()].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd073ea-f56f-43af-8c1c-83313f63df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test\n",
    "# importlib.reload(importlib.import_module('helpers.fence.dataset'))\n",
    "# from helpers.fence.dataset import FenceDataSet\n",
    "# token_length = 128\n",
    "\n",
    "# test_tokens = tokenizer(test_raw['phi3_text'].tolist()[0:6], truncation = True, max_length = token_length, padding = 'max_length', return_tensors = 'pt').to(device)\n",
    "# test_feature_classifications = test_raw[fence_params.fence_dict.keys()].to_dict('records')[0:6]\n",
    "\n",
    "# position_mask_start_token_id = tokenizer.encode('<|assistant|>')[0]\n",
    "# test_ds = FenceDataSet(test_tokens, fence_params.fence_dict, 3072, test_feature_classifications, [position_mask_start_token_id])\n",
    "\n",
    "# print(test_ds.position_mask)\n",
    "\n",
    "# print(tokenizer.batch_decode(test_ds.tokens['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ed90f-c7c1-4c60-a88c-377b4fed5bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.dataset'))\n",
    "from helpers.fence.dataset import FenceDataSet\n",
    "token_length = 1024\n",
    "\n",
    "tmp_tokens_len_test = [tokenizer(x, return_tensors = 'pt').to(device) for x in train_raw['phi3_text'].tolist()]\n",
    "px.histogram(pd.DataFrame({\"j\": [t['input_ids'].shape[1] for t in tmp_tokens_len_test]}), x = \"j\").show('colab')\n",
    "\n",
    "train_tokens = tokenizer(train_raw['phi3_text'].tolist(), truncation = True, max_length = token_length, padding = 'max_length', return_tensors = 'pt').to(device)\n",
    "train_nosup_tokens = tokenizer(train_nosup_raw['phi3_text'].tolist(), truncation = True, max_length = token_length, padding = 'max_length', return_tensors = 'pt').to(device)\n",
    "test_tokens = tokenizer(test_raw['phi3_text'].tolist(), truncation = True, max_length = token_length, padding = 'max_length', return_tensors = 'pt').to(device)\n",
    "\n",
    "position_mask_start_token_id = position_mask_start_token_id = tokenizer.encode('<|assistant|>')[0]\n",
    "train_ds = FenceDataSet(train_tokens, fence_params.fence_dict, 3072, train_feature_classifications, position_mask_start_token_id)\n",
    "train_nosup_ds = FenceDataSet(train_nosup_tokens, fence_params.fence_dict, 3072, train_nosup_feature_classifications, position_mask_start_token_id)\n",
    "test_ds = FenceDataSet(test_tokens, fence_params.fence_dict, 3072, test_feature_classifications, position_mask_start_token_id)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size = 10, shuffle = True)\n",
    "train_nosup_dl = DataLoader(train_nosup_ds, batch_size = 10, shuffle = True)\n",
    "test_dl = DataLoader(test_ds, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5272e4-6448-403f-9e2c-58d52e1f7ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Don't train embeddings/lm head/RMSnorm, only parts within transformer blocks\n",
    "for name, param in my_model.named_parameters():\n",
    "    if 1 == 2: #'model.norm' in name or 'lm_head' in name: #\"embed_tokens\", \"model.norm\", \"lm_head\", \"layernorm\" in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for name, param in my_model.named_parameters():\n",
    "    if 'layers' not in name or '.0.' in name:\n",
    "        print(name, param.requires_grad)\n",
    "\n",
    "del name\n",
    "del param\n",
    "\n",
    "check_memory()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fa42cd-f692-4136-a214-91945d0cd7ca",
   "metadata": {},
   "source": [
    "## Single Batch Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0bc791-3f85-458c-bd34-faf0fd5a2644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set single batch\n",
    "for i, batch in enumerate(train_dl):\n",
    "    if i > 1: break\n",
    "\n",
    "input_ids = batch['input_ids']\n",
    "mask = batch['attention_mask']\n",
    "feature_targets = batch['feature_targets']\n",
    "position_mask = batch['position_mask']\n",
    "\n",
    "print(input_ids, mask, feature_targets, position_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83871f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training R1: Force FENCE (i.e., no Position Loss)\n",
    "model = my_model\n",
    "force_fence = False\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    ##### Forward Pass ######\n",
    "    embeds_output = model.model.embed_tokens(input_ids) # B x N x D\n",
    "    hidden_state = embeds_output\n",
    "\n",
    "    # Execute transformers layers\n",
    "    # B = batch size, N = token length, D = token dim, Dh = token per-head dim, H = number of heads, K = # transformer blocks\n",
    "    # Df = total FENCE dimensino width\n",
    "    # Kfstart, Kfend = starting and ending indices for transformer blocks to include in FENCE (indices starts with 1, not 0)\n",
    "    # Kf = number of transformer blocks to include in FENCE\n",
    "    B, N, D = embeds_output.shape\n",
    "    H = 32\n",
    "    Dh = int(D/H)\n",
    "    K = 32 \n",
    "\n",
    "    Kfstart = fence_params.Kfstart\n",
    "    Kfend = fence_params.Kfend\n",
    "    Kf = fence_params.Kf\n",
    "    Df = fence_params.Df\n",
    "    Dfmask = fence_params.Dfmask\n",
    "\n",
    "    # Prepare SA inputs\n",
    "    position_ids = torch.arange(0, N, dtype = torch.long, device = device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "    if model.model._attn_implementation == 'flash_attention_2':\n",
    "        attention_mask = mask if (mask is not None and 0 in mask) else None  # Flash attention = use default attention mask 2d\n",
    "    else: \n",
    "        attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = model.model.config.sliding_window)  # Non FA: Make a triangular attention mask to hide right context\n",
    "\n",
    "    # Create Hkr and Hk target values for position loss calculation when feature_target = 1\n",
    "    hkr_target_values = torch.tensor(list(fence_params.hkr_target_values.values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "    hk_target_values = torch.tensor(list(fence_params.hk_target_values.values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "\n",
    "    # Multiply it by the actual feature targets by layer. Note that this does not apply position masking so all values are FIXED across Ns\n",
    "    feature_targets_bkd = feature_targets.unsqueeze(1) # B x K x Df\n",
    "    hkr_feature_targets = hkr_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x D\n",
    "    hkr_feature_targets = hkr_feature_targets.unsqueeze(2).expand(B, Kf, N, D) # B x K x N x D\n",
    "    hk_feature_targets = hk_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x D\n",
    "    hk_feature_targets = hk_feature_targets.unsqueeze(2).expand(B, Kf, N, D) # B x K x N x D\n",
    "\n",
    "    # Saved_hk2s will be of shape B x K x N x Df\n",
    "    saved_hkrs = None\n",
    "    saved_hks = None\n",
    "    for l, layer in enumerate(model.model.layers):\n",
    "        \n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        sa_input = layer.input_layernorm(hidden_state)\n",
    "        sa_output = layer.self_attn(sa_input, attention_mask, position_ids)[0]\n",
    "        \n",
    "        # Sum back to resid stream\n",
    "        hidden_state = residual + layer.resid_attn_dropout(sa_output)    \n",
    "\n",
    "        if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "            if force_fence: # Forcibly set H_K^R\n",
    "                # To extract the right layer from hkr_feature_targets:\n",
    "                # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "                # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "                # - => k = l + 1 - Kfstart\n",
    "                hidden_state[:, :, -D: ] = hkr_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x D\n",
    "\n",
    "            this_hidden_state = hidden_state[:, :, -D:].unsqueeze(dim = 1)  # Save B x 1 x N x D\n",
    "            if saved_hkrs is None:\n",
    "                saved_hkrs = this_hidden_state\n",
    "            else:\n",
    "                saved_hkrs = torch.cat((saved_hkrs, this_hidden_state), dim = 1)\n",
    "\n",
    "        # MLP\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "        mlp_output = layer.mlp(hidden_state)\n",
    "\n",
    "        # Sum back to resid stream\n",
    "        hidden_state = residual + layer.resid_mlp_dropout(mlp_output)\n",
    "\n",
    "        if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "            if force_fence: # Forcibly set H_K\n",
    "                hidden_state[:, :, -D: ] = hk_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x D\n",
    "\n",
    "            this_hidden_state = hidden_state[:, :, -Df:].unsqueeze(dim = 1)  # Save B x 1 x N x D\n",
    "            if saved_hks is None:\n",
    "                saved_hks = this_hidden_state\n",
    "            else:\n",
    "                saved_hks = torch.cat((saved_hks, this_hidden_state), dim = 1)\n",
    "                \n",
    "\n",
    "    # RMS norm the final transformer layer output\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "\n",
    "    # Run LM head\n",
    "    logits = model.lm_head(hidden_state).float() # B x N x D\n",
    "    #### End Forward Pass ######\n",
    "\n",
    "    ##### Calculate loss #####\n",
    "    # Mask loss anywhere where the input ids are pad tokens\n",
    "    label_ids = torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids)\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous() # Remove the last token from the sequence\n",
    "    shift_labels = label_ids[..., 1:].contiguous() # Remove the first token from the sequence\n",
    "    # Flatten tokens\n",
    "    loss_fct = CrossEntropyLoss(ignore_index = -100)\n",
    "    shift_logits = shift_logits.view(-1, model.config.vocab_size)\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    # Enable model parallelism\n",
    "    # shift_labels = shift_labels.to(device)\n",
    "    base_loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "    # Calculate the position loss\n",
    "    # Apply the position target mask\n",
    "    position_mask_bknd = position_mask.unsqueeze(1).unsqueeze(-1).expand(B, Kf, N, Df) # B x K X N x Df (recycles across K and Df)\n",
    "\n",
    "    position_loss_hkrs = torch.where(position_mask_bknd == 1, torch.abs((saved_hkrs - hkr_feature_targets)/(hkr_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "    position_loss_hks = torch.where(position_mask_bknd == 1, torch.abs((saved_hks - hk_feature_targets)/(hk_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "    \n",
    "    position_loss_hkrs_by_k = position_loss_hkrs.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "    position_loss_hks_by_k = position_loss_hks.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "    \n",
    "    position_loss_hkrs_by_dim = position_loss_hkrs.sum(dim = (0, 1, 2))/position_mask_bknd.sum(dim = (0, 1, 2))\n",
    "    position_loss_hks_by_dim = position_loss_hks.sum(dim = (0, 1, 2))/position_mask_bknd.sum(dim = (0, 1, 2))\n",
    "\n",
    "    position_loss_hkrs = position_loss_hkrs.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "    position_loss_hks = position_loss_hks.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "    \n",
    "    # # Add additional hinge penalty to disproportionately penalize values with abs value outside desired range\n",
    "    # hinge_loss = (torch.clamp(saved_l2s, max = 0) - 0) ** 2 + (torch.clamp(saved_l2s, min = 1) - 1) ** 2 \n",
    "    # hinge_loss = torch.where(l2_mask == 1, hinge_loss, torch.tensor(0.0)).sum()/l2_mask.sum() \n",
    "\n",
    "    loss = base_loss + position_loss_hks + position_loss_hkrs\n",
    "    ##### End loss calcaulation ######\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "del model\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efdde2c-5e09-46ec-bd36-b0fa8c1723f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_feature_targets[2, 4, 10, -50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff392f09-b993-42b2-bdcd-49f13cc632b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_feature_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ec094-6300-46c4-9a46-7430ba7db8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hkr_feature_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb9fe36-c98e-47b7-a93d-b5f78423b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_targets[4,-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b32903-d5cb-4af1-89b2-3b8fee666c09",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebffd3-8e76-4269-a75d-a07e297cb2bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Pre-train Visualizations\n",
    "test_prompts = [\n",
    "    parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my dog?'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my friend?'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'Hey idiot, what\\'s wrong with my dog?'}], True),\n",
    "]\n",
    "test_gens = [generate_fence(my_model, tokenizer, prompt = t, max_tokens = 16) for t in test_prompts]\n",
    "\n",
    "test_plots = [visualize_fence(gen['text'], gen['hks'], [30], feature_dict, 2950, 0, 5).update_layout(title = 'H<sub>10</sub>', height = 300) for gen in test_gens]\n",
    "\n",
    "for p in test_plots:\n",
    "    p.show('colab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a366a-1828-4271-9fb0-d154ed0822e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def eval_fence(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    test_ds: FenceDataSet, \n",
    "    Kf_target_values: dict,\n",
    "    Kfstart: int = 1, \n",
    "    Kfend: int = 32, \n",
    "    force_fence: bool = True, \n",
    "    batch_size: int = 10, \n",
    "    num_batches: int = 20,\n",
    "    device = 'cuda'\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Evaluation function to get test losses    \n",
    "\n",
    "    Params:\n",
    "        @model: The model to use\n",
    "        @tokenizer: The tokenizer object\n",
    "        @test_ds: The test dataset object, with a FENCE feature dict object\n",
    "        @Kf_target_values: The targeted values by layer, e.g. {'hk': [0, 1, 2], 'hkrs': [1, 2, 3]}\n",
    "        @Kfstart: The starting index layer index to track position losses (or force a FENCE) - starts at 1, not 0\n",
    "        @Kfend: The ending index layer index to track position losses \n",
    "        @force_fence: Whether to force FENCE position indices\n",
    "        @batch_size: The batch size to use for eval\n",
    "        @num_batches: The number of batches to use fo reval\n",
    "        @device: The torch device\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch_results = []\n",
    "    batches_to_eval = num_batches\n",
    "    input_count = 0\n",
    "    \n",
    "    for ix, batch in enumerate(DataLoader(test_ds, batch_size = batch_size, shuffle = True)):\n",
    "        \n",
    "        if ix >= batches_to_eval:\n",
    "            break\n",
    "            \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        feature_targets = batch['feature_targets'].to(device)\n",
    "        position_mask = batch['position_mask'].to(device)\n",
    "                \n",
    "        ##### Forward Pass ######\n",
    "        embeds_output = model.model.embed_tokens(input_ids) # B x N x D\n",
    "        hidden_state = embeds_output\n",
    "\n",
    "        B, N, D = embeds_output.shape\n",
    "        H = 32\n",
    "        Df = feature_targets.shape[1] # Total FENCE width\n",
    "        Kf = Kfend - Kfstart + 1\n",
    "\n",
    "        # Prepare SA inputs\n",
    "        position_ids = torch.arange(0, N, dtype = torch.long, device = device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        if model.model._attn_implementation == 'flash_attention_2':\n",
    "            attention_mask = mask if (mask is not None and 0 in mask) else None  # Flash attention = use default attention mask 2d\n",
    "        else: \n",
    "            attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = model.model.config.sliding_window)  # Non FA: Make a triangular attention mask to hide right context\n",
    "\n",
    "        hkr_target_values = torch.tensor(list(Kf_target_values['hkrs'].values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "        hk_target_values = torch.tensor(list(Kf_target_values['hks'].values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "\n",
    "        # Multiply it by the actual feature targets by layer\n",
    "        feature_targets_bkd = feature_targets.unsqueeze(1) # B x K x Df\n",
    "        hkr_feature_targets = hkr_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hkr_feature_targets = hkr_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "        hk_feature_targets = hk_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hk_feature_targets = hk_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "\n",
    "        # Saved_hk2s will be of shape B x K x N x Df\n",
    "        saved_hkrs = None\n",
    "        saved_hks = None\n",
    "        for l, layer in enumerate(model.model.layers):\n",
    "            \n",
    "            # SA\n",
    "            residual = hidden_state\n",
    "            sa_input = layer.input_layernorm(hidden_state)\n",
    "            sa_output = layer.self_attn(sa_input, attention_mask, position_ids)[0]\n",
    "            \n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_attn_dropout(sa_output)    \n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                if force_fence: # Forcibly set H_K^R\n",
    "                    # To extract the right layer from hkr_feature_targets:\n",
    "                    # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "                    # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "                    # - => k = l + 1 - Kfstart\n",
    "                    hidden_state[:, :, -Df: ] = hkr_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df\n",
    "\n",
    "                this_hidden_state = hidden_state[:, :, -Df:].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hkrs is None:\n",
    "                    saved_hkrs = this_hidden_state\n",
    "                else:\n",
    "                    saved_hkrs = torch.cat((saved_hkrs, this_hidden_state), dim = 1)\n",
    "\n",
    "            # MLP\n",
    "            residual = hidden_state\n",
    "            hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "            mlp_output = layer.mlp(hidden_state)\n",
    "\n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_mlp_dropout(mlp_output)\n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                if force_fence: # Forcibly set H_K\n",
    "                    hidden_state[:, :, -Df: ] = hk_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df\n",
    "\n",
    "                this_hidden_state = hidden_state[:, :, -Df:].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hks is None:\n",
    "                    saved_hks = this_hidden_state\n",
    "                else:\n",
    "                    saved_hks = torch.cat((saved_hks, this_hidden_state), dim = 1)\n",
    "                    \n",
    "\n",
    "        # RMS norm the final transformer layer output\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "\n",
    "        # Run LM head\n",
    "        logits = model.lm_head(hidden_state).float() # B x N x D\n",
    "        #### End Forward Pass ######\n",
    "\n",
    "        ##### Calculate loss #####\n",
    "        # Mask loss anywhere where the input ids are pad tokens\n",
    "        label_ids = torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids)\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous() # Remove the last token from the sequence\n",
    "        shift_labels = label_ids[..., 1:].contiguous() # Remove the first token from the sequence\n",
    "        # Flatten tokens\n",
    "        loss_fct = CrossEntropyLoss(ignore_index = -100)\n",
    "        shift_logits = shift_logits.view(-1, model.config.vocab_size)\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        # Enable model parallelism\n",
    "        # shift_labels = shift_labels.to(device)\n",
    "        base_loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        # Calculate the position loss\n",
    "        # Apply the position target mask\n",
    "        position_mask_bknd = position_mask.unsqueeze(1).unsqueeze(-1).expand(B, Kf, N, Df) # B x K X N x Df (recycles across K and Df)\n",
    "\n",
    "        # Creates a B x K x N x Df tensor with targets differing by K and Df, and values masked (-100) or not (0, .25, .5, etc.) varying by N\n",
    "        # MAPE Loss\n",
    "        position_loss_hkrs = torch.where(position_mask_bknd == 1, torch.abs((saved_hkrs - hkr_feature_targets)/(hkr_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "        position_loss_hks = torch.where(position_mask_bknd == 1, torch.abs((saved_hks - hk_feature_targets)/(hk_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "        \n",
    "        position_loss_hkrs_by_k = position_loss_hkrs.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "        position_loss_hks_by_k = position_loss_hks.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "\n",
    "        position_loss_hkrs_by_dim = position_loss_hkrs.sum(dim = (0, 1, 2))/position_mask_bknd.sum(dim = (0, 1, 2))\n",
    "        position_loss_hks_by_dim = position_loss_hks.sum(dim = (0, 1, 2))/position_mask_bknd.sum(dim = (0, 1, 2))\n",
    "\n",
    "        position_loss_hkrs = position_loss_hkrs.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "        position_loss_hks = position_loss_hks.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "        \n",
    "        loss = base_loss + position_loss_hks + position_loss_hkrs\n",
    "        ##### End loss calcaulation ######\n",
    "        \n",
    "        dim_ix = 0\n",
    "        position_loss_hks_by_feature = {}\n",
    "        for fname, fdim in test_ds.feature_dict.items():\n",
    "            position_loss_hks_by_feature[fname] = position_loss_hks_by_dim[dim_ix : dim_ix + fdim].mean().detach().cpu()\n",
    "            dim_ix = dim_ix + fdim\n",
    "\n",
    "        batch_results.append({\n",
    "            'base_loss': base_loss.detach().cpu().item(),\n",
    "            'position_loss_hkrs': position_loss_hkrs.detach().cpu().item(),\n",
    "            'position_loss_hks': position_loss_hks.detach().cpu().item(),\n",
    "            'position_loss_hkrs_by_k': dict(zip([i + Kfstart for i in list(range(Kfstart - 1, Kfend))], position_loss_hkrs_by_k.detach().cpu().tolist())),\n",
    "            'position_loss_hks_by_k': dict(zip([i + Kfstart for i in list(range(Kfstart - 1, Kfend))], position_loss_hks_by_k.detach().cpu().tolist())),\n",
    "            'position_loss_hks_by_feature': position_loss_hks_by_feature\n",
    "        })\n",
    "        input_count = input_count + B\n",
    "\n",
    "    return {\n",
    "        'input_count': input_count,\n",
    "        'base_loss': np.mean([b['base_loss'] for b in batch_results]),\n",
    "        'position_loss_hkrs': np.mean([b['position_loss_hkrs'] for b in batch_results]),\n",
    "        'position_loss_hks': np.mean([b['position_loss_hks'] for b in batch_results]),\n",
    "        'position_loss_hkrs_by_k': {k: np.mean([b['position_loss_hkrs_by_k'][k] for b in batch_results]) for k in batch_results[0]['position_loss_hkrs_by_k'].keys()},\n",
    "        'position_loss_hks_by_k': {k: np.mean([b['position_loss_hks_by_k'][k] for b in batch_results]) for k in batch_results[0]['position_loss_hkrs_by_k'].keys()},\n",
    "        'position_loss_hks_by_feature': {\n",
    "            fname: torch.stack([b['position_loss_hks_by_feature'][fname] for b in batch_results], dim = 0).mean().item()\n",
    "            for fname in test_ds.feature_dict.keys()\n",
    "        }\n",
    "    }\n",
    "\n",
    "eval_fence(my_model, tokenizer, test_ds, Kf_target_values, Kfstart = Kfstart, Kfend = Kfend, force_fence = False, batch_size = 10, num_batches = 5, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f9fce-519e-443d-b138-1372da98a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nosup_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688b604-801c-4d75-a9d8-e7a4b2ca986b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# Investigate lower LR\n",
    "# Investigate partial position-loss targeting\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, my_model.parameters()), lr = 3e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 10000, gamma = 0.8)\n",
    "train_with_force_fence = True\n",
    "\n",
    "my_model.train()\n",
    "\n",
    "step = 0\n",
    "max_grad_norm = 5.0  # Set the value for gradient clipping\n",
    "\n",
    "# STEPS\n",
    "# 1-100: Nothing, logging purposes only\n",
    "# 100-5k: Always force forcing FENCE [no position loss] - This is just to get the model to adjust to force FENCE (control only!)\n",
    "# 5k-10k: Force FENCE 90% of the time, remaining 10% trains position loss w/weight 2\n",
    "# 10k-15k: Force FENCE 80% of the time, remaining 20% trains position loss w/weight 5\n",
    "# 15k-20k: Force FENCE 70% of the time, remaining 30% trains position loss w/weight 10\n",
    "# 20k-30k: Force FENCE 50% of the time, remaining 50% trains position loss w/weight 20\n",
    "# 30k+: Force FENCE 50% of the time, remaining 50% trains position loss w/weight 25\n",
    "for epoch_ix in range(0, 100):\n",
    "    \n",
    "    for batch_ix, batch in enumerate(train_dl):\n",
    "\n",
    "        # If force FENCE (default), then there is no position loss\n",
    "        force_fence = (\n",
    "            train_with_force_fence and (\n",
    "                (step < 5000) or\n",
    "                (step >= 5000 and step < 10000 and step % 10 >= 1) or\n",
    "                (step >= 10000 and step < 15000 and step % 10 >= 2) or\n",
    "                (step >= 15000 and step < 20000 and step % 10 >= 3) or\n",
    "                (step >= 20000 and step % 10 >= 5)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if step < 4:\n",
    "            check_memory()\n",
    "\n",
    "        # If not force FENCE (i.e., there exists some position loss, then train with no-surprise data)\n",
    "        if force_fence == False:\n",
    "            batch = next(iter(train_nosup_dl))\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        feature_targets = batch['feature_targets'].to(device)\n",
    "        position_mask = batch['position_mask'].to(device)\n",
    "\n",
    "        ##### Forward Pass ######\n",
    "        embeds_output = my_model.model.embed_tokens(input_ids) # B x N x D\n",
    "        hidden_state = embeds_output\n",
    "\n",
    "        # Execute transformers layers\n",
    "        # B = batch size, N = token length, D = token dim, Dh = token per-head dim, H = number of heads, K = # transformer blocks\n",
    "        # Df = total FENCE dimensino width\n",
    "        # Kfstart, Kfend = starting and ending indices for transformer blocks to include in FENCE (indices starts with 1, not 0)\n",
    "        # Kf = number of transformer blocks to include in FENCE\n",
    "        B, N, D = embeds_output.shape\n",
    "        H = 32\n",
    "        Dh = int(D/H)\n",
    "        K = 32 \n",
    "\n",
    "        Df = feature_targets.shape[1] # Total FENCE width\n",
    "        Kf = Kfend - Kfstart + 1\n",
    "\n",
    "        # Prepare SA inputs\n",
    "        position_ids = torch.arange(0, N, dtype = torch.long, device = device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        if my_model.model._attn_implementation == 'flash_attention_2':\n",
    "            attention_mask = mask if (mask is not None and 0 in mask) else None  # Flash attention = use default attention mask 2d\n",
    "        else: \n",
    "            attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = my_model.model.config.sliding_window)  # Non FA: Make a triangular attention mask to hide right context\n",
    "\n",
    "        hkr_target_values = torch.tensor(list(Kf_target_values['hkrs'].values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "        hk_target_values = torch.tensor(list(Kf_target_values['hks'].values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "\n",
    "        # Multiply it by the actual feature targets by layer\n",
    "        feature_targets_bkd = feature_targets.unsqueeze(1) # B x K x Df\n",
    "        hkr_feature_targets = hkr_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hkr_feature_targets = hkr_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "        hk_feature_targets = hk_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hk_feature_targets = hk_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "\n",
    "        # Saved_hk2s will be of shape B x K x N x Df\n",
    "        saved_hkrs = None\n",
    "        saved_hks = None\n",
    "        for l, layer in enumerate(my_model.model.layers):\n",
    "            \n",
    "            # SA\n",
    "            residual = hidden_state\n",
    "            hidden_state = layer.input_layernorm(hidden_state)\n",
    "            hidden_state = layer.self_attn(hidden_state, attention_mask, position_ids)[0]\n",
    "            \n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_attn_dropout(hidden_state)    \n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                if force_fence and step >= 100: # Forcibly set H_K^R\n",
    "                    # To extract the right layer from hkr_feature_targets:\n",
    "                    # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "                    # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "                    # - => k = l + 1 - Kfstart\n",
    "                    hidden_state[:, :, -Df: ] = hkr_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                this_hidden_state = hidden_state[:, :, -Df:].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hkrs is None:\n",
    "                    saved_hkrs = this_hidden_state\n",
    "                else:\n",
    "                    saved_hkrs = torch.cat((saved_hkrs, this_hidden_state), dim = 1)\n",
    "\n",
    "            # MLP\n",
    "            residual = hidden_state\n",
    "            hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "            hidden_state = layer.mlp(hidden_state)\n",
    "\n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_mlp_dropout(hidden_state)\n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                if force_fence and step >= 100: # Forcibly set H_K\n",
    "                    hidden_state[:, :, -Df: ] = hk_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df                \n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                this_hidden_state = hidden_state[:, :, -Df:].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hks is None:\n",
    "                    saved_hks = this_hidden_state\n",
    "                else:\n",
    "                    saved_hks = torch.cat((saved_hks, this_hidden_state), dim = 1)\n",
    "\n",
    "        # RMS norm the final transformer layer output\n",
    "        hidden_state = my_model.model.norm(hidden_state)\n",
    "\n",
    "        # Run LM head\n",
    "        logits = my_model.lm_head(hidden_state).float() # B x N x D\n",
    "        #### End Forward Pass ######\n",
    "\n",
    "        ##### Calculate loss #####\n",
    "        # Mask loss anywhere where the input ids are pad tokens\n",
    "        label_ids = torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids)\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous() # Remove the last token from the sequence\n",
    "        shift_labels = label_ids[..., 1:].contiguous() # Remove the first token from the sequence\n",
    "        # Flatten tokens\n",
    "        loss_fct = CrossEntropyLoss(ignore_index = -100)\n",
    "        shift_logits = shift_logits.view(-1, my_model.config.vocab_size)\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        # Enable model parallelism\n",
    "        # shift_labels = shift_labels.to(device)\n",
    "        base_loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        # Calculate the position loss\n",
    "        # Apply the position target mask\n",
    "        position_mask_bknd = position_mask.unsqueeze(1).unsqueeze(-1).expand(B, Kf, N, Df) # B x K X N x Df (recycles across K and Df)\n",
    "\n",
    "        position_loss_hkrs = torch.where(position_mask_bknd == 1, torch.abs((saved_hkrs - hkr_feature_targets)/(hkr_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "        position_loss_hks = torch.where(position_mask_bknd == 1, torch.abs((saved_hks - hk_feature_targets)/(hk_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "        \n",
    "        position_loss_hkrs_by_k = position_loss_hkrs.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "        position_loss_hks_by_k = position_loss_hks.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "\n",
    "        # Maybe consider using MAPE\n",
    "        position_loss_hkrs = position_loss_hkrs.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "        position_loss_hks = position_loss_hks.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "        \n",
    "        # # Add additional hinge penalty to disproportionately penalize values with abs value outside desired range\n",
    "        # hinge_loss = (torch.clamp(saved_l2s, max = 0) - 0) ** 2 + (torch.clamp(saved_l2s, min = 1) - 1) ** 2 \n",
    "        # hinge_loss = torch.where(l2_mask == 1, hinge_loss, torch.tensor(0.0)).sum()/l2_mask.sum() \n",
    "        if step < 5000:\n",
    "            loss = base_loss\n",
    "        elif step >= 5000 and step < 10000:\n",
    "            loss = base_loss + 2 * position_loss_hks + 2 * position_loss_hkrs\n",
    "        elif step >= 10000 and step < 15000:\n",
    "            loss = base_loss + 5 * position_loss_hks + 5 * position_loss_hkrs\n",
    "        elif step >= 15000 and step < 20000:\n",
    "            loss = base_loss + 10 * position_loss_hks + 10 * position_loss_hkrs\n",
    "        elif step >= 20000 and step < 30000:\n",
    "            loss = base_loss + 20 * position_loss_hks + 20 * position_loss_hkrs\n",
    "        else:\n",
    "            loss = base_loss + 25 * position_loss_hks + 25 * position_loss_hkrs\n",
    "        ##### End loss calcaulation ######\n",
    "\n",
    "        ##### Logging #####\n",
    "        if step % 50 == 0:\n",
    "            print(np.round(base_loss.detach().cpu().item(), 2), np.round(position_loss_hks.detach().cpu().item(), 2))\n",
    "            \n",
    "        logging_dict = {\n",
    "            'epoch': epoch_ix,\n",
    "            'step': step,\n",
    "            'lr': optimizer.param_groups[0]['lr'],\n",
    "            'train': {\n",
    "                'base_loss': base_loss.detach().cpu().item(),\n",
    "                'position_loss_hkrs': position_loss_hkrs.detach().cpu().item(),\n",
    "                'position_loss_hks': position_loss_hks.detach().cpu().item(),\n",
    "                'position_loss_hkrs_by_k': dict(zip([i + Kfstart for i in list(range(Kfstart - 1, Kfend))], position_loss_hkrs_by_k.detach().cpu().tolist())),\n",
    "                'position_loss_hks_by_k': dict(zip([i + Kfstart for i in list(range(Kfstart - 1, Kfend))], position_loss_hks_by_k.detach().cpu().tolist())),\n",
    "                'total_loss': loss.detach().cpu().item()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            logging_dict = {\n",
    "                **logging_dict, \n",
    "                **{'test_forced': eval_fence(my_model, tokenizer, test_ds, Kf_target_values = Kf_target_values, Kfstart = Kfstart, Kfend = Kfend, force_fence = True, batch_size = 10, num_batches = 20, device = device)},\n",
    "                **{'test_unforced': eval_fence(my_model, tokenizer, test_ds, Kf_target_values = Kf_target_values, Kfstart = Kfstart, Kfend = Kfend, force_fence = False, batch_size = 10, num_batches = 20, device = device)}\n",
    "            }\n",
    "            my_model.train()\n",
    "            \n",
    "        # Log losses\n",
    "        if USE_WANDB:\n",
    "            wandb.log(logging_dict)\n",
    "        ##### End Logging #####\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(my_model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Step the scheduler to decay the learning rate\n",
    "        step = step + 1\n",
    "\n",
    "        del input_ids, mask, feature_targets, position_mask, embeds_output, hidden_state, this_hidden_state, logits, residual\n",
    "        del label_ids, shift_logits, shift_labels, base_loss, position_mask_bknd, position_loss_hkrs, position_loss_hks, position_loss_hkrs_by_k, position_loss_hks_by_k\n",
    "        del hkr_target_values, hk_target_values, feature_targets_bkd, hkr_feature_targets, hk_feature_targets\n",
    "        del loss, position_ids, attention_mask\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if epoch_ix % 2 == 0:\n",
    "        torch.save(my_model.state_dict(), f\"{SAVE_DIR}/e{str(epoch_ix + 1)}.pt\")\n",
    "    \n",
    "    # nondog = visualize_fence(list(range(10, 20)), my_model, tokenizer, parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my friend?'}], True), train_ds.feature_dict, max_tokens = 16)    \n",
    "    # dog = visualize_fence(list(range(10, 20)), my_model, tokenizer, parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my dog?'}], True), train_ds.feature_dict, max_tokens = 16)\n",
    "    # angrydog = visualize_fence(list(range(10, 20)), my_model, tokenizer, parse_phi([{'role': 'user', 'content': 'My dogs and cats make me so MAD!'}], True), train_ds.feature_dict, max_tokens = 16)\n",
    "\n",
    "    # nondog[2].write_html(f\"{SAVE_DIR}/{str(epoch_ix + 1)}_nondog.html\")\n",
    "    # dog[2].write_html(f\"{SAVE_DIR}/{str(epoch_ix + 1)}_yesdog.html\")\n",
    "\n",
    "    # angrydog[2].write_html(f\"{SAVE_DIR}/{str(epoch_ix + 1)}_angrydog.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb5c99-c2f8-44de-9326-70b20eb00729",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(Kf_target_values['hkrs'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c1d90-9ed6-4d75-9020-59ba20533d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = ['input_ids', 'mask', 'feature_targets', 'position_mask', 'embeds_output', 'hidden_state', 'this_hidden_state', 'logits', 'residual'\n",
    "             'label_ids', 'shift_logits', 'shift_labels', 'base_loss', 'position_mask_bknd', 'position_loss_hkrs', 'position_loss_hks',  'position_loss_hkrs_by_k', 'position_loss_hks_by_k',\n",
    "             'hkr_target_values', 'hk_target_values', 'feature_targets_bkd', 'hkr_feature_targets', 'hk_feature_targets',\n",
    "             'loss', 'position_ids', 'attention_mask',\n",
    "             'optimizer', 'scheduler']\n",
    "\n",
    "for var in to_delete:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b6207-8fc9-408b-bb4c-b6dee3f45355",
   "metadata": {},
   "outputs": [],
   "source": [
    "nondog = visualize_fence(list(range(20, 21)), my_model, tokenizer, parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling?'}], True), train_ds.feature_dict, max_tokens = 32)\n",
    "nondog[2].show('colab')\n",
    "\n",
    "dog = visualize_fence(list(range(20, 21)), my_model, tokenizer, parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my dog?'}], True), train_ds.feature_dict, max_tokens = 32)\n",
    "dog[2].show('colab')\n",
    "\n",
    "table = wandb.Table(columns = ['Nondog', 'Dog'])\n",
    "nondog[2].write_html('./fig1.html')\n",
    "dog[2].write_html('./fig2.html')\n",
    "table.add_data(wandb.Html('./fig1.html'), wandb.Html('./fig2.html'))\n",
    "run.log({\"name\": 'Trained - Nondog/dog, Layer 20'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
