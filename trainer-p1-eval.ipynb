{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582de48a-c44f-47d5-868b-799c4ddd3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML, Markdown\n",
    "import plotly.express as px\n",
    "import os\n",
    "import wandb \n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib\n",
    "import pathlib\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from helpers.phi3.phi3 import Phi3Config, Phi3ForCausalLM, _prepare_4d_causal_attention_mask\n",
    "from helpers.phi3.parse import parse_phi\n",
    "from helpers.memory import check_memory\n",
    "\n",
    "load_dotenv('secrets.env')\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa2490-bc9f-4c66-a380-8d7493386a39",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9ab74-27cd-4347-8b94-2ce2ada4cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_implementation = 'flash_attention_2' # None/flash_attention_2\n",
    "\n",
    "# Load Model\n",
    "# Padding side not important EXCEPT for flash attention, needs to be left\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-4k-instruct', add_eos_token = False, add_bos_token = False, padding_side = 'left') \n",
    "\n",
    "# Load the usual model from HF transformers - attn_implementation = None to disable flash attention\n",
    "my_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'microsoft/Phi-3-mini-4k-instruct',\n",
    "    device_map = device,\n",
    "    trust_remote_code = True, \n",
    "    torch_dtype = torch.bfloat16, \n",
    "    attn_implementation = attn_implementation\n",
    "    ).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f8ac3-eb20-4735-92ea-fb009bfaefc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(my_model.state_dict(), f'./models/phi3_base.pt')\n",
    "my_model.load_state_dict(torch.load('./models/20241003T1957/e11.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df19e7-0286-4106-864e-e7900ab9528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FenceParams():\n",
    "    def __init__(self, fence_dict: dict[str, int], Kfstart: int, Kfend: int, hkr_target_values: dict[int, float], hk_target_values: dict[int, float]):\n",
    "        \"\"\"\n",
    "        Creates a new FENCE Params object\n",
    "        \n",
    "        Description:\n",
    "            Creates a FENCE params object which stores the FENCE dict and the position loss target values\n",
    "        \n",
    "        Params: \n",
    "            @fence_dict: A dict of features and their corresponding fence dimensions, e.g. {'dogs': (3065, 3068), 'cats': (3061, 3064)}. \n",
    "             - These dimensions are 1-indexed and inclusive of both the start and ending numbers passed into the tuples. (3061, 3064) means dimensions 3061, 3062, 3063, and 3064.\n",
    "            @Kfstart: The index (1-indexed) of the first transformer block with which to calculate position loss.\n",
    "            @Kfend: The index (1-indexed) of the last transformer block with which to calculate position loss.\n",
    "            @hkr_target_values: A dict where the keys are the layer index (1-indexed), and the values representing the target FENCE values for each layer's residual stream output.\n",
    "            @hk_target_values:  A dict where the keys are the layer index (1-indexed), and the values representing the target FENCE values for each layer's transformer block output.\n",
    "        \"\"\"\n",
    "        if not all(r1[1] < r2[0] for r1, r2 in zip(sorted(fence_dict.values()), sorted(fence_dict.values())[1:])):\n",
    "            raise ValueError('FENCE dict contains overlapping values')\n",
    "\n",
    "        if not all(curr[0] > prev[0] for prev, curr in zip(fence_dict.values(), list(fence_dict.values())[1:])):\n",
    "            raise ValueError('FENCE dict not passed in order!')\n",
    "\n",
    "        self.fence_dict = fence_dict\n",
    "        self.Kfstart = Kfstart\n",
    "        self.Kfend = Kfend\n",
    "        self.Kf = Kfend - Kfstart + 1\n",
    "        self.hkr_target_values = hkr_target_values\n",
    "        self.hk_target_values = hk_target_values\n",
    "        self.Df = sum([v[1] - v[0] + 1 for k, v in fence_dict.items()])\n",
    "        self.Df_indices = [i - 1 for start, end in fence_dict.values() for i in range(start, end + 1)]\n",
    "        # Dfmask = torch.zeros(D)\n",
    "        # for k, v in fence_dict.items():\n",
    "        #     Dfmask[fence_dict[k][0] - 1:fence_dict[k][1]] = 1\n",
    "        # self.Dfmask = Dfmask\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'fence_dict': self.fence_dict,\n",
    "            'Kfstart': self.Kfstart,\n",
    "            'Kfend': self.Kfend,\n",
    "            'hkr_target_values': self.hkr_target_values,\n",
    "            'hk_target_values': self.hk_target_values,\n",
    "            'Df': self.Df,\n",
    "            'Df_indices': self.Df_indices\n",
    "        }\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"FENCE Params Object:\\n\"\n",
    "                f\"  FENCE Dictionary: {self.fence_dict}\\n\"\n",
    "                f\"  Number of position loss layers: {self.Kf} (Layer {self.Kfstart} to layer {self.Kfend})\\n\"\n",
    "                f\"  Df: {self.Df}  \\n\"\n",
    "                f\"  hkr_target_values: {self.hkr_target_values}\\n\"\n",
    "                f\"  hk_target_values: {self.hk_target_values}\\n\"\n",
    "                f\"  Df_indices (0-indexed): {self.Df_indices}\")\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "# Pass indices starting at 1\n",
    "Kfstart = 1\n",
    "Kfend = 32\n",
    "fence_params = FenceParams(\n",
    "    fence_dict = {\n",
    "        'programming': (2980, 2983), # 4\n",
    "        'food': (3010, 3011), # 2\n",
    "        'animals': (3030, 3032), # 3\n",
    "        'dogs': (3050, 3053), # 4\n",
    "        'cats': (3070, 3070) # 1\n",
    "    },\n",
    "    Kfstart = 1,\n",
    "    Kfend = 32,\n",
    "    hkr_target_values = {Kfstart + j - 1: (j - 1) * .25 + .25/2 for j in range(Kfstart, Kfend + 1)},\n",
    "    hk_target_values = {Kfstart + j - 1: (j - 1) * .25 + .25 for j in range(Kfstart, Kfend + 1)}\n",
    ")\n",
    "\n",
    "fence_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b6023-bd49-463f-84e8-1e0e6c5b320f",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e5316-2523-4a94-8490-ff7b78f4fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.eval'))\n",
    "from helpers.fence.eval import generate_fence\n",
    "\n",
    "# Test\n",
    "test_prompts = [\n",
    "    '<s>My favorite animal is',\n",
    "    '<s>I spent the afternoon with python',\n",
    "    '<s>Let\\'s cook a great, healthy recipe for my pet',\n",
    "    parse_phi([{'role': 'user', 'content': 'I want to cook a great, healthy pet recipe!'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'Where should I take my dog hiking?'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'What should I bring to take my friend hiking?'}], True)\n",
    "]\n",
    "\n",
    "test_gens = [generate_fence(my_model, tokenizer, prompt = test_prompt, max_tokens = 8) for test_prompt in test_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2caf08-28a6-44a3-8727-9596f88094e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.visualize'))\n",
    "from helpers.fence.visualize import visualize_fence\n",
    "\n",
    "for l in [10]:\n",
    "    visualize_fence(\n",
    "        test_gens[2]['text'],\n",
    "        test_gens[2]['hks'],\n",
    "        [l],\n",
    "        fence_params.fence_dict,\n",
    "        start_dim = 2970, end_dim = 3072,\n",
    "        min_range = 0, max_range = fence_params.hk_target_values[l]\n",
    "    ).update_layout(title = 'H<sub>' + str(l) + '</sub>', height = 330).show('colab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f8d1cc-4f85-4f81-8622-acc5b383e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fence_params.feature_classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66377781-c30c-45a6-bb5f-a789bb169604",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_with_force_fence(model, tokenizer, fence_params: FenceParams, force_pos: list[str], force_neg: list[str], prompt: str, max_tokens = 128, device = 'cuda'):\n",
    "    model.eval()\n",
    "    generated_tokens = 0\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']\n",
    "    mask = tokenizer(prompt, return_tensors = 'pt').to(device)['attention_mask']\n",
    "    \n",
    "    Kfstart = fence_params.Kfstart\n",
    "    Kfend = fence_params.Kfend\n",
    "    Kf = fence_params.Kf\n",
    "    Df = fence_params.Df\n",
    "    Df_indices = fence_params.Df_indices # 0-indexed Df indices used for extracting the Df values\n",
    "\n",
    "    # Prepare force features        \n",
    "    feature_targets_for_ex = torch.cat([\n",
    "        torch.ones(v[1] - v[0] + 1) if k in force_pos else \n",
    "        torch.zeros(v[1] - v[0] + 1) if k in force_neg else\n",
    "        -torch.ones(v[1] - v[0] + 1) * 9999999999\n",
    "        for k, v in fence_params.fence_dict.items()\n",
    "    ], dim = 0)\n",
    "    feature_targets = feature_targets_for_ex.unsqueeze(0).to(input_ids.device)\n",
    "    while True:\n",
    "        \n",
    "        ##### Forward Pass ######\n",
    "        embeds_output = my_model.model.embed_tokens(input_ids) # B x N x D\n",
    "        hidden_state = embeds_output\n",
    "\n",
    "        # Execute transformers layers\n",
    "        # B = batch size, N = token length, D = token dim, Dh = token per-head dim, H = number of heads, K = # transformer blocks\n",
    "        # Df = total FENCE dimensino width\n",
    "        # Kfstart, Kfend = starting and ending indices for transformer blocks to include in FENCE (indices starts with 1, not 0)\n",
    "        # Kf = number of transformer blocks to include in FENCE\n",
    "        B, N, D = embeds_output.shape\n",
    "\n",
    "        # Prepare SA inputs\n",
    "        position_ids = torch.arange(0, N, dtype = torch.long, device = device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        if my_model.model._attn_implementation == 'flash_attention_2':\n",
    "            attention_mask = mask if (mask is not None and 0 in mask) else None  # Flash attention = use default attention mask 2d\n",
    "        else: \n",
    "            attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = my_model.model.config.sliding_window)  # Non FA: Make a triangular attention mask to hide right context\n",
    "        \n",
    "        # Create Hkr and Hk target values for position loss calculation when feature_target = 1\n",
    "        hkr_target_values = torch.tensor(list(fence_params.hkr_target_values.values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "        hk_target_values = torch.tensor(list(fence_params.hk_target_values.values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "    \n",
    "        # Multiply it by the actual feature targets by layer. Note that this does not apply position masking so all values are FIXED across Ns\n",
    "        feature_targets_bkd = feature_targets.unsqueeze(1).bfloat16() # B x K x Df\n",
    "        hkr_feature_targets = hkr_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hkr_feature_targets = hkr_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "        hk_feature_targets = hk_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hk_feature_targets = hk_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "\n",
    "        # Saved_hk2s will be of shape B x K x N x Df\n",
    "        saved_hkrs = None\n",
    "        saved_hks = None\n",
    "        for l, layer in enumerate(my_model.model.layers):\n",
    "            \n",
    "            # SA\n",
    "            residual = hidden_state\n",
    "            hidden_state = layer.input_layernorm(hidden_state)\n",
    "            hidden_state = layer.self_attn(hidden_state, attention_mask, position_ids)[0]\n",
    "            \n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_attn_dropout(hidden_state)    \n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                # Forcibly set H_K^R\n",
    "                # To extract the right layer from hkr_feature_targets:\n",
    "                # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "                # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "                # - => k = l + 1 - Kfstart\n",
    "                hidden_state[:, :, Df_indices] = torch.where(\n",
    "                    hkr_feature_targets[:, l + 1 - Kfstart, :, :] > -100,\n",
    "                    hkr_feature_targets[:, l + 1 - Kfstart, :, :],\n",
    "                    hidden_state[:, :, Df_indices]\n",
    "                )\n",
    "                this_hidden_state = hidden_state[:, :, Df_indices].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hkrs is None:\n",
    "                    saved_hkrs = this_hidden_state\n",
    "                else:\n",
    "                    saved_hkrs = torch.cat((saved_hkrs, this_hidden_state), dim = 1)\n",
    "\n",
    "            # MLP\n",
    "            residual = hidden_state\n",
    "            hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "            hidden_state = layer.mlp(hidden_state)\n",
    "\n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_mlp_dropout(hidden_state)\n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                hidden_state[:, :, Df_indices] = torch.where(\n",
    "                    hk_feature_targets[:, l + 1 - Kfstart, :, :] > -100,\n",
    "                    hk_feature_targets[:, l + 1 - Kfstart, :, :],\n",
    "                    hidden_state[:, :, Df_indices]\n",
    "                )\n",
    "    \n",
    "                this_hidden_state = hidden_state[:, :, Df_indices].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hks is None:\n",
    "                    saved_hks = this_hidden_state\n",
    "                else:\n",
    "                    saved_hks = torch.cat((saved_hks, this_hidden_state), dim = 1)\n",
    "                    \n",
    "\n",
    "        # RMS norm the final transformer layer output\n",
    "        hidden_state = my_model.model.norm(hidden_state)\n",
    "\n",
    "        # Run LM head\n",
    "        logits = my_model.lm_head(hidden_state).float() # B x N x D\n",
    "        #### End Forward Pass ######\n",
    "\n",
    "        # Get argmax tokens + concatenate onto previous tokens\n",
    "        output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "        input_ids = torch.cat((input_ids, output_token.view(1, 1)), dim = 1)\n",
    "\n",
    "        # Break while loop if EOS or generation > max tokens\n",
    "        generated_tokens = generated_tokens + 1\n",
    "        if output_token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end|>\")] or generated_tokens >= max_tokens:\n",
    "            break\n",
    "\n",
    "    # Use it on the last pasa\n",
    "    cleaned_dims = [h.cpu().to(torch.float16).numpy() for h in saved_hks]\n",
    "\n",
    "    final_output = input_ids.squeeze()\n",
    "    return final_output, cleaned_dims\n",
    "\n",
    "input_prompt = parse_phi(\n",
    "    [{'role': 'user', 'content': 'Tell me a funny story!'}],\n",
    "    True\n",
    "    )\n",
    "\n",
    "\n",
    "prompt_combinations = [\n",
    "    # (parse_phi([{'role': 'user', 'content': 'Tell me a funny story!'}], True),  [], []),\n",
    "    # (parse_phi([{'role': 'user', 'content': 'Tell me a funny story!'}], True),  ['animals', 'dogs'], []),\n",
    "    # (parse_phi([{'role': 'user', 'content': 'Tell me a funny story!'}], True),  ['animals'], ['dogs']),\n",
    "    # (parse_phi([{'role': 'user', 'content': 'Tell me a funny story!'}], True),  ['programming', 'animals', 'dogs'], [])\n",
    "    (parse_phi([{'role': 'user', 'content': 'What\\'s your favorite animal?'}], True),  [], []),\n",
    "    (parse_phi([{'role': 'user', 'content': 'What\\'s your favorite animal?'}], True),  ['food'], ['animals']),\n",
    "    (parse_phi([{'role': 'user', 'content': 'What\\'s your favorite animal?'}], True),  ['programming'], ['animals']),\n",
    "    (parse_phi([{'role': 'user', 'content': 'What\\'s your favorite animal?'}], True),  ['animals', 'dogs'], [])\n",
    "]\n",
    "\n",
    "for p in prompt_combinations:\n",
    "    \n",
    "    input_prompt = p[0]\n",
    "    # missing = [item for item in ['programming', 'food', 'animals', 'dogs', 'cats'] if item not in p[1]]\n",
    "    \n",
    "    my_output, states_by_layer = generate_with_force_fence(\n",
    "        model = my_model, \n",
    "        tokenizer = tokenizer,\n",
    "        fence_params = fence_params,\n",
    "        force_pos = p[1], \n",
    "        force_neg = p[2],\n",
    "        prompt = input_prompt,\n",
    "        max_tokens = 64,\n",
    "        device = device\n",
    "    )\n",
    "    \n",
    "    input_tokens = tokenizer(input_prompt, return_tensors = 'pt')\n",
    "    display(HTML(\n",
    "        '<div style=\"padding: 1rem 2rem; width: 35rem; background-color:honeydew\">' +\n",
    "            '<h5 style=\"margin-top:4px;margin-bottom:4px\">Forced positive classifications: ' + (', '.join(p[1]) if len(p[1]) > 0 else 'none') +'  </h5>' +\n",
    "            '<h5 style=\"margin-top:4px;margin-bottom:4px\">Forced negative classifications: ' + (', '.join(p[2]) if len(p[2]) > 0 else 'none') +'  </h5>' +\n",
    "\n",
    "            '<span style=\"color:green\">' + tokenizer.batch_decode(input_tokens['input_ids'])[0][3:] + '</span> ' + \n",
    "            '<span style=\"color:red\">' + tokenizer.decode(my_output[input_tokens['input_ids'].size()[1]:]) + '</span>' +\n",
    "        '</div>'\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
