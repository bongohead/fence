{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb0bcec-cadd-4ec1-8294-5a57b77785c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "v5.0 (10/8/24):\n",
    "- Training with cleaner DS\n",
    "- Training with variable FENECE positions/dimensions\n",
    "\n",
    "v5.1 (10/29/24):\n",
    "- First round test of modularity loss\n",
    "- Single-dimension FENCE region per feature\n",
    "- Highly restricted and memory-efficient modularity loss for interactions with D=3050 (dogs,cats,animals region) only \n",
    "\n",
    "Notes:\n",
    "- Each iteration takes 1s with .65/.35 split between backwards and forward (10/29/24)\n",
    "- Adding modularity loss adds ~.10s/||Df||\n",
    "~ TBD: Consider alternative normalization schemes as well as non-L1 modularity losses (brain costs scale around distance^1.5)\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a93d9e37-2fbc-49f6-9c89-78be8199302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML, Markdown\n",
    "import plotly.express as px\n",
    "import os\n",
    "import wandb \n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib\n",
    "import pathlib\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from helpers.phi3.phi3 import Phi3Config, Phi3ForCausalLM, _prepare_4d_causal_attention_mask\n",
    "from helpers.phi3.parse import parse_phi\n",
    "from helpers.memory import check_memory\n",
    "\n",
    "load_dotenv('secrets.env')\n",
    "device = 'cuda'\n",
    "\n",
    "RUN_ID = f\"{datetime.now(pytz.timezone('US/Eastern')).strftime('%Y%m%dT%H%M')}\"\n",
    "SAVE_DIR = f\"./models/{RUN_ID}\"\n",
    "USE_WANDB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d86416",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f80fa6-a7af-4fdd-9e80-7663f76099de",
   "metadata": {},
   "source": [
    "## Setup Save Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0e751c-7557-4647-a01b-f301e1aae180",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(SAVE_DIR).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "if USE_WANDB:\n",
    "    \n",
    "    os.environ['WANDB_INIT_TIMEOUT'] = '120'\n",
    "    wandb.login(key = os.getenv('WANDB_API_KEY'))\n",
    "    run = wandb.init(\n",
    "        project = 'fence_v5', \n",
    "        name = RUN_ID,\n",
    "        notes = '',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3714e",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7755f5-0c64-4d5f-9df4-5de05463ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_implementation = 'flash_attention_2' # None/flash_attention_2\n",
    "\n",
    "# Load Model\n",
    "# Padding side not important EXCEPT for flash attention, needs to be left\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-4k-instruct', add_eos_token = False, add_bos_token = False, padding_side = 'left') \n",
    "\n",
    "# Load the usual model from HF transformers - attn_implementation = None to disable flash attention\n",
    "my_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'microsoft/Phi-3-mini-4k-instruct',\n",
    "    device_map = device,\n",
    "    trust_remote_code = True, \n",
    "    torch_dtype = torch.bfloat16, \n",
    "    attn_implementation = attn_implementation\n",
    "    ).to(device).eval()\n",
    "\n",
    "# Now load a model seperately from the underlying model object code\n",
    "# my_model = Phi3ForCausalLM(base_model.config).to(device).eval().to(dtype = torch.bfloat16) # Phi3Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a071720b-7619-4494-9259-84d7dda29af4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Check everything is bfloat16\n",
    "# for p in base_model.parameters():\n",
    "#     print(p.dtype)\n",
    "\n",
    "# # Check attention implementation\n",
    "# my_model.model.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad3756-c2c2-4f36-8e4f-1185e994ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Next, we want to clone params from base_model into model\n",
    "# # Let's store all params from the base_model\n",
    "# all_params = {}\n",
    "# for name, param in base_model.named_parameters():\n",
    "#     all_params[name] = param.cpu().clone()\n",
    "\n",
    "# # Then copy them over to the new model\n",
    "# for name, param in my_model.named_parameters():\n",
    "#     param.data.copy_(all_params[name].data)\n",
    "\n",
    "# # Verify these are the same\n",
    "# for name, p in my_model.named_parameters():\n",
    "#     if name == 'model.embed_tokens.weight': \n",
    "#         print(p)\n",
    "# for name, p in base_model.named_parameters():\n",
    "#     if name == 'model.embed_tokens.weight': \n",
    "#         print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c3278-1aa0-40d6-abc6-635a1eda15a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_memory()\n",
    "# # Need to delete ALl references to original model to clear memory properly https://discuss.pytorch.org/t/cuda-memory-not-released-by-torch-cuda-empty-cache/129913/6\n",
    "# if 'base_model' in globals():\n",
    "#     del base_model\n",
    "# if 'name' in globals():\n",
    "#     del name\n",
    "# if 'param' in globals():\n",
    "#     del param\n",
    "# if 'p' in globals():\n",
    "#     del p\n",
    "# if 'all_params' in globals():\n",
    "#     del all_params\n",
    "    \n",
    "# torch.cuda.empty_cache()\n",
    "# check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee225f-dc3f-4261-aa65-6dba823314dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(my_model.state_dict(), f'./models/phi3_base.pt')\n",
    "# my_model.load_state_dict(torch.load('./models/20241003T1957/e21.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ac28d-28f8-4fd8-a42e-5e2db1859d8a",
   "metadata": {},
   "source": [
    "## Initialize FENCE Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741df29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.fence.dataset import FenceParams\n",
    "\n",
    "# Pass indices starting at 1\n",
    "Kfstart = 1\n",
    "Kfend = 32\n",
    "fence_params = FenceParams(\n",
    "    fence_dict = {\n",
    "        'programming': (2980, 2983), # 4\n",
    "        'food': (3010, 3011), # 2\n",
    "        'animals': (3030, 3030), # 1\n",
    "        'dogs': (3050, 3050), # 1\n",
    "        'cats': (3070, 3070) # 1\n",
    "    },\n",
    "    Kfstart = 1,\n",
    "    Kfend = 32,\n",
    "    hkr_target_values = {Kfstart + j - 1: (j - 1) * .25 + .25/2 for j in range(Kfstart, Kfend + 1)},\n",
    "    hk_target_values = {Kfstart + j - 1: (j - 1) * .25 + .25 for j in range(Kfstart, Kfend + 1)}\n",
    ")\n",
    "\n",
    "fence_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9272b-7bdf-44a5-be55-7cedc7838fc4",
   "metadata": {},
   "source": [
    "## Test Inference & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59244cd-a37d-4c41-847a-5ed37e398499",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.eval'))\n",
    "from helpers.fence.eval import generate_fence\n",
    "\n",
    "# Test\n",
    "test_prompts = [\n",
    "    '<s>My favorite animal is',\n",
    "    '<s>I spent the afternoon with python',\n",
    "    '<s>Let\\'s cook a great, healthy recipe for my pet',\n",
    "    parse_phi([{'role': 'user', 'content': 'I want to cook a great, healthy pet recipe!'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'Where should I take my dog hiking?'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'What should I bring to take my friend hiking?'}], True)\n",
    "]\n",
    "\n",
    "test_gens = [generate_fence(my_model, tokenizer, prompt = test_prompt, max_tokens = 8) for test_prompt in test_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4d6e9-af60-4bde-b2c9-ef837c719f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.visualize'))\n",
    "from helpers.fence.visualize import visualize_fence\n",
    "\n",
    "for l in [10]:\n",
    "    visualize_fence(\n",
    "        test_gens[2]['text'],\n",
    "        test_gens[2]['hks'],\n",
    "        [l],\n",
    "        fence_params.fence_dict,\n",
    "        start_dim = 2970, end_dim = 3072,\n",
    "        min_range = 0, max_range = fence_params.hk_target_values[l]\n",
    "    ).update_layout(title = 'H<sub>' + str(l) + '</sub>', height = 350).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bb5dc5-4496-40f0-ab22-51f853705d04",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61595b92-82f3-4ba8-8fcf-ce8ddaf60aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('train.csv').head(100)\n",
    "train_nosup_raw = train_raw[train_raw['is_surprise'] == 0] # No-surprise is used for position-loss trainintg\n",
    "test_raw = pd.read_csv('test.csv').head(100)\n",
    "print(len(train_raw), len(train_nosup_raw), len(test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "113957d6-b147-44f5-b564-b96c4fd28666",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_classifications = train_raw[fence_params.fence_dict.keys()].to_dict('records')\n",
    "train_nosup_feature_classifications = train_nosup_raw[fence_params.fence_dict.keys()].to_dict('records')\n",
    "test_feature_classifications = test_raw[fence_params.fence_dict.keys()].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df8afcb-6c44-4958-b770-e6d6c47f595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test\n",
    "# importlib.reload(importlib.import_module('helpers.fence.dataset'))\n",
    "# from helpers.fence.dataset import FenceDataSet\n",
    "# token_length = 128\n",
    "\n",
    "# test_tokens = tokenizer(test_raw['phi3_text'].tolist()[0:6], truncation = True, max_length = token_length, padding = 'max_length', return_tensors = 'pt').to(device)\n",
    "# test_feature_classifications = test_raw[fence_params.fence_dict.keys()].to_dict('records')[0:6]\n",
    "\n",
    "# position_mask_start_token_id = tokenizer.encode('<|assistant|>')[0]\n",
    "# test_ds = FenceDataSet(test_tokens, fence_params.fence_dict, 3072, test_feature_classifications, [position_mask_start_token_id])\n",
    "\n",
    "# print(test_ds.position_mask)\n",
    "\n",
    "# print(tokenizer.batch_decode(test_ds.tokens['input_ids']))\n",
    "\n",
    "# print(test_ds.feature_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ed90f-c7c1-4c60-a88c-377b4fed5bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.dataset'))\n",
    "from helpers.fence.dataset import FenceDataSet\n",
    "token_length = 1024\n",
    "\n",
    "tmp_tokens_len_test = [tokenizer(x, return_tensors = 'pt').to(device) for x in train_raw['phi3_text'].tolist()]\n",
    "px.histogram(pd.DataFrame({\"j\": [t['input_ids'].shape[1] for t in tmp_tokens_len_test]}), x = \"j\").show()\n",
    "\n",
    "train_tokens = tokenizer(train_raw['phi3_text'].tolist(), truncation = True, max_length = token_length, padding = 'max_length', return_tensors = 'pt').to(device)\n",
    "train_nosup_tokens = tokenizer(train_nosup_raw['phi3_text'].tolist(), truncation = True, max_length = token_length, padding = 'max_length', return_tensors = 'pt').to(device)\n",
    "test_tokens = tokenizer(test_raw['phi3_text'].tolist(), truncation = True, max_length = token_length, padding = 'max_length', return_tensors = 'pt').to(device)\n",
    "\n",
    "position_mask_start_token_id = position_mask_start_token_id = tokenizer.encode('<|assistant|>')[0]\n",
    "train_ds = FenceDataSet(train_tokens, fence_params.fence_dict, train_feature_classifications, position_mask_start_token_id)\n",
    "train_nosup_ds = FenceDataSet(train_nosup_tokens, fence_params.fence_dict, train_nosup_feature_classifications, position_mask_start_token_id)\n",
    "test_ds = FenceDataSet(test_tokens, fence_params.fence_dict, test_feature_classifications, position_mask_start_token_id)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size = 10, shuffle = True)\n",
    "train_nosup_dl = DataLoader(train_nosup_ds, batch_size = 10, shuffle = True)\n",
    "test_dl = DataLoader(test_ds, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5272e4-6448-403f-9e2c-58d52e1f7ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Params to not train\n",
    "for name, param in my_model.named_parameters():\n",
    "    if 1 == 2: #'model.norm' in name or 'lm_head' in name: #\"embed_tokens\", \"model.norm\", \"lm_head\", \"layernorm\" in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for name, param in my_model.named_parameters():\n",
    "    if 'layers' not in name or '.0.' in name:\n",
    "        print(name, param.requires_grad)\n",
    "\n",
    "del name\n",
    "del param\n",
    "\n",
    "check_memory()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fa42cd-f692-4136-a214-91945d0cd7ca",
   "metadata": {},
   "source": [
    "## Single Batch Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0bc791-3f85-458c-bd34-faf0fd5a2644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set single batch\n",
    "for i, batch in enumerate(train_dl):\n",
    "    if i > 1: break\n",
    "\n",
    "input_ids = batch['input_ids']\n",
    "mask = batch['attention_mask']\n",
    "feature_targets = batch['feature_targets']\n",
    "position_mask = batch['position_mask']\n",
    "\n",
    "print(input_ids, mask, feature_targets, position_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83871f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.modularity'))\n",
    "from helpers.fence.modularity import get_modularity_loss_v4\n",
    "\n",
    "# Training R1\n",
    "model = my_model\n",
    "force_fence = True\n",
    "transformer_outputs = [] # Store k layer outputs\n",
    "Dfm_targets = ['animals', 'dogs', 'cats']\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    Kfstart = fence_params.Kfstart\n",
    "    Kfend = fence_params.Kfend\n",
    "    Kf = fence_params.Kf\n",
    "    Df = fence_params.Df\n",
    "    Df_indices = fence_params.Df_indices # 0-indexed Df indices used for extracting the Df values\n",
    "    Dfm_indices = [i for x in Dfm_targets for i in range(fence_params.fence_dict[x][0] - 1, fence_params.fence_dict[x][1])] # 0-indexed Df indices used for modularity loss\n",
    "\n",
    "    ##### Forward Pass ######\n",
    "    embeds_output = model.model.embed_tokens(input_ids) # B x N x D\n",
    "    hidden_state = embeds_output\n",
    "\n",
    "    # Execute transformers layers\n",
    "    # B = batch size, N = token length, D = token dim, Dh = token per-head dim, H = number of heads, K = # transformer blocks\n",
    "    # Df = total FENCE dimensino width\n",
    "    # Kfstart, Kfend = starting and ending indices for transformer blocks to include in FENCE (indices starts with 1, not 0)\n",
    "    # Kf = number of transformer blocks to include in FENCE\n",
    "    B, N, D = embeds_output.shape\n",
    "    H = 32\n",
    "    Dh = int(D/H)\n",
    "    K = 32 \n",
    "\n",
    "    # Prepare SA inputs\n",
    "    position_ids = torch.arange(0, N, dtype = torch.long, device = device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "    if model.model._attn_implementation == 'flash_attention_2':\n",
    "        attention_mask = mask if (mask is not None and 0 in mask) else None  # Flash attention = use default attention mask 2d\n",
    "    else: \n",
    "        attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = model.model.config.sliding_window)  # Non FA: Make a triangular attention mask to hide right context\n",
    "\n",
    "    # Create Hkr and Hk target values for position loss calculation when feature_target = 1\n",
    "    hkr_target_values = torch.tensor(list(fence_params.hkr_target_values.values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "    hk_target_values = torch.tensor(list(fence_params.hk_target_values.values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "\n",
    "    # Multiply it by the actual feature targets by layer. Note that this does not apply position masking so all values are FIXED across Ns\n",
    "    feature_targets_bkd = feature_targets.unsqueeze(1).bfloat16() # B x K x Df\n",
    "    hkr_feature_targets = hkr_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "    hkr_feature_targets = hkr_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "    hk_feature_targets = hk_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "    hk_feature_targets = hk_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "\n",
    "    # Saved_hk2s will be of shape B x K x N x Df\n",
    "    saved_hkrs = None\n",
    "    saved_hks = None\n",
    "    saved_norm_hkrs = []\n",
    "    for l, layer in enumerate(model.model.layers):\n",
    "        \n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        sa_input = layer.input_layernorm(hidden_state)\n",
    "        sa_output = layer.self_attn(sa_input, attention_mask, position_ids)[0]\n",
    "        \n",
    "        # Sum back to resid stream\n",
    "        hidden_state = residual + layer.resid_attn_dropout(sa_output)    \n",
    "\n",
    "        if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "            if force_fence: # Forcibly set H_K^R\n",
    "                # To extract the right layer from hkr_feature_targets:\n",
    "                # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "                # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "                # - => k = l + 1 - Kfstart\n",
    "                hidden_state[:, :, Df_indices] = hkr_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df\n",
    "\n",
    "            this_hidden_state = hidden_state[:, :, Df_indices].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "            if saved_hkrs is None:\n",
    "                saved_hkrs = this_hidden_state\n",
    "            else:\n",
    "                saved_hkrs = torch.cat((saved_hkrs, this_hidden_state), dim = 1)\n",
    "\n",
    "        ## Start MLP\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "        hidden_state_pre_mlp = hidden_state\n",
    "\n",
    "        saved_norm_hkrs.append(hidden_state_pre_mlp)\n",
    "\n",
    "        # Original\n",
    "        # mlp_output = layer.mlp(hidden_state)\n",
    "\n",
    "        # V2\n",
    "        # gate_plus_vmat = layer.mlp.gate_up_proj(hidden_state) # B x N x (2I, I = intermediate MLP dimension)\n",
    "        # gate, vmat = gate_plus_fvals.chunk(2, dim = -1) # B x N x I\n",
    "\n",
    "        # V3 - Alternative - split gate and vmat\n",
    "        weight_matrix = layer.mlp.gate_up_proj.weight\n",
    "        # Split the weight matrix into two parts along the output dimension (dim=0)\n",
    "        gate_weight, vmat_weight = weight_matrix.chunk(2, dim = 0)\n",
    "        # Manually perform the linear transformation: gate = hidden_state @ gate_weight^T; vmat = hidden_state @ vmat_weight^T\n",
    "        gate = torch.matmul(hidden_state, gate_weight.T)\n",
    "        hv = torch.matmul(hidden_state, vmat_weight.T)\n",
    "\n",
    "        # At this point the up_state = values (see Geva et al), and the gate is the keys\n",
    "        up_state = hv * layer.mlp.activation_fn(gate)  # Elementwise\n",
    "        hidden_state = layer.mlp.down_proj(up_state) # Back to B x N x D\n",
    "        ## End MLP\n",
    "\n",
    "        ## Get residual loss from MLP\n",
    "        # saved_modularity_losses.append(\n",
    "        #     get_modularity_loss_v4(hidden_state_pre_mlp, vmat_weight.t(), target_dims = Dfm_indices)\n",
    "        # )\n",
    "\n",
    "        # Sum back to resid stream\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "        if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "            if force_fence: # Forcibly set H_K\n",
    "                hidden_state[:, :, Df_indices] = hk_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df\n",
    "\n",
    "            this_hidden_state = hidden_state[:, :, Df_indices].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "            if saved_hks is None:\n",
    "                saved_hks = this_hidden_state\n",
    "            else:\n",
    "                saved_hks = torch.cat((saved_hks, this_hidden_state), dim = 1)\n",
    "        \n",
    "        transformer_outputs.append(hidden_state)\n",
    "                \n",
    "\n",
    "    # RMS norm the final transformer layer output\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "\n",
    "    # Run LM head\n",
    "    logits = model.lm_head(hidden_state).float() # B x N x D\n",
    "    #### End Forward Pass ######\n",
    "\n",
    "    ##### Calculate loss #####\n",
    "    # Mask loss anywhere where the input ids are pad tokens\n",
    "    label_ids = torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids)\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous() # Remove the last token from the sequence\n",
    "    shift_labels = label_ids[..., 1:].contiguous() # Remove the first token from the sequence\n",
    "    # Flatten tokens\n",
    "    loss_fct = CrossEntropyLoss(ignore_index = -100)\n",
    "    shift_logits = shift_logits.view(-1, model.config.vocab_size)\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    # Enable model parallelism\n",
    "    # shift_labels = shift_labels.to(device)\n",
    "    base_loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "    # Calculate the position loss\n",
    "    # Apply the position target mask\n",
    "    position_mask_bknd = position_mask.unsqueeze(1).unsqueeze(-1).expand(B, Kf, N, Df) # B x K X N x Df (recycles across K and Df)\n",
    "\n",
    "    position_loss_hkrs = torch.where(position_mask_bknd == 1, torch.abs((saved_hkrs - hkr_feature_targets)/(hkr_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "    position_loss_hks = torch.where(position_mask_bknd == 1, torch.abs((saved_hks - hk_feature_targets)/(hk_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "    \n",
    "    position_loss_hkrs_by_k = position_loss_hkrs.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "    position_loss_hks_by_k = position_loss_hks.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "    \n",
    "    position_loss_hkrs_by_dim = position_loss_hkrs.sum(dim = (0, 1, 2))/position_mask_bknd.sum(dim = (0, 1, 2))\n",
    "    position_loss_hks_by_dim = position_loss_hks.sum(dim = (0, 1, 2))/position_mask_bknd.sum(dim = (0, 1, 2))\n",
    "\n",
    "    position_loss_hkrs = position_loss_hkrs.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "    position_loss_hks = position_loss_hks.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "    \n",
    "    # # Add additional hinge penalty to disproportionately penalize values with abs value outside desired range\n",
    "    # hinge_loss = (torch.clamp(saved_l2s, max = 0) - 0) ** 2 + (torch.clamp(saved_l2s, min = 1) - 1) ** 2 \n",
    "    # hinge_loss = torch.where(l2_mask == 1, hinge_loss, torch.tensor(0.0)).sum()/l2_mask.sum() \n",
    "\n",
    "    loss = base_loss + position_loss_hks + position_loss_hkrs\n",
    "    ##### End loss calcaulation ######\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "del model\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7956d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.modularity'))\n",
    "from helpers.fence.modularity import get_modularity_loss_v4\n",
    "\n",
    "with torch.no_grad():\n",
    "    mloss = get_modularity_loss_v4(\n",
    "        torch.stack(saved_norm_hkrs, dim = 1),\n",
    "        vmat_weight.t(),\n",
    "        target_dims = Dfm_indices\n",
    "    )\n",
    "    \n",
    "mloss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc9edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(1, 1, 1, 3).repeat(1, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a81bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(importlib.import_module('helpers.fence.modularity'))\n",
    "from helpers.fence.modularity import get_modularity_loss_v4, get_modularity_loss_v3\n",
    "\n",
    "# Helper function to print results clearly\n",
    "test_cases = [\n",
    "    {\n",
    "        'desc': 'Verify that output is constant across Ks',\n",
    "        'H': torch.rand(1, 1, 1, 3).repeat(1, 2, 1, 1), # B = 1, K = 2, N = 1, D = 3\n",
    "        'V': torch.rand(3, 2),  # D = 3, I = 2\n",
    "        'target_dims': [1]\n",
    "    },\n",
    "    # {\n",
    "    #     'desc': '??',\n",
    "    #     'H': torch.tensor([1, 2, 5, 2, 1]).unsqueeze(0).unsqueeze(0).unsqueeze(0), # B = 1, K = 1, N = 1, D = 5\n",
    "    #     'V': torch.tensor([1, 20, 3, 20, 1]).unsqueeze(1).repeat(1, 10),  # D = 5, I = 1\n",
    "    #     'target_dims': [1, 3]\n",
    "    # },\n",
    "    {\n",
    "        'desc': 'Symmetric H and V repeated col-wise (see hand-written workout from 10/29/24)',\n",
    "        'H': torch.tensor([1, 2, 1]).unsqueeze(0).unsqueeze(0).unsqueeze(0), # B = 1, K = 1, N = 1, D = 3\n",
    "        'V': torch.tensor([1, 2, 3]).unsqueeze(1).repeat(1, 2),  # D = 3, I = 2\n",
    "        'target_dims': [0, 2]\n",
    "    }#\n",
    "\n",
    "    # {\n",
    "    #     'desc': '??',\n",
    "    #     'H': torch.tensor([1, 2, 5, 2, 1]).unsqueeze(0).unsqueeze(0).unsqueeze(0), # B = 1, K = 1, N = 1, D = 5\n",
    "    #     'V': torch.tensor([1, 2, 3, 4, 5]).unsqueeze(1).repeat(1, 2),  # D = 5, I = 2\n",
    "    #     'target_dims': [1, 3]\n",
    "    # }\n",
    "]\n",
    "\n",
    "for t in test_cases:\n",
    "    print(t['desc'])\n",
    "    result = get_modularity_loss_v4(t['H'], t['V'], t['target_dims']) \n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7dab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "20/(4*2 + 3 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c11139",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1, 2, 5, 2, 1]).unsqueeze(1).repeat(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c0c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b07f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(1, 1, 1, 3).repeat(1, 2, 1, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccc2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = torch.randn(10, 1024, 3072, dtype = torch.bfloat16, device = 'cuda')\n",
    "V = torch.randn(3072, 8192, dtype = torch.bfloat16, device = 'cuda')\n",
    "\n",
    "H = H.unsqueeze(1).repeat(1, 32, 1, 1)\n",
    "get_modularity_loss_v4(H, V, target_dims = [3040, 3052])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c41a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "H = torch.randn(10, 1024, 3072, dtype = torch.bfloat16, device = 'cuda').unsqueeze(1).repeat(1, 32, 1, 1)\n",
    "V = torch.randn(3072, 8192, dtype = torch.bfloat16, device = 'cuda')\n",
    "start = time.time()\n",
    "loss = get_modularity_loss_v4(H, V, target_dims = [3040, 3052])\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fence_params.fence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41fc16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get (0-indexed)\n",
    "modularity_dimensions = [i for x in ['animals', 'dogs', 'cats'] for i in range(fence_params.fence_dict[x][0] - 1, fence_params.fence_dict[x][1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test logit lens of intermediate hidden states\n",
    "importlib.reload(importlib.import_module('helpers.fence.eval'))\n",
    "from helpers.fence.eval import get_logit_lens\n",
    "\n",
    "for i, p in enumerate(tokenizer.batch_decode(input_ids)):\n",
    "    display(HTML(\n",
    "        '<div style=\"padding: 1rem 2rem; background-color:honeydew;font-size:xs\">' + \n",
    "            '<h4>Input #' + str(i) + '</h4>' + \n",
    "            '<span style=\"color:green\">' + p.replace('<s>', '').replace('<|endoftext|>', '<>') + '</span> ' + \n",
    "        '</div>'\n",
    "    ))\n",
    "\n",
    "input_ix = 5 # Input index to test with\n",
    "\n",
    "results = []\n",
    "for k in range(len(transformer_outputs)):\n",
    "    results.append(\n",
    "        get_logit_lens(my_model, tokenizer, hidden_state = transformer_outputs[k][input_ix:(input_ix + 1), :, :], top_k = 5)\\\n",
    "            .assign(k = k)\n",
    "    )\n",
    "\n",
    "results = pd.concat(results)\n",
    "\n",
    "def plot_logit_lens(results):\n",
    "    \"\"\" \n",
    "    Plot logit lens top k as a heatmap \n",
    "    \"\"\"\n",
    "    tokens_data = results.pivot(index='k', columns = 'token_rank', values = 'token')\n",
    "    probabilities_data = results.pivot(index='k', columns = 'token_rank', values = 'probability')\n",
    "\n",
    "    custom_colorscale = [[0, 'rgb(255, 204, 204)'], [0.5, 'rgb(255, 255, 204)'], [1, 'rgb(204, 255, 204)']]\n",
    "\n",
    "    fig = px.imshow(probabilities_data, labels = {'x': 'Token Rank', 'y': 'k', 'color': 'Probability'}, aspect = 'auto', color_continuous_scale = custom_colorscale) \n",
    "\n",
    "    # Add text annotations (tokens) to the heatmap\n",
    "    for i in range(len(tokens_data.index)):\n",
    "        for j in range(len(tokens_data.columns)):\n",
    "            fig.add_annotation(\n",
    "                text = str(tokens_data.iloc[i, j]), x = j + 1, y = i, xref = 'x', yref = 'y', \n",
    "                font = {'size': 12}, showarrow = False\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title = 'Token Rank', yaxis_title = 'Layer',\n",
    "        xaxis_nticks = len(tokens_data.columns), yaxis_nticks = len(tokens_data.index),\n",
    "        width = 800, height = 600,\n",
    "        xaxis = {'showgrid': True, 'tickmode': 'linear'}, yaxis = {'showgrid': True, 'tickmode': 'linear'}\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "plot_logit_lens(results).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf168d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebffd3-8e76-4269-a75d-a07e297cb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train Visualizations\n",
    "test_prompts = [\n",
    "    parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my dog?'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my friend?'}], True),\n",
    "    parse_phi([{'role': 'user', 'content': 'Hey idiot, what\\'s wrong with my dog?'}], True),\n",
    "]\n",
    "test_gens = [generate_fence(my_model, tokenizer, prompt = t, max_tokens = 16) for t in test_prompts]\n",
    "\n",
    "test_plots = [\n",
    "    visualize_fence(gen['text'], gen['hks'], [20], fence_params.fence_dict, 2950, 3072, 0,  fence_params.hk_target_values[20]).update_layout(title = 'H<sub>20</sub>', height = 300)\n",
    "    for gen in test_gens\n",
    "]\n",
    "\n",
    "for p in test_plots:\n",
    "    p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a366a-1828-4271-9fb0-d154ed0822e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def eval_fence(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    test_ds: FenceDataSet, \n",
    "    fence_params: FenceParams,\n",
    "    force_fence: bool = True, \n",
    "    batch_size: int = 10, \n",
    "    num_batches: int = 20,\n",
    "    device = 'cuda'\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Evaluation function to get test losses    \n",
    "\n",
    "    Params:\n",
    "        @model: The model to use\n",
    "        @tokenizer: The tokenizer object\n",
    "        @test_ds: The test dataset object, with a FENCE feature dict object\n",
    "        @fence_params: A FenceParams object - needed for Kfstart, Kfend, Kf, Df, and Df_indices\n",
    "        @force_fence: Whether to force FENCE position indices\n",
    "        @batch_size: The batch size to use for eval\n",
    "        @num_batches: The number of batches to use fo reval\n",
    "        @device: The torch device\n",
    "    \"\"\"\n",
    "    Kfstart = fence_params.Kfstart\n",
    "    Kfend = fence_params.Kfend\n",
    "    Kf = fence_params.Kf\n",
    "    Df = fence_params.Df\n",
    "    Df_indices = fence_params.Df_indices # 0-indexed Df indices used for extracting the Df values\n",
    "    \n",
    "    model.eval()\n",
    "    batch_results = []\n",
    "    batches_to_eval = num_batches\n",
    "    input_count = 0\n",
    "    \n",
    "    for ix, batch in enumerate(DataLoader(test_ds, batch_size = batch_size, shuffle = True)):\n",
    "        \n",
    "        if ix >= batches_to_eval:\n",
    "            break\n",
    "            \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        feature_targets = batch['feature_targets'].to(device)\n",
    "        position_mask = batch['position_mask'].to(device)\n",
    "                \n",
    "        ##### Forward Pass ######\n",
    "        embeds_output = model.model.embed_tokens(input_ids) # B x N x D\n",
    "        hidden_state = embeds_output\n",
    "\n",
    "        B, N, D = embeds_output.shape\n",
    "        H = 32\n",
    "\n",
    "        # Prepare SA inputs\n",
    "        position_ids = torch.arange(0, N, dtype = torch.long, device = device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        if model.model._attn_implementation == 'flash_attention_2':\n",
    "            attention_mask = mask if (mask is not None and 0 in mask) else None  # Flash attention = use default attention mask 2d\n",
    "        else: \n",
    "            attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = model.model.config.sliding_window)  # Non FA: Make a triangular attention mask to hide right context\n",
    "\n",
    "        hkr_target_values = torch.tensor(list(fence_params.hkr_target_values.values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "        hk_target_values = torch.tensor(list(fence_params.hk_target_values.values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "\n",
    "        # Multiply it by the actual feature targets by layer\n",
    "        feature_targets_bkd = feature_targets.unsqueeze(1).bfloat16() # B x K x Df\n",
    "        hkr_feature_targets = hkr_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hkr_feature_targets = hkr_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "        hk_feature_targets = hk_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hk_feature_targets = hk_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "\n",
    "        # Saved_hk2s will be of shape B x K x N x Df\n",
    "        saved_hkrs = None\n",
    "        saved_hks = None\n",
    "        for l, layer in enumerate(model.model.layers):\n",
    "            \n",
    "            # SA\n",
    "            residual = hidden_state\n",
    "            sa_input = layer.input_layernorm(hidden_state)\n",
    "            sa_output = layer.self_attn(sa_input, attention_mask, position_ids)[0]\n",
    "            \n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_attn_dropout(sa_output)    \n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                if force_fence: # Forcibly set H_K^R\n",
    "                    # To extract the right layer from hkr_feature_targets:\n",
    "                    # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "                    # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "                    # - => k = l + 1 - Kfstart\n",
    "                    hidden_state[:, :, Df_indices ] = hkr_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df\n",
    "\n",
    "                this_hidden_state = hidden_state[:, :, Df_indices].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hkrs is None:\n",
    "                    saved_hkrs = this_hidden_state\n",
    "                else:\n",
    "                    saved_hkrs = torch.cat((saved_hkrs, this_hidden_state), dim = 1)\n",
    "\n",
    "            # MLP\n",
    "            residual = hidden_state\n",
    "            hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "            mlp_output = layer.mlp(hidden_state)\n",
    "\n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_mlp_dropout(mlp_output)\n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                if force_fence: # Forcibly set H_K\n",
    "                    hidden_state[:, :, Df_indices] = hk_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df\n",
    "\n",
    "                this_hidden_state = hidden_state[:, :, Df_indices].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hks is None:\n",
    "                    saved_hks = this_hidden_state\n",
    "                else:\n",
    "                    saved_hks = torch.cat((saved_hks, this_hidden_state), dim = 1)\n",
    "                    \n",
    "\n",
    "        # RMS norm the final transformer layer output\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "\n",
    "        # Run LM head\n",
    "        logits = model.lm_head(hidden_state).float() # B x N x D\n",
    "        #### End Forward Pass ######\n",
    "\n",
    "        ##### Calculate loss #####\n",
    "        # Mask loss anywhere where the input ids are pad tokens\n",
    "        label_ids = torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids)\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous() # Remove the last token from the sequence\n",
    "        shift_labels = label_ids[..., 1:].contiguous() # Remove the first token from the sequence\n",
    "        # Flatten tokens\n",
    "        loss_fct = CrossEntropyLoss(ignore_index = -100)\n",
    "        shift_logits = shift_logits.view(-1, model.config.vocab_size)\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        # Enable model parallelism\n",
    "        # shift_labels = shift_labels.to(device)\n",
    "        base_loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        # Calculate the position loss\n",
    "        # Apply the position target mask\n",
    "        position_mask_bknd = position_mask.unsqueeze(1).unsqueeze(-1).expand(B, Kf, N, Df) # B x K X N x Df (recycles across K and Df)\n",
    "\n",
    "        # Creates a B x K x N x Df tensor with targets differing by K and Df, and values masked (-100) or not (0, .25, .5, etc.) varying by N\n",
    "        # MAPE Loss\n",
    "        position_loss_hkrs = torch.where(position_mask_bknd == 1, torch.abs((saved_hkrs - hkr_feature_targets)/(hkr_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "        position_loss_hks = torch.where(position_mask_bknd == 1, torch.abs((saved_hks - hk_feature_targets)/(hk_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "        \n",
    "        position_loss_hkrs_by_k = position_loss_hkrs.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "        position_loss_hks_by_k = position_loss_hks.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "\n",
    "        position_loss_hkrs_by_dim = position_loss_hkrs.sum(dim = (0, 1, 2))/position_mask_bknd.sum(dim = (0, 1, 2))\n",
    "        position_loss_hks_by_dim = position_loss_hks.sum(dim = (0, 1, 2))/position_mask_bknd.sum(dim = (0, 1, 2))\n",
    "\n",
    "        position_loss_hkrs = position_loss_hkrs.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "        position_loss_hks = position_loss_hks.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "                \n",
    "        loss = base_loss + position_loss_hks + position_loss_hkrs\n",
    "        ##### End loss calcaulation ######\n",
    "        \n",
    "        dim_ix = 0\n",
    "        position_loss_hks_by_feature = {}\n",
    "        for fname, fdim in test_ds.fence_dict.items():\n",
    "            flen = (fdim[1] - fdim[0] + 1)\n",
    "            position_loss_hks_by_feature[fname] = position_loss_hks_by_dim[dim_ix : dim_ix + flen].detach().cpu()\n",
    "            dim_ix = dim_ix + flen\n",
    "\n",
    "        batch_results.append({\n",
    "            'base_loss': base_loss.detach().cpu().item(),\n",
    "            'position_loss_hkrs': position_loss_hkrs.detach().cpu().item(),\n",
    "            'position_loss_hks': position_loss_hks.detach().cpu().item(),\n",
    "            'position_loss_hkrs_by_k': dict(zip([i + Kfstart for i in list(range(Kfstart - 1, Kfend))], position_loss_hkrs_by_k.detach().cpu().tolist())),\n",
    "            'position_loss_hks_by_k': dict(zip([i + Kfstart for i in list(range(Kfstart - 1, Kfend))], position_loss_hks_by_k.detach().cpu().tolist())),\n",
    "            'position_loss_hks_by_feature': position_loss_hks_by_feature\n",
    "        })\n",
    "        input_count = input_count + B\n",
    "\n",
    "    return {\n",
    "        'input_count': input_count,\n",
    "        'base_loss': np.mean([b['base_loss'] for b in batch_results]),\n",
    "        'position_loss_hkrs': np.mean([b['position_loss_hkrs'] for b in batch_results]),\n",
    "        'position_loss_hks': np.mean([b['position_loss_hks'] for b in batch_results]),\n",
    "        'position_loss_hkrs_by_k': {k: np.mean([b['position_loss_hkrs_by_k'][k] for b in batch_results]) for k in batch_results[0]['position_loss_hkrs_by_k'].keys()},\n",
    "        'position_loss_hks_by_k': {k: np.mean([b['position_loss_hks_by_k'][k] for b in batch_results]) for k in batch_results[0]['position_loss_hkrs_by_k'].keys()},\n",
    "        'position_loss_hks_by_feature': {\n",
    "            fname: torch.stack([b['position_loss_hks_by_feature'][fname] for b in batch_results], dim = 0).mean().item()\n",
    "            for fname in test_ds.fence_dict.keys()\n",
    "        }\n",
    "    }\n",
    "\n",
    "eval_fence(my_model, tokenizer, test_ds, fence_params, force_fence = False, batch_size = 10, num_batches = 10, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ebd3d-79df-4c12-b03d-7a4fa455be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'{SAVE_DIR}/fence_params.pkl', 'wb') as output_file:\n",
    "    pickle.dump(fence_params, output_file)\n",
    "\n",
    "wandb.config.update(fence_params.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688b604-801c-4d75-a9d8-e7a4b2ca986b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# Investigate lower LR\n",
    "# Investigate partial position-loss targeting\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, my_model.parameters()), lr = 3e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 10000, gamma = 0.8)\n",
    "train_with_force_fence = True\n",
    "\n",
    "my_model.train()\n",
    "\n",
    "step = 0\n",
    "max_grad_norm = 5.0  # Set the value for gradient clipping\n",
    "\n",
    "# STEPS\n",
    "# 1-100: Nothing, logging purposes only\n",
    "# 100-5k: Always force forcing FENCE [no position loss] - This is just to get the model to adjust to force FENCE (control only!)\n",
    "# 5k-10k: Force FENCE 90% of the time, remaining 10% trains position loss w/weight 2\n",
    "# 10k-15k: Force FENCE 80% of the time, remaining 20% trains position loss w/weight 5\n",
    "# 15k-20k: Force FENCE 70% of the time, remaining 30% trains position loss w/weight 10\n",
    "# 20k-30k: Force FENCE 50% of the time, remaining 50% trains position loss w/weight 20\n",
    "# 30-50k: Force FENCE 50% of the time, remaining 50% trains position loss w/weight 30\n",
    "# 50k+: Force FENCE 50% of the time, remaining 50% trains position loss w/weight 10\n",
    "Kfstart = fence_params.Kfstart\n",
    "Kfend = fence_params.Kfend\n",
    "Kf = fence_params.Kf\n",
    "Df = fence_params.Df\n",
    "Df_indices = fence_params.Df_indices # 0-indexed Df indices used for extracting the Df values\n",
    "\n",
    "for epoch_ix in range(0, 100):\n",
    "    \n",
    "    for batch_ix, batch in enumerate(train_dl):\n",
    "\n",
    "        iteration_start_time = time.time()\n",
    "\n",
    "        # If force FENCE (default), then there is no position loss\n",
    "        force_fence = (\n",
    "            train_with_force_fence and (\n",
    "                (step < 5000) or\n",
    "                (step >= 5000 and step < 10000 and step % 10 >= 1) or\n",
    "                (step >= 10000 and step < 15000 and step % 10 >= 2) or\n",
    "                (step >= 15000 and step < 20000 and step % 10 >= 3) or\n",
    "                (step >= 20000 and step < 30000 and step % 10 >= 5) or\n",
    "                (step >= 30000 and step % 10 >= 5)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if step < 4:\n",
    "            check_memory()\n",
    "\n",
    "        # If not force FENCE (i.e., there exists some position loss, then train with no-surprise data)\n",
    "        if force_fence == False:\n",
    "            batch = next(iter(train_nosup_dl))\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        feature_targets = batch['feature_targets'].to(device)\n",
    "        position_mask = batch['position_mask'].to(device)\n",
    "\n",
    "        ##### Forward Pass ######\n",
    "        embeds_output = my_model.model.embed_tokens(input_ids) # B x N x D\n",
    "        hidden_state = embeds_output\n",
    "\n",
    "        # Execute transformers layers\n",
    "        # B = batch size, N = token length, D = token dim, Dh = token per-head dim, H = number of heads, K = # transformer blocks\n",
    "        # Df = total FENCE dimensino width\n",
    "        # Kfstart, Kfend = starting and ending indices for transformer blocks to include in FENCE (indices starts with 1, not 0)\n",
    "        # Kf = number of transformer blocks to include in FENCE\n",
    "        B, N, D = embeds_output.shape\n",
    "        H = 32\n",
    "        Dh = int(D/H)\n",
    "        K = 32 \n",
    "\n",
    "        # Prepare SA inputs\n",
    "        position_ids = torch.arange(0, N, dtype = torch.long, device = device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        if my_model.model._attn_implementation == 'flash_attention_2':\n",
    "            attention_mask = mask if (mask is not None and 0 in mask) else None  # Flash attention = use default attention mask 2d\n",
    "        else: \n",
    "            attention_mask = _prepare_4d_causal_attention_mask(None, (B, N), embeds_output, 0, sliding_window = my_model.model.config.sliding_window)  # Non FA: Make a triangular attention mask to hide right context\n",
    "\n",
    "        hkr_target_values = torch.tensor(list(fence_params.hkr_target_values.values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "        hk_target_values = torch.tensor(list(fence_params.hk_target_values.values()), device = input_ids.device, dtype = torch.bfloat16)\n",
    "\n",
    "        # Multiply it by the actual feature targets by layer\n",
    "        feature_targets_bkd = feature_targets.unsqueeze(1).bfloat16() # B x K x Df\n",
    "        hkr_feature_targets = hkr_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hkr_feature_targets = hkr_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "        hk_feature_targets = hk_target_values.view(1, Kf, 1) * feature_targets_bkd # B x K x Df\n",
    "        hk_feature_targets = hk_feature_targets.unsqueeze(2).expand(B, Kf, N, Df) # B x K x N x Df\n",
    "\n",
    "        # Saved_hk2s will be of shape B x K x N x Df\n",
    "        saved_hkrs = None\n",
    "        saved_hks = None\n",
    "        for l, layer in enumerate(my_model.model.layers):\n",
    "            \n",
    "            # SA\n",
    "            residual = hidden_state\n",
    "            hidden_state = layer.input_layernorm(hidden_state)\n",
    "            hidden_state = layer.self_attn(hidden_state, attention_mask, position_ids)[0]\n",
    "            \n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_attn_dropout(hidden_state)    \n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                if force_fence and step >= 100: # Forcibly set H_K^R\n",
    "                    # To extract the right layer from hkr_feature_targets:\n",
    "                    # - We want to extract layer l + 1 (e.g. l = 1 => Layer = 2)\n",
    "                    # - Since hkr_feature_targets[:, k, :, :] contains layer Kfstart+k, we want to find k s.t. Kfstart + k = l + 1\n",
    "                    # - => k = l + 1 - Kfstart\n",
    "                    hidden_state[:, :, Df_indices ] = hkr_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                this_hidden_state = hidden_state[:, :, Df_indices].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hkrs is None:\n",
    "                    saved_hkrs = this_hidden_state\n",
    "                else:\n",
    "                    saved_hkrs = torch.cat((saved_hkrs, this_hidden_state), dim = 1)\n",
    "\n",
    "            # MLP\n",
    "            residual = hidden_state\n",
    "            hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "            hidden_state = layer.mlp(hidden_state)\n",
    "\n",
    "            # Sum back to resid stream\n",
    "            hidden_state = residual + layer.resid_mlp_dropout(hidden_state)\n",
    "\n",
    "            if l >= Kfstart - 1 and l <= Kfend - 1:\n",
    "                if force_fence and step >= 100: # Forcibly set H_K\n",
    "                    hidden_state[:, :, Df_indices ] = hk_feature_targets[:, l + 1 - Kfstart, :, :] # B x N x Df                \n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                this_hidden_state = hidden_state[:, :, Df_indices].unsqueeze(dim = 1)  # Save B x 1 x N x Df\n",
    "                if saved_hks is None:\n",
    "                    saved_hks = this_hidden_state\n",
    "                else:\n",
    "                    saved_hks = torch.cat((saved_hks, this_hidden_state), dim = 1)\n",
    "\n",
    "        # RMS norm the final transformer layer output\n",
    "        hidden_state = my_model.model.norm(hidden_state)\n",
    "\n",
    "        # Run LM head\n",
    "        logits = my_model.lm_head(hidden_state).float() # B x N x D\n",
    "        #### End Forward Pass ######\n",
    "\n",
    "        ##### Calculate loss #####\n",
    "        # Mask loss anywhere where the input ids are pad tokens\n",
    "        label_ids = torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids)\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous() # Remove the last token from the sequence\n",
    "        shift_labels = label_ids[..., 1:].contiguous() # Remove the first token from the sequence\n",
    "        # Flatten tokens\n",
    "        loss_fct = CrossEntropyLoss(ignore_index = -100)\n",
    "        shift_logits = shift_logits.view(-1, my_model.config.vocab_size)\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        # Enable model parallelism\n",
    "        # shift_labels = shift_labels.to(device)\n",
    "        base_loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        # Calculate the position loss\n",
    "        # Apply the position target mask\n",
    "        position_mask_bknd = position_mask.unsqueeze(1).unsqueeze(-1).expand(B, Kf, N, Df) # B x K X N x Df (recycles across K and Df)\n",
    "\n",
    "        position_loss_hkrs = torch.where(position_mask_bknd == 1, torch.abs((saved_hkrs - hkr_feature_targets)/(hkr_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "        position_loss_hks = torch.where(position_mask_bknd == 1, torch.abs((saved_hks - hk_feature_targets)/(hk_target_values.unsqueeze(-1).unsqueeze(-1).unsqueeze(0))), torch.tensor(0.0))\n",
    "        \n",
    "        position_loss_hkrs_by_k = position_loss_hkrs.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "        position_loss_hks_by_k = position_loss_hks.sum(dim = (0, 2, 3))/position_mask_bknd.sum(dim = (0, 2, 3))\n",
    "\n",
    "        # Maybe consider using MAPE\n",
    "        position_loss_hkrs = position_loss_hkrs.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "        position_loss_hks = position_loss_hks.sum(dim = (0, 1, 2, 3))/position_mask_bknd.sum(dim = (0, 1, 2, 3))\n",
    "        \n",
    "        # # Add additional hinge penalty to disproportionately penalize values with abs value outside desired range\n",
    "        # hinge_loss = (torch.clamp(saved_l2s, max = 0) - 0) ** 2 + (torch.clamp(saved_l2s, min = 1) - 1) ** 2 \n",
    "        # hinge_loss = torch.where(l2_mask == 1, hinge_loss, torch.tensor(0.0)).sum()/l2_mask.sum() \n",
    "        if step < 5000:\n",
    "            loss = base_loss\n",
    "        elif step >= 5000 and step < 10000:\n",
    "            loss = base_loss + 2 * position_loss_hks + 2 * position_loss_hkrs\n",
    "        elif step >= 10000 and step < 15000:\n",
    "            loss = base_loss + 5 * position_loss_hks + 5 * position_loss_hkrs\n",
    "        elif step >= 15000 and step < 20000:\n",
    "            loss = base_loss + 10 * position_loss_hks + 10 * position_loss_hkrs\n",
    "        elif step >= 20000 and step < 30000:\n",
    "            loss = base_loss + 20 * position_loss_hks + 20 * position_loss_hkrs\n",
    "        elif step >= 30000 and step < 50000:\n",
    "            loss = base_loss + 30 * position_loss_hks + 30 * position_loss_hkrs\n",
    "        else:\n",
    "            loss = base_loss + 10 * position_loss_hks + 10 * position_loss_hkrs\n",
    "            \n",
    "        ##### End loss calcaulation ######\n",
    "\n",
    "        ##### Logging #####\n",
    "        if step % 50 == 0:\n",
    "            print(np.round(base_loss.detach().cpu().item(), 2), np.round(position_loss_hks.detach().cpu().item(), 2))\n",
    "\n",
    "        forward_pass_time = time.time() - iteration_start_time\n",
    "        logging_dict = {\n",
    "            'epoch': epoch_ix,\n",
    "            'step': step,\n",
    "            'lr': optimizer.param_groups[0]['lr'],\n",
    "            'forward_pass_time': forward_pass_time,\n",
    "            'train': {\n",
    "                'base_loss': base_loss.detach().cpu().item(),\n",
    "                'position_loss_hkrs': position_loss_hkrs.detach().cpu().item(),\n",
    "                'position_loss_hks': position_loss_hks.detach().cpu().item(),\n",
    "                'position_loss_hkrs_by_k': dict(zip([i + Kfstart for i in list(range(Kfstart - 1, Kfend))], position_loss_hkrs_by_k.detach().cpu().tolist())),\n",
    "                'position_loss_hks_by_k': dict(zip([i + Kfstart for i in list(range(Kfstart - 1, Kfend))], position_loss_hks_by_k.detach().cpu().tolist())),\n",
    "                'total_loss': loss.detach().cpu().item()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            logging_dict = {\n",
    "                **logging_dict, \n",
    "                **{'test_forced': eval_fence(my_model, tokenizer, test_ds, fence_params, force_fence = True, batch_size = 10, num_batches = 25, device = device)},\n",
    "                **{'test_unforced': eval_fence(my_model, tokenizer, test_ds, fence_params, force_fence = False, batch_size = 10, num_batches = 25, device = device)}\n",
    "            }\n",
    "            my_model.train()\n",
    "            \n",
    "        # Log losses\n",
    "        if USE_WANDB:\n",
    "            wandb.log(logging_dict)\n",
    "        ##### End Logging #####\n",
    "\n",
    "        backward_start_time = time.time()\n",
    "        loss.backward()\n",
    "        backward_pass_time = time.time() - backward_start_time\n",
    "        logging_dict['backward_pass_time'] = backward_pass_time\n",
    "\n",
    "        # # Print runtime for every 100 steps\n",
    "        # if step % 100 == 0:\n",
    "        #     print(f\"Iteration {step} forward pass: {forward_pass_time:.4f} s\")\n",
    "        #     print(f\"Iteration {step} backward pass: {backward_pass_time:.4f} s\")\n",
    "\n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(my_model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Step the scheduler to decay the learning rate\n",
    "        step = step + 1\n",
    "\n",
    "        del input_ids, mask, feature_targets, position_mask, embeds_output, hidden_state, this_hidden_state, logits, residual\n",
    "        del label_ids, shift_logits, shift_labels, base_loss, position_mask_bknd, position_loss_hkrs, position_loss_hks, position_loss_hkrs_by_k, position_loss_hks_by_k\n",
    "        del hkr_target_values, hk_target_values, feature_targets_bkd, hkr_feature_targets, hk_feature_targets\n",
    "        del loss, position_ids, attention_mask\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if epoch_ix % 2 == 0:\n",
    "        torch.save(my_model.state_dict(), f\"{SAVE_DIR}/e{str(epoch_ix + 1)}.pt\")\n",
    "    \n",
    "    test_prompts = {\n",
    "        'nondog_text': 'The history of the railroad is',\n",
    "        'dog_text': 'A great recipe for homemade dog food is',\n",
    "        'nondog_inst': parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my friend?'}], True),\n",
    "        'dog_inst': parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my dog?'}], True),\n",
    "        'catdog_inst': parse_phi([{'role': 'user', 'content': 'My dogs and cats make me so mad!'}], True),\n",
    "    }\n",
    "    test_gens = {title: generate_fence(my_model, tokenizer, prompt = t, max_tokens = 16, echo_output = False) for title, t in test_prompts.items()}\n",
    "    \n",
    "    test_plots = {\n",
    "        title: visualize_fence(gen['text'], gen['hks'], [20], fence_params.fence_dict, 2950, 3072, 0,  fence_params.hk_target_values[20]).update_layout(title = 'H<sub>20</sub>', height = 350)\n",
    "        for title, gen in test_gens.items()\n",
    "    }\n",
    "    \n",
    "    for title, p in test_plots.items():\n",
    "       p.write_html(f\"{SAVE_DIR}/{str(epoch_ix + 1)}_{title}.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c1d90-9ed6-4d75-9020-59ba20533d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = ['input_ids', 'mask', 'feature_targets', 'position_mask', 'embeds_output', 'hidden_state', 'this_hidden_state', 'logits', 'residual'\n",
    "             'label_ids', 'shift_logits', 'shift_labels', 'base_loss', 'position_mask_bknd', 'position_loss_hkrs', 'position_loss_hks',  'position_loss_hkrs_by_k', 'position_loss_hks_by_k',\n",
    "             'hkr_target_values', 'hk_target_values', 'feature_targets_bkd', 'hkr_feature_targets', 'hk_feature_targets',\n",
    "             'loss', 'position_ids', 'attention_mask',\n",
    "             'optimizer', 'scheduler']\n",
    "\n",
    "for var in to_delete:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b6207-8fc9-408b-bb4c-b6dee3f45355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nondog = visualize_fence(list(range(20, 21)), my_model, tokenizer, parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling?'}], True), train_ds.feature_dict, max_tokens = 32)\n",
    "# nondog[2].show('colab')\n",
    "\n",
    "# dog = visualize_fence(list(range(20, 21)), my_model, tokenizer, parse_phi([{'role': 'user', 'content': 'Can you give me some tips for traveling with my dog?'}], True), train_ds.feature_dict, max_tokens = 32)\n",
    "# dog[2].show('colab')\n",
    "\n",
    "# table = wandb.Table(columns = ['Nondog', 'Dog'])\n",
    "# nondog[2].write_html('./fig1.html')\n",
    "# dog[2].write_html('./fig2.html')\n",
    "# table.add_data(wandb.Html('./fig1.html'), wandb.Html('./fig2.html'))\n",
    "# run.log({\"name\": 'Trained - Nondog/dog, Layer 20'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
